input,correct,expected,predicted,score
"def register_component_producer(self, component_id: str, atom_name: str):
        
        logger.info(f""[DAG] Registering component producer {component_id=} {atom_name=}"")
        if <IFMASK>:
            self._component_producers[component_id] = atom_name
            if self._current_atom:
                logger.info(f""[DAG] Component registered while atom was active self._current_atom={self._current_atom}"")
        else:
            logger.warning(f""[DAG] Skipping producer registration for unknown atom {atom_name=}"") <ANS> ",true,atom_name in self.atoms,self._current_atom is None or self._current_ atom == 'unknown' or (self._current_atom,59.58
"def filter_property(self, siid: int, piid: int) -> bool:
        
        if (self._cache and 'properties' in self._cache and
            
            <IFMASK>):
            return True
        return False <ANS> ",true,self._cache and 'properties' in self._cache and,self._cache and 'properties' in self._cache and (not self._cached_properties) and (not self,65.94
"def _save_ontology_to_db(self, ontology: ""Ontology"") -> None:  # type: ignore[no-any-unimported]
        
        if <IFMASK>:
            raise ValueError(f""Knowledge graph {self.name} is already created."")
        graph = self.__get_ontology_storage_graph()
        ontology.save_to_graph(graph) <ANS> ",false,self.ontology_table_name in self.falkordb.list_graphs(),self.dtype.kind == 'f' and self.shape == SHAPE_SINGLETON and (not self.shape,50.91
"def _build_merge_function(self):
        

        def merge_fn(ctrl_msg: IngestControlMessage):
            do_trace_tagging = ctrl_msg.get_metadata(""config::add_trace_tagging"") is True
            if <IFMASK>:
                ts_exit = datetime.now()
                ctrl_msg.set_timestamp(f""trace::exit::{self._task_desc}"", ts_exit)
                ctrl_msg.set_timestamp(""latency::ts_send"", ts_exit)
            return ctrl_msg

        return merge_fn <ANS> ",false,do_trace_tagging,not self._task_desc.passed and self._task_desc.passed_categorical_count > 0 and (,51.64
"def filter_paths(openapi_schema: Dict[str, Any]) -> Dict[str, Any]:
    
    if <IFMASK>:
        return openapi_schema

    filtered_paths = {}
    for path, path_item in openapi_schema[""paths""].items():
        include_path = False
        filtered_operations = {}

        for method, operation in path_item.items(): <ANS> ",false,"""paths"" not in openapi_schema",not path_item.startswith('/') and (not path_item.endswith('.json')) and (not path_item,65.77
"def _handle_task_completion(self, task: asyncio.Task) -> None:
        
        task_id = getattr(task, ""task_id"", ""unknown"")
        self.pending_tasks.discard(task)

        if <IFMASK>:
            logger.warning(f""üö´ WORKER_CANCELLED [{task_id}] Task was cancelled"")
        elif task.exception() is not None:
            logger.error(
                f""üí• WORKER_EXCEPTION [{task_id}] Task completed with exception: {task.exception()}""
            )
        else:
            logger.info(f""üèÅ WORKER_CLEANUP [{task_id}] Task cleaned up successfully"") <ANS> ",true,task.cancelled(),self.pending_tasks and task.cancelled() and (not task.cancelling()) and (not task.cancelling()),63.28
"def _get_mime_type(self) -> str:
        
        if <IFMASK>:
            return f""image/{self._format.lower()}""

        if self.path:
            suffix = self.path.suffix.lower()
            return {
                "".png"": ""image/png"",
                "".jpg"": ""image/jpeg"",
                "".jpeg"": ""image/jpeg"", <ANS> ",true,self._format,self.path and self.path.endswith('.png') and (not self.path.endswith('.jpg')) and,65.13
"def check_and_install_dependencies():
    
    required_packages = [
        ""gunicorn"",
        ""tiktoken"",
        ""psutil"",
        # Add other required packages here
    ]

    for package in required_packages:
        if <IFMASK>:
            print(f""Installing {package}..."")
            pm.install(package)
            print(f""{package} installed successfully"") <ANS> ",false,not pm.is_installed(package),not os.path.exists(cache_dir) or not os.path.exists(cache_dir) or,66.76
"def get(cls, name: str, raise_on_missing: bool = True) -> Optional[T]:
        

        matches = [registered for registered in cls._get_storage() if re.match(registered, name)]

        if <IFMASK>:
            raise ValueError(f""Multiple taggers match {name}: {', '.join(matches)}"")

        elif len(matches) == 0:
            if raise_on_missing:
                tagger_names = "", "".join([tn for tn, _ in cls.items()])
                raise ValueError(f""Unknown tagger {name}; available taggers: {tagger_names}"")
            return None <ANS> ",true,len(matches) > 1,not matches and (not raise_on_missing) and (not cls._is_re_pattern_with_,60.25
"def get_single_param_type_from_schema(param_schema: Dict[str, Any]) -> str:
    
    if <IFMASK>:
        types = {schema.get(""type"") for schema in param_schema[""anyOf""] if schema.get(""type"")}
        if ""null"" in types:
            types.remove(""null"")
        if types:
            return next(iter(types))
        return ""string""
    return param_schema.get(""type"", ""string"") <ANS> ",true,"""anyOf"" in param_schema",param_type == 'string' and 'anyOf' in param_type_name and (not isinstance(param_,57.79
"def start(self):
        
        try:
            self.reset_trace()
            self.current_trace[""start_time""] = datetime.now()
            self._active = True
            self._monkey_patch()

            if <IFMASK>:
                loop = asyncio.get_event_loop()
                self._save_task = loop.create_task(self._periodic_save())

            logger.info(""Tracing started"")
        except Exception as e:
            logger.error(f""Error starting tracer: {e}"")
            self.on_error(e, context=""start"")
            raise <ANS> ",true,self.save_interval,self.enabled_for_logging and _CONNECTION_LOGGER.isEnabledFor(logging.DEBUG) and (not self.,77.09
"def generate_audio_from_phonemes(phonemes: str, voice: str = ""af_bella""):
    
    response = requests.post(
        json={""phonemes"": phonemes, ""voice"": voice},
        headers={""Accept"": ""audio/wav""}
    )
    if <IFMASK>:
        print(f""Error: {response.text}"")
        return None
    return response.content <ANS> ",true,response.status_code != 200,len(response.body) != 1 and (not response.body[0].startswith('https://')) and (,64.43
"def _build_conversation_context(self) -> str:
        
        context_parts = []

        # Include recent messages (last 5 for now)
        recent_messages = (
            self.state.messages[-5:]
            if len(self.state.messages) > 5
            else self.state.messages
        )

        for msg in recent_messages:
            if <IFMASK>:  # Skip system message in context
                role_label = ""User"" if msg.role == ""user"" else ""Assistant""
                context_parts.append(f""{role_label}: {msg.content}"")

        return (
            ""\n"".join(context_parts)
            if context_parts
            else ""This is the start of our conversation.""
        ) <ANS> ",false,"msg.role != ""system""",self.state.is_terminal and self.state.is_terminal and self.state.is_terminal and,58.67
"def to_api_params(self) -> Dict[str, JsonValue]:
        
        params: Dict[str, JsonValue] = {
            ""includeResolvedCases"": cast(JsonValue, self.include_resolved_cases),
            ""includeCommunications"": cast(JsonValue, self.include_communications),
            ""language"": cast(JsonValue, self.language),
        }

        if <IFMASK>:
            params[""caseIdList""] = cast(JsonValue, self.case_id_list)
        if self.display_id:
            params[""displayId""] = cast(JsonValue, self.display_id)
        if self.after_time:
            params[""afterTime""] = cast(JsonValue, self.after_time)
        if self.before_time:
            params[""beforeTime""] = cast(JsonValue, self.before_time)
        if self.max_results: <ANS> ",true,self.case_id_list,self.display_name and self.display_name != self.display_name_plural and self.display_,55.10
"def interpreter_feedback(self, output: str) -> str:
        
        if <IFMASK>:
            raise ValueError(""No output to interpret."")
        return f <ANS> ",false,not output,self.verbose > 0 and self.verbose_mod is not None and (not self.verbose_mod.is,51.93
"def auto_discover_processors():
    
    # get processors directory path
    processors_path = Path(__file__).parent.parent / ""processors""

    # iterate over all subdirectories in processors directory
    for _, name, _ in pkgutil.iter_modules([str(processors_path)]):
        # if it is a directory and contains processor.py
        processor_file = processors_path / name / ""processor.py""
        if <IFMASK>:
            module_path = f""lpm_kernel.file_data.processors.{name}.processor""
            try:
                importlib.import_module(module_path)
            except ImportError as e:
                print(f""Failed to load processor module {module_path}: {e}"") <ANS> ",true,processor_file.exists(),not os.path.exists(processor_file) or not os.path.exists(processor_file) or,71.04
"def download_youtube_video(video_url, output_path):
    
    # video_url, output_path = info
    try:
        if <IFMASK>:
            os.makedirs(os.path.dirname(output_path), exist_ok=True)
        # download command
        command = ['yt-dlp', '-f', 'bestvideo[ext=mp4]+bestaudio[ext=m4a]/best[ext=mp4]', '--merge-output-format',
               'mp4', '--output', output_path , video_url]
        # subprocess.run
        result = subprocess.run(command, capture_output=True, text=True, encoding='utf-8')

        if result.returncode == 0: <ANS> ",true,not os.path.exists(os.path.dirname(output_path)),not os.path.exists(output_path) or not os.path.exists(output_path) or,75.50
"def sub_device_state(
        self, did: str, handler: Callable[[str, MIoTDeviceState, Any], None],
        handler_ctx: Any = None
    ) -> bool:
        
        if <IFMASK>:
            raise MIoTClientError(f'did not exist, {did}')
        self._sub_device_state[did] = MipsDeviceState(
            did=did, handler=handler, handler_ctx=handler_ctx)
        _LOGGER.debug('client sub device state, %s', did)
        return True <ANS> ",true,did not in self._device_list_cache,self.opts.get('detect_mode') is True and self.opts.get('detect_mode') is,59.38
"def generate_audio_from_phonemes(phonemes: str, voice: str = ""af_bella"") -> Optional[bytes]:
    
    response = requests.post(
        json={""phonemes"": phonemes, ""voice"": voice},
        headers={""Accept"": ""audio/wav""}
    )
    
    print(f""Response status: {response.status_code}"")
    print(f""Response headers: {dict(response.headers)}"")
    print(f""Response content type: {response.headers.get('Content-Type')}"")
    print(f""Response length: {len(response.content)} bytes"")
    
    if <IFMASK>:
        print(f""Error response: {response.text}"")
        return None
        
    if not response.content:
        print(""Error: Empty response content"")
        return None
        
    return response.content <ANS> ",true,response.status_code != 200,len(response.body) != 1 and (not response.body[0].startswith('0')) or (len,56.78
"def _setup_user_dir(self):
        
        if '--user-data-dir' not in [
            
        ]:<IFMASK>:
            # For all browsers, use a temporary directory
            temp_dir = self._temp_directory_manager.create_temp_dir()
            self.options.arguments.append(f'--user-data-dir={temp_dir.name}') <ANS> ",false,'--user-data-dir' not in [,self.options.snapshot_dir and self.options.snapshot_dir.is_dir() and (not self,65.18
"def __post_init__(
        self,
    ) -> ""BaseSecurity"":  # dataclasses uses __post_init__ instead of model_validator
        
        valid_in_values = {
            ""apiKey"": [""header"", ""query"", ""cookie""],
            ""http"": [""bearer"", ""basic""],
            ""oauth2"": [""bearer""],
            ""openIdConnect"": [""bearer""],
            ""mutualTLS"": [""tls""],
            ""unsupported"": [""unsupported""],
        }
        if <IFMASK>:
            raise ValueError(f""Invalid in_value '{self.in_value}' for type '{self.type}'"")
        return self <ANS> ",true,self.in_value not in valid_in_values[self.type],self.in_value is None and self.type is None and (self.type is not None) and (,59.91
"def __call__(
      self, query_sequence: str, chain_polymer_type: str
  ) -> tuple[msa.Msa, MsaErrors]:
    
    if <IFMASK>:
      raise ValueError(
          f'EmptyMsaProvider of type {self._chain_polymer_type} called with '
          f'sequence of {chain_polymer_type=}, {query_sequence=}.'
      )
    return (
        msa.Msa.from_empty(
            query_sequence=query_sequence,
            chain_poly_type=self._chain_polymer_type, <ANS> ",true,chain_polymer_type != self._chain_polymer_type,self._chain_polymer_type == 'identity' and self._chain_polymer_type ==,66.78
"def interpreter_feedback(self, output: str) -> str:
        
        if <IFMASK>:
            return f""Web search failed: {output}""
        return f""Web search result:\n{output}"" <ANS> ",true,self.execution_failure_check(output),self.wait_for_completion is not None and self.wait_for_completion != 'timeout' and (,67.98
"def initialize_cache(self, num_gpu_blocks: int, num_cpu_blocks: int) -> None:
        

        # NOTE: We log here to avoid multiple logs when number of workers is
        # greater than one. We could log in the engine, but not all executors
        # have GPUs.
        logger.info(""# GPU blocks: %d, # CPU blocks: %d"", num_gpu_blocks, num_cpu_blocks)

        self.cache_config.num_gpu_blocks = num_gpu_blocks
        self.cache_config.num_cpu_blocks = num_cpu_blocks

        if <IFMASK>:
            print(f""before init cache memory allocated: {torch.cuda.memory_allocated() / 1e9}GB, reserved: {torch.cuda.memory_reserved() / 1e9}GB"")
        self.worker.initialize_cache(num_gpu_blocks=num_gpu_blocks, num_cpu_blocks=num_cpu_blocks)
        if torch.distributed.get_rank() == 0:
            print(f""after init cache memory allocated: {torch.cuda.memory_allocated() / 1e9}GB, reserved: {torch.cuda.memory_reserved() / 1e9}GB"") <ANS> ",false,torch.distributed.get_rank() == 0,self.use_cuda or self.use_cpu or self.use_fp16 or self.use_fp,57.39
"def validate_dimensions(dimension: Union[int, float], attribute_name: str) -> int:
    
    dimension = int(dimension)
    if <IFMASK>:
        raise ValueError(f""The '{attribute_name}' must be a multiple of 64."")
    return dimension <ANS> ",true,dimension <= 0 or dimension % 64 != 0,"not isinstance(dimension, (int, float)) or not isinstance(attribute_name, (str, int)) or",74.10
"def _format_comment_body(self, comment: GitHubPRComment) -> str:
        
        body = comment.comment_body

        # Add code snippet reference if provided
        if <IFMASK>:
            body += f""\n\n```\n{comment.code_snippet}\n```""

        # Add suggestion if provided
        if comment.suggestion:
            body += f""\n\n```suggestion\n{comment.suggestion}\n```""

        return body <ANS> ",true,comment.code_snippet,self.comment_body is not None and self.comment_body.startswith('comment') and (not self.,52.23
"def _save_credentials(self) -> None:
        
        if <IFMASK>:
            logger.error(f""Attempted to save null credentials for user {self.user_morphik_id}."")
            return

        creds_path = self._get_user_credentials_path()
        try:
            with open(creds_path, ""w"") as creds_file:
                json.dump(self.credentials, creds_file)
            logger.info(f""Successfully saved Zotero credentials for user {self.user_morphik_id}"") <ANS> ",true,not self.credentials,self.user_m changemeik_id is None and self.user_p changemeik_id is None and,46.31
"def extract_json_from_response(content: str) -> dict | None:
    
    try:
        json_start = content.find(""```json"")
        if <IFMASK>:
            json_text = content[json_start + 7 :]  # Skip past ```json
            json_end = json_text.find(""```"")
            if json_end != -1:
                json_text = json_text[:json_end].strip()
                return json.loads(json_text)
    except Exception as e:
        print(f""Error extracting JSON from response: {e}"")
    return None <ANS> ",false,json_start != -1,not content.endswith('.json') or not content.endswith('.json') or content.endswith('.json') or content,63.29
"def normalize_pandas(obj):
    
    if <IFMASK>:
        return obj.tolist()
    elif isinstance(obj, pd.DataFrame):
        return obj.to_dict(""records"")
    elif isinstance(obj, dict):
        return {k: normalize_pandas(v) for k, v in obj.items()}
    elif isinstance(obj, (list, tuple)):
        return [normalize_pandas(item) for item in obj]
    return obj <ANS> ",true,"isinstance(obj, pd.Series)","isinstance(obj, dict) and isinstance(obj, ABCIndex) and isinstance(obj, ABCIndex) and (not isinstance",63.48
"def get_client_and_project():
        
        is_dev_mode = os.getenv(""isDevelopmentMode"", ""enabled"") == ""enabled""
        if <IFMASK>:
            return None, None

        project_id = os.environ.get(""GCP_PROJECT"")
        if not project_id:
            raise HTTPException(
                status_code=500, detail=""GCP_PROJECT environment variable is not set""
            ) <ANS> ",true,is_dev_mode,project_id is None and location is None and (not is_dev_mode) and (not is_dev,59.58
"def is_fastTrack_eligible(self):
        
        if (<IFMASK>):
            logger.debug(""Fast track not eligible: context_url present"")
            return False
        if (len(self.handler.prev_queries) > 0):
            logger.debug(f""Fast track not eligible: {len(self.handler.prev_queries)} previous queries present"")
            return False
        logger.info(""Query is eligible for fast track"")
        return True <ANS> ",true,self.handler.context_url != '',self.handler.handler is not None and self.handler.handler.should_exit() and (not self.,63.50
"def load_server_config(cls) -> Dict[str, MCPServerConfig]:
        
        config_path = PROJECT_ROOT / ""config"" / ""mcp.json""

        try:
            config_file = config_path if config_path.exists() else None
            if <IFMASK>:
                return {}

            with config_file.open() as f:
                data = json.load(f)
                servers = {}

                for server_id, server_config in data.get(""mcpServers"", {}).items():
                    servers[server_id] = MCPServerConfig( <ANS> ",false,not config_file,server_config.get('port') is None or server_config.get('port') == port or server_,56.42
"def set_default_attn_processor(self):
        
        if all(
            
            
        ):<IFMASK>:
            processor = AttnAddedKVProcessor()
        elif all(
            proc.__class__ in CROSS_ATTENTION_PROCESSORS
            for proc in self.attn_processors.values()
        ):
            processor = AttnProcessor()
        else:
            raise ValueError( <ANS> ",false,all(,not self.is_premature() or self.is_talk() or self.is_dumb_terminal,53.06
"def state(self, value: JobStateEnum) -> None:
        
        if <IFMASK>:
            logger.error(f""Attempt to change state from {self._state.name} to {value.name} denied."")
            raise ValueError(f""Cannot change state from {self._state.name} to {value.name}."")
        if value.value < self._state.value:
            logger.error(f""Invalid state transition attempt from {self._state.name} to {value.name}."")
            raise ValueError(f""State can only transition forward, from {self._state.name} to {value.name} not allowed."")
        self._state = value <ANS> ",true,self._state in _TERMINAL_STATES,self._state == JobState.JOB_STATE_RUNNING and value.state != self._state.name and (self,64.00
"def _get_hash(identifier: str) -> str:
        
        identifier = identifier.lower().strip()
        if <IFMASK>:
            # Hash functions have to take bytes
            identifier = identifier.encode('utf-8')

        hash_value = sha256(identifier).hexdigest()
        return f""{hash_value}_{len(identifier)}"" <ANS> ",true,"isinstance(identifier, str)",identifier == 'sha256' or identifier == 'sha256' or identifier == 'sha256' or identifier == ',62.11
"def is_valid_tool_call_item(call_item: dict) -> bool:
    
    if <IFMASK>:
        return False

    if set(call_item.keys()) - {""name"", ""arguments""}:  # noqa: SIM103
        return False

    return True <ANS> ",true,"""name"" not in call_item or not isinstance(call_item[""name""], str)","not isinstance(call_item, dict) or not isinstance(call_item, list) or (not isinstance(",67.66
"def _handle_content(self, content):
        
        if <IFMASK>:
            text = content.get(""text"", """")
            if text == ""<DONE>"":
                return
            logger.info(f""Assistant: {text}"") <ANS> ",false,"content.get(""type"") == ""text""",self.content_type == 'application/json' and self.content_type == 'application/json-rpc,62.17
"def add_prompt(self, prompt: Prompt) -> Prompt:
        
        logger.debug(f""Adding prompt: {prompt.name}"")
        existing = self._prompts.get(prompt.name)
        if <IFMASK>:
            if self.warn_on_duplicate_prompts:
                logger.warning(f""Prompt already exists: {prompt.name}"")
            return existing
        self._prompts[prompt.name] = prompt
        return prompt <ANS> ",false,existing,self.config.option.verbose >= 1 and self.config.option.debug == 'debug' and (not,53.07
"def _create_visualizations(self):
        
        if <IFMASK>:
            logger.warning(""Matplotlib not available, skipping visualization creation"")
            return

        if not self.study or len(self.study.trials) < 2:
            logger.warning(""Not enough trials to create visualizations"")
            return

        # Create directory for visualizations <ANS> ",false,not PLOTTING_AVAILABLE,self.study.trials == 0 and self.study.mode == 'exec' and (not,62.34
"def delete_all_output_files() -> bool:
    
    try:
        for filename in os.listdir(OUTPUTS_DIR):
            if <IFMASK>:
                file_path = os.path.join(OUTPUTS_DIR, filename)
                os.remove(file_path)
        return True
    except Exception as e:
        print(f""Error deleting output files: {e}"")
        return False <ANS> ",false,any(filename.endswith(ext) for ext in AUDIO_FORMATS),len(all_output_files) == 0 and all_output_files[0] == 'true' and,60.08
"def _respect_rate_limit(self):
        
        current_time = time.time()
        time_since_last_request = current_time - self.last_request_time

        if <IFMASK>:
            wait_time = self.delay_between_requests - time_since_last_request
            logger.info(f""Rate limiting: waiting {wait_time:.2f} seconds"")
            time.sleep(wait_time)

        self.last_request_time = time.time() <ANS> ",true,time_since_last_request < self.delay_between_requests,self.last_request_time < self.last_request_time + self.last_request_time +,55.92
"def _cleanup_existing_alembic(self) -> None:
        
        # logger.info(""Cleaning up existing Alembic configuration..."")

        # Remove entire alembic directory if it exists
        if <IFMASK>:
            import shutil

            shutil.rmtree(self.alembic_dir)
            logger.info(f""Removed alembic directory: {self.alembic_dir}"")

        # Remove alembic.ini if it exists
        if self.alembic_ini_path.exists():
            self.alembic_ini_path.unlink() <ANS> ",true,self.alembic_dir.exists(),not self.config.option.verbose or self.config.option.verbose > 1 or self.config.option,60.42
"def _format_server_info(self, server_name: str) -> str:
        
        server_config = self.server_registry.get_server_config(server_name)
        server_str = f""Server Name: {server_name}""
        if <IFMASK>:
            return server_str

        description = server_config.description
        if description:
            server_str = f""{server_str}\nDescription: {description}""

        return server_str <ANS> ",false,not server_config,self.server_info is not None and self.server_info.info.mode == 'wrap' and self,58.50
"def format_question(category: str, question: str, answer: str) -> Dict[str, Any]:
    
    # Basic sanity checks
    if <IFMASK>:
        raise ValueError(f""Empty question or answer in {category}"")
        
    return {
        ""id"": f""{category}_{random.getrandbits(32):08x}"",
        ""category"": category,
        ""question"": clean_text(question),
        ""answer"": clean_text(answer),
        ""metadata"": { <ANS> ",true,not question or not answer,not answer or answer == 'random' or answer == 'random.getrandbits(32) or answer == ',60.62
"def get_tool(self, key: str) -> Tool:
        
        if <IFMASK>:
            return self._tools[key]
        raise NotFoundError(f""Unknown tool: {key}"") <ANS> ",true,key in self._tools,self._is_kubernetes and self.is_kubernetes_package(key) and (not self.is_kubernetes,52.03
"def stable_serialize(obj: Any) -> Any:
    
    if <IFMASK>:
        return {k: stable_serialize(v) for k, v in sorted(obj.items())}
    elif isinstance(obj, (list, tuple)):
        return [stable_serialize(x) for x in obj]
    elif isinstance(obj, (str, int, float, bool, type(None))):
        return obj
    else:
        return str(obj) <ANS> ",true,"isinstance(obj, dict)","isinstance(obj, dict) and isinstance(obj, list) and (not isinstance(obj, dict)) and (",66.50
"def register_mcp_tools(self):
        
        if <IFMASK>:
            for server_name in self.mcp_client.server_name_to_tools:
                for tool in self.mcp_client.server_name_to_tools[server_name]:
                    tool_name = f""mcp.{server_name}.{tool.name}""
                    self.registry.registry.actions[tool_name] = RegisteredAction(
                        name=tool_name,
                        description=tool.description,
                        function=tool,
                        param_model=create_tool_param_model(tool), <ANS> ",true,self.mcp_client,self.mcp_client.server_name_to_tools is not None and self.mcp_client,65.77
"def get_model_config(version: str):
    
    if <IFMASK>:
        raise ValueError(f""Unsupported model version '{version}'. Supported versions are: {list(MODEL_CONFIGS.keys())}"")
    return MODEL_CONFIGS[version] <ANS> ",true,version not in MODEL_CONFIGS,version == 'auto' and (not is_classifier(version)) and (not is_regressor(version)) and,53.00
"def get_cortex_search_service(cfg: RetrievalProviderConfig) -> Tuple[str,str,str]:
    
    if <IFMASK>:
        raise snowflake.ConfigurationError(""Unable to determine Snowflake configuration"")
    index_name = cfg.index_name
    if not index_name:
        raise snowflake.ConfigurationError(""Unable to determine Snowflake Cortex Search Service, is SNOWFLAKE_CORTEX_SEARCH_SERVICE set?"")
    parts = index_name.split(""."")
    if len(parts) != 3:
        raise snowflake.ConfigurationError(f""Invalid SNOWFLAKE_CORTEX_SEARCH_SERVICE, expected format:<database>.<schema>.<service>, got {index_name}"")
    return (parts[0], parts[1], parts[2]) <ANS> ",true,not cfg,"not isinstance(cfg.index_urls, list) or not isinstance(cfg.index_urls[0], list",66.44
"def _jsonl_to_csv(self, jsonl_file, csv_file):
        
        with open(jsonl_file, 'r', encoding='utf-8') as infile:
            data = [json.loads(line) for line in infile]
        
        if <IFMASK>:
            print(""Empty JSONL file."")
            return
        
        with open(csv_file, 'w', newline='', encoding='utf-8') as outfile:
            writer = csv.DictWriter(outfile, fieldnames=data[0].keys())
            writer.writeheader()
            writer.writerows(data) <ANS> ",true,not data,self.header is not None and self.header.name not in self.data[0] and self.header,53.05
"def recursive_mask_values(obj, parent_key=None):
            
            if <IFMASK>:
                return {k: recursive_mask_values(v, k) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [recursive_mask_values(item, parent_key) for item in obj]
            elif isinstance(obj, str):
                # List of keys that should NOT be masked
                excluded_keys = {
                    'start_time', 'end_time', 'name', 'id', 
                    'hash_id', 'parent_id', 'source_hash_id', <ANS> ",true,"isinstance(obj, dict)","isinstance(obj, str) and obj.endswith('start_time') and obj.endswith('end_time')",67.80
"def deprecated(replacement: str = """"):
    
    import functools
    import warnings

    def decorator(func):
        qualified_name = _get_qualified_name(func)
        @functools.wraps(func)
        def wrapped(*args, **kwargs):
            msg = f""Warning: API '{qualified_name}' is deprecated.""
            if <IFMASK>:
                msg += f"" Please use '{replacement}' instead.""
            warnings.warn(msg, category=DeprecationWarning, stacklevel=2)
            return func(*args, **kwargs)
        return wrapped
    return decorator <ANS> ",true,replacement,"not isinstance(replacement, str) or not isinstance(args, (list, tuple)) or len(args) ==",67.94
"def _format_timestamp(self, timestamp: str) -> str:
        
        if <IFMASK>:
            return timestamp

        try:
            year = timestamp[0:4]
            month = timestamp[4:6]
            day = timestamp[6:8]
            hour = timestamp[8:10]
            minute = timestamp[10:12] <ANS> ",false,len(timestamp) < 14,self.tz is not None and self.tz_convert is not None and (not isinstance(self.tz_,67.62
"def get_skill_states(skill_category: str) -> Set[str]:
    
    if <IFMASK>:
        return _skill_states_cache[skill_category]

    try:
        # Import the skill category module
        skill_module = importlib.import_module(f""skills.{skill_category}"")

        # Look for the SkillStates TypedDict class
        if hasattr(skill_module, ""SkillStates""): <ANS> ",false,skill_category in _skill_states_cache,"not hasattr(skill_module, 'SkillStates') or not hasattr(skill_module, 'get",67.07
"def _progress_callback(self, message: str, progress: int, metadata: dict) -> None:
        
        logger.info(f""Progress: {progress}% - {message}"")
        if <IFMASK>:
            self.progress_callback(message, progress, metadata) <ANS> ",true,"hasattr(self, ""progress_callback"")",self.wait_for_completion is not None and self.wait_for_completion.done() and (not,71.66
"def custom_get_evaluation_llm(custom_config=None):
            
            if <IFMASK>:
                custom_config = evaluation_config

            print(f""Getting evaluation LLM with config: {custom_config}"")
            return get_llm(**custom_config) <ANS> ",true,custom_config is None,"custom_config is not None and (not isinstance(custom_config, dict)) and (not isinstance(custom_",77.74
"def to_dict(self):
        
        result = {
            ""id"": self.id,
            ""name"": self.name,
            ""type"": self.type,
            ""path"": self.path,
            ""created_at"": self.created_at.isoformat() if self.created_at else None,
            ""meta_data"": self.meta_data,
        }
        if <IFMASK>:
            result[""document_id""] = self.document_id
        return result <ANS> ",true,self.document_id,self.document_id is not None and self.document_name is not None and (self.document_type,70.56
"def trace_id_to_uuid(trace_id: str) -> str:
    
    trace_id = str(trace.format_trace_id(trace_id))
    if <IFMASK>:
        raise ValueError(""Trace ID must be a 32-character hexadecimal string"")
    return f""{trace_id[:8]}-{trace_id[8:12]}-{trace_id[12:16]}-{trace_id[16:20]}-{trace_id[20:]}"" <ANS> ",true,len(trace_id) != 32,not trace_id.startswith('http') or not trace_id.endswith('https') or not trace_id,64.72
"def _get_endpoint_config(self):
        
        endpoint_config = CONFIG.retrieval_endpoints.get(self.endpoint_name)
        
        if <IFMASK>:
            error_msg = f""No configuration found for endpoint {self.endpoint_name}""
            logger.error(error_msg)
            raise ValueError(error_msg)
        
        # Verify this is an Azure AI Search endpoint
        if endpoint_config.db_type != ""azure_ai_search"":
            error_msg = f""Endpoint {self.endpoint_name} is not an Azure AI Search endpoint (type: {endpoint_config.db_type})""
            logger.error(error_msg) <ANS> ",false,not endpoint_config,not self.endpoint_name.endswith('.json') or self.endpoint_name.endswith('.json') or self,64.91
"def save_results(results, output_dir):
    
    output_path = Path(output_dir) / ""rich_autoscan_results.json""

    # Convert results to serializable format
    serializable_results = []
    for result in results:
        if <IFMASK>:
            continue
        serializable_results.append(result)

    with open(output_path, ""w"") as f:
        json.dump(serializable_results, f, indent=2, default=lambda o: o.value if isinstance(o, Enum) else o)

    print(f""Results saved to {output_path}"") <ANS> ",false,result is None,not results or not results[0].endswith('.json') or results[1].endswith('.json') or results[,66.83
"def __exit__(self, exc_type, exc_value, exc_tb):
        
        if <IFMASK>:
            self.end.record()
            torch.cuda.synchronize()
            self.time = self.start.elapsed_time(self.end)
            if self.name is not None:
                logger.info(f'{self.name} takes {self.time} ms') <ANS> ",false,"os.environ.get('HY3DGEN_DEBUG', '0') == '1'",self.name is not None and self.pid != os.getpid() and (self.name != 'MainProcess'),56.97
"def create_turn_dir(self) -> None:
        
        if <IFMASK>:
            logger.warning(""Cannot create turn directory: run_dir not set"")
            return

        # Increment turn counter
        self.turn_count += 1

        # Create turn directory with padded number
        turn_name = f""turn_{self.turn_count:03d}"" <ANS> ",true,not self.run_dir,self.config.getvalue('do_xcom_push') or self.config.getvalue('write_index') or,53.12
"def copy_and_update(self, *args, **kwargs):
        
        new_info = asdict(self)

        for si in args:
            assert isinstance(si, StreamInfo)
            new_info.update({k: v for k, v in asdict(si).items() if v is not None})

        if <IFMASK>:
            new_info.update(kwargs)

        return StreamInfo(**new_info) <ANS> ",false,len(kwargs) > 0,self.update_perms or update_perms or update_perms or update_perms or update_perms or (self,67.44
"def write_output(
    inference_result: model.InferenceResult,
    output_dir: os.PathLike[str] | str,
    terms_of_use: str | None = None,
    name: str | None = None,
) -> None:
  
  processed_result = post_process_inference_result(inference_result)

  prefix = f'{name}_' if name is not None else ''

  with open(os.path.join(output_dir, f'{prefix}model.cif'), 'wb') as f:
    f.write(processed_result.cif)

  with open(
      os.path.join(output_dir, f'{prefix}summary_confidences.json'), 'wb'
  ) as f:
    f.write(processed_result.structure_confidence_summary_json)

  with open(os.path.join(output_dir, f'{prefix}confidences.json'), 'wb') as f:
    f.write(processed_result.structure_full_data_json)

  if <IFMASK>:
    with open(os.path.join(output_dir, 'TERMS_OF_USE.md'), 'wt') as f:
      f.write(terms_of_use) <ANS> ",false,terms_of_use is not None,"not isinstance(constraints_file, str) or not isinstance(constraints_file_path, str) or (constraints",53.74
"def xpath_attr_functional_pseudo_element(
            xpath: OriginalXPathExpr, function: FunctionalPseudoElement
    ) -> XPathExpr:
        
        if <IFMASK>:
            raise ExpressionError(
                f""Expected a single string or ident for ::attr(), got {function.arguments!r}""
            )
        return XPathExpr.from_xpath(xpath, attribute=function.arguments[0].value) <ANS> ",false,"function.argument_types() not in ([""STRING""], [""IDENT""])","not isinstance(xpath, str) or not isinstance(function, str) or (isinstance(xpath, str) and",73.14
"def initialize_cache(self, num_gpu_blocks: int, num_cpu_blocks: int) -> None:
        

        # NOTE: We log here to avoid multiple logs when number of workers is
        # greater than one. We could log in the engine, but not all executors
        # have GPUs.
        logger.info(""# GPU blocks: %d, # CPU blocks: %d"", num_gpu_blocks, num_cpu_blocks)

        self.cache_config.num_gpu_blocks = num_gpu_blocks
        self.cache_config.num_cpu_blocks = num_cpu_blocks

        if <IFMASK>:
            print(
                f""before init cache memory allocated: {torch.cuda.memory_allocated() / 1e9}GB, reserved: {torch.cuda.memory_reserved() / 1e9}GB""
            )
        self.worker.initialize_cache(num_gpu_blocks=num_gpu_blocks, num_cpu_blocks=num_cpu_blocks)
        if torch.distributed.get_rank() == 0:
            print(
                f""after init cache memory allocated: {torch.cuda.memory_allocated() / 1e9}GB, reserved: {torch.cuda.memory_reserved() / 1e9}GB""
            ) <ANS> ",false,torch.distributed.get_rank() == 0,self.use_cuda or self.use_cpu or self.use_rng_state or self.use_,58.19
"def extract_urls_from_jsonl(file_path):
    
    urls = set()
    url_to_data = {}
    with open(file_path, ""r"", encoding=""utf-8"") as f:
        for line in f:
            try:
                data = json.loads(line.strip())
                if <IFMASK>:
                    url = data[""url""]
                    urls.add(url)
                    # Store minimal context for each URL
                    url_to_data[url] = {""id"": data.get(""id"", """"), ""type"": data.get(""type"", """"), ""page"": data.get(""page"", """")}
            except json.JSONDecodeError:
                print(f""Warning: Could not parse JSON from line in {file_path}"")
                continue
    return urls, url_to_data <ANS> ",false,"""url"" in data and data[""url""]",not file_path.startswith('http') or file_path.startswith('https') or file_path.startswith,74.69
"def get_port():
    
    if <IFMASK>:
        port = int(os.environ['PORT'])
        print(f""Using PORT from environment variable: {port}"")
        return port
    elif 'WEBSITE_SITE_NAME' in os.environ:
        # Running in Azure App Service
        print(""Running in Azure App Service, using default port 8000"")
        return 8000  # Azure will redirect requests to this port
    else: <ANS> ",true,'PORT' in os.environ,not os.environ.get('NO_COLOR') or not os.environ.get('SANIC_COLOR') or,56.27
"def parse_rules_file(file_path):
    
    pdf_rules = defaultdict(list)

    with open(file_path, ""r"") as f:
        for line in f:
            line = line.strip()
            if <IFMASK>:
                continue
            try:
                rule = json.loads(line)
                if ""pdf"" in rule:
                    pdf_rules[rule[""pdf""]].append(rule)
            except json.JSONDecodeError:
                print(f""Warning: Could not parse line as JSON: {line}"") <ANS> ",true,not line,not line.startswith('Filesystem') or line.startswith('Filesystem') or line.startswith('Filesystem'),70.58
"def recreate_collection(collection_name, vector_size):
    
    if <IFMASK>:
        print(f""Dropping existing collection '{collection_name}'"")
        client.delete_collection(collection_name)

    print(f""Creating collection '{collection_name}' with vector size {vector_size}"")
    client.create_collection(
        collection_name=collection_name,
        vectors_config=VectorParams(size=vector_size, distance=Distance.COSINE),
    ) <ANS> ",true,client.collection_exists(collection_name),vector_size is None or vector_size == 0 or collection_name.endswith('vector') or collection_name,57.99
"def _fetch_max_batch_size(self, model_name, model_version: str = """") -> int:
        
        if <IFMASK>:
            return self._max_batch_sizes[model_name]

        with self._lock:
            # Double check, just in case another thread set the value while we were waiting
            if model_name in self._max_batch_sizes:
                return self._max_batch_sizes[model_name]

            if not self._grpc_endpoint: <ANS> ",true,model_name in self._max_batch_sizes,self._max_batch_sizes is None or self._max_batch_sizes.get('max_batch_,63.85
"def extract_json_from_string(s: str) -> Optional[Any]:
    
    # Regex to find JSON objects (greedy, matches first { to last })
    match = re.search(r""\{.*\}"", s, re.DOTALL)
    if <IFMASK>:
        json_str = match.group(0)
        try:
            return json.loads(json_str)
        except json.JSONDecodeError:
            return None
    return None <ANS> ",false,match,not s.endswith('.json') or s.endswith('.json') or s.endswith('.json') or s.,71.68
"def create_gpt_vector_store(client: ""OpenAI"", name: str, fild_ids: list[str]) -> Any:
    
    try:
        vector_store = client.vector_stores.create(name=name)
    except Exception as e:
        raise AttributeError(f""Failed to create vector store, please install the latest OpenAI python package: {e}"")

    # poll the status of the file batch for completion.
    batch = client.vector_stores.file_batches.create_and_poll(vector_store_id=vector_store.id, file_ids=fild_ids)

    if <IFMASK>:
        time.sleep(1)
        logging.debug(f""file batch status: {batch.file_counts}"")
        batch = client.vector_stores.file_batches.poll(vector_store_id=vector_store.id, batch_id=batch.id)

    if batch.status == ""completed"":
        return vector_store

    raise ValueError(f""Failed to upload files to vector store {vector_store.id}:{batch.status}"") <ANS> ",false,"batch.status == ""in_progress""","not isinstance(vector_store, dict) or not isinstance(vector_store, list) or len(vector_",69.21
"def get_config_for_model(model_string: str) -> Dict[str, Any]:
    
    if <IFMASK>:
        return MODEL_CONFIG_MAP[model_string]
    # If model not found, use default configuration based on provider
    provider, _ = parse_model_string(model_string)
    return {
        ""provider"": provider,
        ""default_params"": {""temperature"": 0.3},
    } <ANS> ",true,model_string in MODEL_CONFIG_MAP,not model_string.startswith('providers/') or not model_string.endswith('provider/') or not model_string,63.94
"def _validate_resolution_format(resolution: str):
    
    pattern = r""^\d+x\d+$""  # Matches a pattern of digits, ""x"", and digits
    matched_resolution = re.match(pattern, resolution)
    if <IFMASK>:
        raise ValueError(f""Invalid resolution format: {resolution}"") <ANS> ",false,matched_resolution is None,resolution == 'x' or (resolution == 'f' and (not (resolution == 'f' and (not,55.66
"def create_item(
        name: str, value: int, state: dict[str, Any] | None = None
    ) -> dict[str, Any]:
        
        if <IFMASK>:
            # state was read
            pass
        return {""name"": name, ""value"": value} <ANS> ",true,state,state is None and name not in state or state[name] != value or state[name] != value or state,60.81
"def create_named_schedule_sampler(name, diffusion):
    
    if <IFMASK>:
        return UniformSampler(diffusion)
    elif name == ""loss-second-moment"":
        return LossSecondMomentResampler(diffusion)
    else:
        raise NotImplementedError(f""unknown schedule sampler: {name}"") <ANS> ",true,"name == ""uniform""",not name.startswith('loss-second-moment') and name.endswith('loss-second-moment') and (,60.18
"def validate_window_size(configured: dict[str, Any], actual: dict[str, Any]) -> None:
	
	# Allow for small differences due to browser chrome, scrollbars, etc.
	width_diff = abs(configured['width'] - actual['width'])
	height_diff = abs(configured['height'] - actual['height'])

	# Tolerance of 5% or 20px, whichever is greater
	width_tolerance = max(configured['width'] * 0.05, 20)
	height_tolerance = max(configured['height'] * 0.05, 20)

	if <IFMASK>:
		print(f'‚ö†Ô∏è  WARNING: Significant difference between expected and actual page size! ¬±{width_diff}x{height_diff}px')
		raise Exception('Window size validation failed')
	else:
		print('‚úÖ Window size validation passed: actual size matches configured size within tolerance') <ANS> ",false,width_diff > width_tolerance or height_diff > height_tolerance,len(configured) != actual.get('width') or len(actual) != configured['width'] or len(,67.70
"def cleanup_memory(self):
        
        # Clear CUDA cache
        if <IFMASK>:
            torch.cuda.empty_cache()

            # Log memory usage if in verbose mode
            allocated = torch.cuda.memory_allocated() / (1024 ** 3)
            reserved = torch.cuda.memory_reserved() / (1024 ** 3)
            logger.info(f""GPU Memory: {allocated:.2f}GB allocated, {reserved:.2f}GB reserved"")

        # Collect Python garbage <ANS> ",false,torch.cuda.is_available(),self.is_alive() and self.is_alive() and (not self.is_alive()) and (,66.52
"def copy_to_shm(src:str):
    
    shm_model_root = '/dev/shm/verl-cache/'
    src_abs = os.path.abspath(os.path.normpath(src))
    dest = os.path.join(shm_model_root, hashlib.md5(src_abs.encode('utf-8')).hexdigest())
    os.makedirs(dest, exist_ok=True)
    dest = os.path.join(dest, os.path.basename(src_abs))
    if <IFMASK>:
        # inform user and depends on him
        print(f""[WARNING]: The memory model path {dest} already exists. If it is not you want, please clear it and restart the task."")
    else:
        if os.path.isdir(src):
            shutil.copytree(src, dest, symlinks=False, dirs_exist_ok=True)
        else:
            shutil.copy2(src, dest)
    return dest <ANS> ",true,"os.path.exists(dest) and verify_copy(src, dest)",not os.path.exists(src) or not os.path.exists(dst) or os.path.,74.99
"def from_pretrained(self, repo_id: str) -> ""Vocos"":
        
        config_path = hf_hub_download(repo_id=repo_id, filename=""config.yaml"")
        model_path = hf_hub_download(repo_id=repo_id, filename=""pytorch_model.bin"")
        model = self.from_hparams(config_path)
        state_dict = torch.load(model_path, map_location=""cpu"")
        if <IFMASK>:
            encodec_parameters = {
                ""feature_extractor.encodec."" + key: value
                for key, value in model.feature_extractor.encodec.state_dict().items()
            }
            state_dict.update(encodec_parameters)
        model.load_state_dict(state_dict)
        model.eval()
        return model <ANS> ",false,"isinstance(model.feature_extractor, EncodecFeatures)",not self.do_xcom_push or not self.do_xcom_push_all_kwargs or self.,61.30
"def _parse_datetime_field(self, dt_obj: Optional[Dict]) -> Optional[datetime]:
        
        if <IFMASK>:
            return None
        try:
            dt_str = dt_obj[""dateTime""]
            if ""T"" in dt_str:
                # Handle timezone info
                if dt_str.endswith(""Z""):
                    dt_str = dt_str.replace(""Z"", ""+00:00"")
                elif ""+"" not in dt_str and ""-"" not in dt_str[-6:]: <ANS> ",false,"not dt_obj or not dt_obj.get(""dateTime"")",self.tz is not None and dt_str.startswith('Z') and (self.tz != dt_str,68.90
"def _get_auth_headers(self) -> Dict[str, str]:
        
        headers = {
            ""Content-Type"": ""application/json"",
            ""Accept"": ""application/json""
        }
        
        if <IFMASK>:
            # Basic authentication (username:password)
            encoded_credentials = base64.b64encode(self.credentials.encode()).decode()
            headers[""Authorization""] = f""Basic {encoded_credentials}""
        else:
            # API key authentication
            headers[""Authorization""] = f""Bearer {self.credentials}""
        
        return headers <ANS> ",true,':' in self.credentials,"self.username and self.password and self.database_type and self.database_type not in (None,",61.53
"def build_agent_tree(parent_tree, agent_obj):
            
            parent_tree.add(create_tools_section(agent_obj.tools))

            if <IFMASK>:
                agents_branch = parent_tree.add(""ü§ñ [italic #1E90FF]Managed agents:"")
                for name, managed_agent in agent_obj.managed_agents.items():
                    agent_tree = agents_branch.add(get_agent_headline(managed_agent, name))
                    if managed_agent.__class__.__name__ == ""CodeAgent"":
                        agent_tree.add(
                            f""‚úÖ [italic #1E90FF]Authorized imports:[/italic #1E90FF] {managed_agent.additional_authorized_imports}""
                        )
                    agent_tree.add(f""üìù [italic #1E90FF]Description:[/italic #1E90FF] {managed_agent.description}"") <ANS> ",true,agent_obj.managed_agents,"not isinstance(agent_obj.config.getoption('workerinput', 'False') or not isinstance(agent_obj",53.05
"def _read_file_safe(file_path: str) -> Optional[str]:
    
    if <IFMASK>:
        return None
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            return f.read()
    except Exception as e:
        logger.error(f""Error reading file {file_path}: {e}"")
        return None <ANS> ",false,not os.path.exists(file_path),not file_path.startswith('http') or file_path.startswith('https') or file_path.startswith,73.81
"def __init__(
        self,
        exit_stack: contextlib.AsyncExitStack | None = None,
        component_name_hook: _ComponentNameHook | None = None,
    ) -> None:
        

        self._tools = {}
        self._resources = {}
        self._prompts = {}

        self._sessions = {}
        self._tool_to_session = {}
        if <IFMASK>:
            self._exit_stack = contextlib.AsyncExitStack()
            self._owns_exit_stack = True
        else:
            self._exit_stack = exit_stack
            self._owns_exit_stack = False
        self._session_exit_stacks = {}
        self._component_name_hook = component_name_hook <ANS> ",true,exit_stack is None,self._exit_stack is None and self._exit_stack.is_closing() and (not self._exit,62.10
"def _fix_chrome_permissions(self, user_data_dir):
        
        try:
            if <IFMASK>:  # macOS
                import subprocess
                import pwd
                
                # Get current user
                current_user = pwd.getpwuid(os.getuid()).pw_name
                
                # Fix permissions for Chrome directory
                chrome_dir = os.path.expanduser('~/Library/Application Support/Google/Chrome') <ANS> ",false,sys.platform == 'darwin',self.use_managed_permissions and user_data_dir in self.get_managed_permissions() and (,54.81
"def init_progressive(self, noise_scale=0.01):
        
        self._copy_non_transformer_params()

        # copy pretrained layers
        for i in range(self.pretrained_layers):
            for key in self.pretrained_state:
                if <IFMASK>:
                    new_key = key.replace(f""blocks.{i}."", f""blocks.{i}."")
                    self.target_state[new_key] = self.pretrained_state[key]

        # progressive init new layers
        for i in range(self.pretrained_layers, self.target_layers):
            prev_layer = i - 1
            for key in self.target_state:
                if f""blocks.{i}."" in key: <ANS> ",false,"f""blocks.{i}."" in key",noise_scale < 0.1 or noise_scale > 1.0 or (noise_scale < 0.1,66.96
"def check_and_install_dependencies():
    
    required_packages = [
        ""uvicorn"",
        ""tiktoken"",
        ""fastapi"",
        # Add other required packages here
    ]

    for package in required_packages:
        if <IFMASK>:
            print(f""Installing {package}..."")
            pm.install(package)
            print(f""{package} installed successfully"") <ANS> ",false,not pm.is_installed(package),AIRFLOW_V_3_0_PLUS and (not AIRFLOW_V_3_0_PLUS.value()),76.37
"def _analyze_contrarian_sentiment(news):
    

    max_score = 1
    score = 0
    details: list[str] = []

    if <IFMASK>:
        details.append(""No recent news"")
        return {""score"": score, ""max_score"": max_score, ""details"": ""; "".join(details)}

    # Count negative sentiment articles
    sentiment_negative_count = sum(
        1 for n in news if n.sentiment and n.sentiment.lower() in [""negative"", ""bearish""]
    ) <ANS> ",true,not news,len(news) == 0 and len(news) == 1 and (len(news) == 1) and (,58.83
"def opt_repetitions(up_to_n, prefix_with_sep=False):
        

        content = (
            f""{separator_rule} {item_rule}""
            if prefix_with_sep and separator_rule
            else item_rule
        )
        if <IFMASK>:
            return """"
        elif up_to_n == 1:
            return f""({content})?""
        elif separator_rule and not prefix_with_sep:
            return f""({content} {opt_repetitions(up_to_n - 1, prefix_with_sep=True)})?""
        else:
            return (f""({content} "" * up_to_n).rstrip() + ("")?"" * up_to_n) <ANS> ",false,up_to_n == 0,not sep and (not prefix_with_sep) and (not sep) and (not sep) and (not,71.63
"def comment_magic_commands(script_content: str) -> str:
    
    lines = script_content.splitlines()
    commented_lines = []
    for line in lines:
        # Check for magic commands, shell commands, or direct execution commands
        if <IFMASK>:
            commented_lines.append(f""# {line}"")  # Comment the line
        else:
            commented_lines.append(line)  # Keep the line unchanged
    return ""\n"".join(commented_lines) <ANS> ",false,"re.match(r'^\s*(!|%|pip|apt-get|curl|conda)', line.strip())",not script_content.startswith('main') or script_content.endswith('main') or script_content.endswith,63.42
"def to_native_types(obj: Any, resolve: bool = True, throw_on_missing: bool = True, enum_to_str: bool = True) -> Any:
    

    # convert dataclass to structured config
    if <IFMASK>:
        # huggingface objects have a to_dict method, we prefer that
        obj = obj.to_dict()
    elif is_dataclass(obj):
        # we go through structured config instead and hope for the best
        obj = om.to_container(obj)

    if isinstance(obj, DictConfig) or isinstance(obj, ListConfig):
        obj = om.to_container(obj, resolve=resolve, throw_on_missing=throw_on_missing, enum_to_str=enum_to_str) <ANS> ",true,"hasattr(obj, ""to_dict"")","isinstance(obj, DictConfig) and isinstance(obj, DictConfig) and (not isinstance(obj, DictConfig",67.04
"def detect_mutating_keywords(sql: str) -> list[str]:
    
    matched = []

    if <IFMASK>:
        matched.append('DDL')

    if PERMISSION_REGEX.search(sql):
        matched.append('PERMISSION')

    if SYSTEM_REGEX.search(sql):
        matched.append('SYSTEM') <ANS> ",true,DDL_REGEX.search(sql),"not isinstance(sql, str) or not isinstance(sql, str) or (not isinstance(sql, str))",65.73
"def _get_trace_filename(self) -> str:
        
        path_pattern = self.path_settings.path_pattern
        unique_id_type = self.path_settings.unique_id

        if <IFMASK>:
            unique_id = self.session_id
        elif unique_id_type == ""timestamp"":
            now = datetime.now()
            time_format = self.path_settings.timestamp_format
            unique_id = now.strftime(time_format)
        else:
            raise ValueError(
                f""Invalid unique_id type: {unique_id_type}. Expected 'session_id' or 'timestamp'."" <ANS> ",true,"unique_id_type == ""session_id""",self.session_id is None and self.path_settings.session_id is None and (not self.,68.44
"def print_curl(self, method: str, path: str, data: Optional[Dict[str, Any]] = None) -> None:
        
        curl_cmd = f
        
        if <IFMASK>:
            curl_cmd += f"" \\\n  -H 'Content-Type: application/json' \\\n  -d '{json.dumps(data)}'""
        
        print(""\nEquivalent curl command:"")
        print(curl_cmd)
        print() <ANS> ",false,data,self.method == 'POST' and self.path == path and (not self.headers) and (not self,61.65
"def _log_available_capabilities(self):
        
        capabilities = []
        if <IFMASK>:
            capabilities.append(""Mistral OCR"")
        if self.exiftool_available:
            capabilities.append(""Exiftool metadata extraction"")
        if self.openai_client:
            capabilities.append(""OpenAI image description"")

        if capabilities:
            logger.info(f""Image converter initialized with: {', '.join(capabilities)}"") <ANS> ",true,self.mistral_client,self.use_uv and self.use_bin_type and (not self.use_uv) and (,57.06
"def _run(self, **kwargs) -> str:
        
        if <IFMASK>:
            return (
                ""No MCP server is currently active. ""
                ""Use connect_to_mcp_server to connect to a server.""
            )
        return f""Currently active MCP server: {self.server_manager.active_server}"" <ANS> ",true,not self.server_manager.active_server,not self.server_manager.is_alive() and self.server_manager.is_alive() and (,67.06
"def load_images_from_directory(image_dir, valid_extensions=('.jpg', '.jpeg', '.png', '.webp')):
    
    images_dict = {}

    if <IFMASK>:
        raise ValueError(f""Directory {image_dir} does not exist"")

    for filename in os.listdir(image_dir):
        if filename.lower().endswith(valid_extensions):
            image_path = os.path.join(image_dir, filename)
            try:
                image = Image.open(image_path).convert(""RGB"")
                images_dict[image_path] = image <ANS> ",false,not os.path.exists(image_dir),not image_file_to_load.name.startswith(f'-{filename}') or not image_file_,60.91
"def _update_component_states(self, states: dict[str, Any]):
        
        with self._lock:
            logger.info(""[STATE] Updating states"")
            for component_id, new_value in states.items():
                old_value = self._component_states.get(component_id)

                cleaned_new_value = clean_nan_values(new_value)
                cleaned_old_value = clean_nan_values(old_value)

                if <IFMASK>:
                    self._component_states[component_id] = cleaned_new_value
                    logger.info(f""[STATE] State changed for {component_id=}"")
                    if logger.isEnabledFor(logging.DEBUG):
                        logger.debug(f""[STATE]  - {cleaned_old_value=}\n  - {cleaned_new_value=}"") <ANS> ",false,cleaned_old_value != cleaned_new_value,self._is_updated_after(old_value) or self._is_updated_after(new_value,66.92
"def safe_save_model_for_hf_trainer(trainer: transformers.Trainer, output_dir: str):
    

    if <IFMASK>:
        torch.cuda.synchronize()
        trainer.save_model(output_dir)
        return

    state_dict = trainer.model.state_dict()
    if trainer.args.should_save:
        cpu_state_dict = {key: value.cpu() for key, value in state_dict.items()}
        del state_dict <ANS> ",false,trainer.deepspeed,not os.path.exists(output_dir) or not os.path.exists(output_dir) or,75.14
"def load_model(self) -> None:
        
        # TODO: add mps (apple metal) support, currently cant benchmark mps device accurately for energy
        if <IFMASK>:
            nexa_model = NexaTextInference(model_path=self.config.model, device=""gpu"", **self.config.model_kwargs)
        elif self.config.device == ""cpu"":
            nexa_model = NexaTextInference(model_path=self.config.model, device=""cpu"", **self.config.model_kwargs)
        else:
            raise ValueError(f""Invalid device: {self.config.device}"")
        
        self.pretrained_model = nexa_model.model <ANS> ",true,"self.config.device == ""cuda"" or self.config.device == ""mps""",self.config.model_type == 'auto' and self.config.auto_generate and (not self.,56.70
"def switch_applications(self, app_code):
        
        if <IFMASK>:
            return f""import pyautogui; import time; pyautogui.hotkey('command', 'space', interval=0.5); pyautogui.typewrite({repr(app_code)}); pyautogui.press('enter'); time.sleep(1.0)""
        elif self.platform == ""linux"":
            return UBUNTU_APP_SETUP.replace(""APP_NAME"", app_code)
        elif self.platform == ""windows"":
            return f""import pyautogui; import time; pyautogui.hotkey('win', 'd', interval=0.5); pyautogui.typewrite({repr(app_code)}); pyautogui.press('enter'); time.sleep(1.0)"" <ANS> ",true,"self.platform == ""darwin""",self.platform == 'darwin' and self.platform == 'darwin' and (not self.platform.startswith(',53.05
"def _merge_data(self, existing: list[dict] | None, new_data: list[dict], key_field: str) -> list[dict]:
        
        if <IFMASK>:
            return new_data

        # Create a set of existing keys for O(1) lookup
        existing_keys = {item[key_field] for item in existing}

        # Only add items that don't exist yet
        merged = existing.copy()
        merged.extend([item for item in new_data if item[key_field] not in existing_keys]) <ANS> ",false,not existing,"self.merge_cells and isinstance(self.merge_cells, dict) and isinstance(self.merge_cells",68.29
"def check_hub_revision_exists(training_args: SFTConfig | GRPOConfig):
    
    if <IFMASK>:
        if training_args.push_to_hub_revision is True:
            # First check if the revision exists
            revisions = [rev.name for rev in list_repo_refs(training_args.hub_model_id).branches]
            # If the revision exists, we next check it has a README file
            if training_args.hub_model_revision in revisions:
                repo_files = list_repo_files(
                    repo_id=training_args.hub_model_id,
                    revision=training_args.hub_model_revision, <ANS> ",true,repo_exists(training_args.hub_model_id),not (args.push_to_hub_revision and args.hub_model_id in checks_on_,57.19
"def __update_session_state(self, session: Session, event: Event) -> None:
    
    if <IFMASK>:
      return
    for key, value in event.actions.state_delta.items():
      if key.startswith(State.TEMP_PREFIX):
        continue
      session.state.update({key: value}) <ANS> ",false,not event.actions or not event.actions.state_delta,self.session.in_transaction() and (not self.in_transaction()) and (not self.session.,69.38
"def restart(self) -> None:
        
        self._container.restart()
        if <IFMASK>:
            raise ValueError(f""Failed to restart container. Logs: {self._container.logs()}"") <ANS> ",true,"self._container.status != ""running""",self._container.logs and self._container.logs.should_stop() and (not self._container.logs,63.65
"def save_fid_stats(paths, batch_size, device, dims, num_workers=1):
    
    if <IFMASK>:
        raise RuntimeError(""Invalid path: %s"" % paths[0])

    if os.path.exists(paths[1]):
        raise RuntimeError(""Existing output file: %s"" % paths[1])

    block_idx = InceptionV3.BLOCK_INDEX_BY_DIM[dims]

    model = InceptionV3([block_idx]).to(device) <ANS> ",false,not os.path.exists(paths[0]),batch_size < 0 or batch_size > 1 or batch_size > 2 or batch_size > 3 or,67.01
"def _update_macos_platform_uuid(self, new_ids):
        
        try:
            uuid_file = ""/var/root/Library/Preferences/SystemConfiguration/com.apple.platform.uuid.plist""
            if <IFMASK>:
                # Use sudo to execute plutil command
                cmd = f'sudo plutil -replace ""UUID"" -string ""{new_ids[""telemetry.macMachineId""]}"" ""{uuid_file}""'
                result = os.system(cmd)
                if result == 0:
                    print(f""{Fore.GREEN}{EMOJI['SUCCESS']} {self.translator.get('reset.macos_platform_uuid_updated')}{Style.RESET_ALL}"")
                else:
                    raise Exception(f""{Fore.RED}{EMOJI['ERROR']} {self.translator.get('reset.failed_to_execute_plutil_command')}{Style.RESET_ALL}"")
        except Exception as e: <ANS> ",false,os.path.exists(uuid_file),self.wait_for_completion is not None and self.deferrable and (not self.wait_for_completion,68.22
"def process_audio_file(audio_path, text, polyphone):
    
    if <IFMASK>:
        print(f""audio {audio_path} not found, skipping"")
        return None
    try:
        audio_duration = get_audio_duration(audio_path)
        if audio_duration <= 0:
            raise ValueError(f""Duration {audio_duration} is non-positive."")
        return (audio_path, text, audio_duration)
    except Exception as e: <ANS> ",false,not Path(audio_path).exists(),"not isinstance(text, str) or not isinstance(text[0], str) or text[0] == ''",70.70
"def _search_by_url_sync(self, url: str, collection_name: str) -> Optional[List[str]]:
        
        client = self._get_milvus_client()
        
        logger.debug(f""Querying collection: {collection_name} for URL: {url}"")
        res = client.query(
            collection_name=collection_name,
            filter=f""url == '{url}'"",
            limit=1,
            output_fields=[""url"", ""text"", ""name"", ""site""],
        )
        
        if <IFMASK>:
            logger.warning(f""No item found for URL: {url}"")
            return None
        
        item = res[0]
        txt = json.dumps(item[""text""])
        logger.info(f""Successfully retrieved item for URL: {url}"")
        return [item[""url""], txt, item[""name""], item[""site""]] <ANS> ",false,len(res) == 0,self.query_ids and self.query_task_ids and self.query_task_group_ids and,62.56
"def get_all_pages(s3_client, document_files):
    
    file_contents = {}

    # First, collect all file paths and their document info
    for file_path in tqdm(document_files, desc=""Loading document files""):
        lines = load_document_file(s3_client, file_path)
        if <IFMASK>:
            logger.warning(f""Empty or invalid file: {file_path}"")
            continue

        # Parse each line for document info
        documents = []
        for i, line in enumerate(lines):
            doc_info = get_document_info_from_line(line, file_path, i)
            # Always add an entry for each line, even if None, to preserve line alignment <ANS> ",false,not lines,doc_info is not None and (not line.startswith(' ') or line.startswith(' ') or line.startswith,61.38
"def _store_conversation_turn(
        self, prompt: str, response_content: Dict[str, Any], call_type: str
    ):
        
        if <IFMASK>:
            _conversation_history[self.request_id] = []

        # Add user message
        _conversation_history[self.request_id].append(
            {""role"": ""user"", ""content"": prompt}
        )

        # Add AI response based on call type <ANS> ",true,self.request_id not in _conversation_history,call_type == 'response' and self.response_body_buf and self.response_content_buf and,59.95
"def resolve_ref(ref_string, current_doc):
      
      parts = ref_string.split(""/"")
      if <IFMASK>:
        raise ValueError(f""External references not supported: {ref_string}"")

      current = current_doc
      for part in parts[1:]:
        if part in current:
          current = current[part]
        else:
          return None  # Reference not found <ANS> ",false,"parts[0] != ""#""","isinstance(current_doc, dict) and 'name' in current_doc and 'value' in current_doc",60.92
"def toggle_light(light_name: str, state: bool) -> dict[str, Any]:
    
    if <IFMASK>:
        return {""error"": ""Bridge not connected"", ""success"": False}
    try:
        result = bridge.set_light(light_name, ""on"", state)
        return {
            ""light"": light_name,
            ""set_on_state"": state,
            ""success"": True,
            ""phue2_result"": result, <ANS> ",false,not (bridge := _get_bridge()),not result.failed() or not result.skipped() or not result.failed() or not result.skipped() or,61.19
"def list_files(relative_path: str = """") -> List[str]:
    
    path = (CONTEXT_PATH / relative_path).resolve()
    if <IFMASK>:
        return [f""Access denied: {relative_path}""]
    if not path.exists() or not path.is_dir():
        return [f""Not a directory: {relative_path}""]
    return os.listdir(path) <ANS> ",false,not str(path).startswith(str(CONTEXT_PATH)),not path.is_dir() or not path.is_symlink() or not path.is_symlink() or,79.13
"def process_query(ner_query):
            
            candidate_entities = self.ner.invoke(ner_query, **kwargs)
            for candidate_entity in candidate_entities:
                query_type = candidate_entity.get_entity_first_type_or_un_std()

                ner_id = f""{candidate_entity.entity_name}_{query_type}""
                if <IFMASK>:
                    ner_maps[ner_id] = {
                        ""candidate"": candidate_entity,
                        ""query"": ner_query,
                        ""query_type"": query_type,
                    } <ANS> ",false,ner_id not in ner_maps,not self.selected_fields and self.selected_fields[0].get('extra') != 'allow' and,53.71
"def _generate_context(self) -> str:
        
        context_parts = []

        for idx, intent in enumerate(self.intents.values(), 1):
            description = (
                f""{idx}. Intent: {intent.name}\nDescription: {intent.description}""
            )

            if <IFMASK>:
                examples = ""\n"".join(f""- {example}"" for example in intent.examples)
                description += f""\nExamples:\n{examples}""

            if intent.metadata:
                metadata = ""\n"".join(
                    f""- {key}: {value}"" for key, value in intent.metadata.items()
                )
                description += f""\nAdditional Information:\n{metadata}"" <ANS> ",false,intent.examples,self.context_type == 'string' and self.context_type == 'dict' and (not self.,50.40
"def from_orm_with_collection_mapping(cls, obj):
        
        # Convert to dict and filter out SQLAlchemy internal attributes
        obj_dict = {k: v for k, v in obj.__dict__.items() if not k.startswith(""_"")}

        # Map the readable_collection_id to collection if needed
        if <IFMASK>:
            obj_dict[""collection""] = obj.readable_collection_id

        return cls.model_validate(obj_dict) <ANS> ",true,"hasattr(obj, ""readable_collection_id"")","isinstance(obj, dict) and isinstance(obj, list) and (not isinstance(obj, dict)) and (",62.74
"def _replace_agentlist_placeholder(cls: Type[""GroupManagerSelectionMessageContextStr""], v: Any) -> Union[str, Any]:  # noqa: N805
        
        if <IFMASK>:
            if ""{agentlist}"" in v:
                return v.replace(""{agentlist}"", ""<<agent_list>>"")  # Perform the replacement
            else:
                return v  # If no replacement is needed, return the original value
        return """" <ANS> ",true,"isinstance(v, str)","not isinstance(v, Group) or not isinstance(v, Group) or v.is_template(template)",53.60
"def from_pretrained0802(self, config_path, model_path):
        
        model = self.from_hparams0802(config_path)
        state_dict_raw = torch.load(model_path, map_location=""cpu"")['state_dict']
        state_dict = dict()
        for k, v in state_dict_raw.items():
            if <IFMASK>:
                state_dict[k] = v
        # if isinstance(model.feature_extractor, EncodecFeatures):
        #     encodec_parameters = {
        #         ""feature_extractor.encodec."" + key: value
        #         for key, value in model.feature_extractor.encodec.state_dict().items()
        #     }
        #     state_dict.update(encodec_parameters)
        model.load_state_dict(state_dict) <ANS> ",false,k.startswith('backbone.') or k.startswith('head.') or k.startswith('feature_extractor.'),not self.feature_extractor.can_be_positional() or not self.feature_extractor.has_positional,60.27
"def from_mcp_tool_result(
        cls, result: CallToolResult, tool_use_id: str
    ) -> types.Content:
        
        if <IFMASK>:
            function_response = {""error"": str(result.content)}
        else:
            function_response_parts = mcp_content_to_google_parts(result.content)
            function_response = {""result"": function_response_parts}

        function_response_part = types.Part.from_function_response(
            name=tool_use_id,
            response=function_response, <ANS> ",true,result.isError,"not isinstance(result.content, str) or not isinstance(result.content, list) or not isinstance(result",62.22
"def load_user_config():
    
    if <IFMASK>:
        file_content = open(f'{args.model_dir}/config-user.yaml', 'r').read().strip()

        if file_content:
            user_config = yaml.safe_load(file_content)
        else:
            user_config = {}
    else:
        user_config = {} <ANS> ",false,Path(f'{args.model_dir}/config-user.yaml').exists(),args.verbose_feature_names_out is None and args.verbose_feature_names_out == 'all,63.43
"def shopping_get_sku_latest_review_author(sku: str) -> str:
    
    header = {
        ""Authorization"": f""Bearer {shopping_get_auth_token()}"",
        ""Content-Type"": ""application/json"",
    }
    response = requests.get(
        f""{SHOPPING}/rest/V1/products/{sku}/reviews"", headers=header
    )
    assert response.status_code == 200
    response_obj = response.json()
    if <IFMASK>:
        return """"
    author: str = response_obj[-1][""nickname""]
    return author <ANS> ",false,len(response_obj) == 0,"not isinstance(sku, str) or not isinstance(sku_str, str) or (not isinstance(sku_",62.25
"def __init__(self):
        
        self.reader = None
        # Determine best available device
        self.device = ""cpu""
        if <IFMASK>:
            self.device = ""cuda""
        elif (
            hasattr(torch, ""backends"")
            and hasattr(torch.backends, ""mps"")
            and torch.backends.mps.is_available()
        ):
            self.device = ""mps""
        logger.info(f""OCR processor initialized with device: {self.device}"") <ANS> ",false,torch.cuda.is_available(),self.device == 'cpu' or self.device == 'mps' or self.device == 'cpu' or,73.44
"def add_tool(
        self,
        fn: Callable[..., Any],
        name: str | None = None,
        description: str | None = None,
        annotations: ToolAnnotations | None = None,
    ) -> Tool:
        
        tool = Tool.from_function(
            fn, name=name, description=description, annotations=annotations
        )
        existing = self._tools.get(tool.name)
        if <IFMASK>:
            if self.warn_on_duplicate_tools:
                logger.warning(f""Tool already exists: {tool.name}"")
            return existing
        self._tools[tool.name] = tool
        return tool <ANS> ",false,existing,"not isinstance(self.tool, str) or not isinstance(self.tool, list) or not isinstance(self",51.48
"def cleanup_temp_files(self):
        
        try:
            for part_index in list(self._part_working_dirs.keys()):
                self.cleanup_part_working_dir(part_index)
            if <IFMASK>:
                logger.info(f""cleanup temp files: {self.working_dir}"")
                shutil.rmtree(self.working_dir)
        except Exception:
            logger.exception(""Error cleaning up temporary files"") <ANS> ",true,self._is_temp_dir,self.cleanup_temp_files is None and self.cleanup_files is None and (not self.cleanup_,60.90
"def print_validation_result(result: dict, rel_path: Path):
    
    print(f""\nValidating: {rel_path}"")
    if <IFMASK>:
        print(f""Error: {result['error']}"")
    else:
        print(f""Duration: {result['duration']}"")
        print(f""Sample Rate: {result['sample_rate']} Hz"")
        print(f""Peak Amplitude: {result['peak_amplitude']}"")
        print(f""RMS Level: {result['rms_level']}"")
        print(f""DC Offset: {result['dc_offset']}"") <ANS> ",true,"""error"" in result",not result['result'] or not result['result'] or not result['result'] or not result['result'] or,71.14
"def get_api_key(self, context: SkillContext) -> Optional[str]:
        
        # Check agent config first
        agent_api_key = context.config.get(""api_key"")
        if <IFMASK>:
            logger.debug(f""Using agent-specific Venice API key for skill {self.name}"")
            return agent_api_key

        # Fallback to system config
        system_api_key = self.skill_store.get_system_config(""venice_api_key"")
        if system_api_key:
            logger.debug(f""Using system Venice API key for skill {self.name}"")
            return system_api_key <ANS> ",false,agent_api_key,self.config.get('api_key') is None or self.config.get('system_API_KEY,64.32
"def process_revision_directives(context, revision, directives):
    
    for directive in directives:
        if <IFMASK>:
            # Get the current timestamp
            timestamp = time.strftime(""%Y%m%d%H%M%S"")
            # Modify the revision ID to include the timestamp
            directive.rev_id = f""{timestamp}_{directive.rev_id}"" <ANS> ",false,"isinstance(directive, ops.MigrationScript)","revision is None and directives is None and (revision is None) and (not isinstance( directives, str)) and (",60.96
"def delete_subscription_by_stripe_id(db_session: Session, stripe_subscription_id: str) -> None:
    
    subscription = get_subscription_by_stripe_id(db_session, stripe_subscription_id)
    if <IFMASK>:
        logger.warning(
            f""Subscription not found for stripe_subscription_id={stripe_subscription_id} during delete attempt.""
        )
        return

    db_session.delete(subscription)
    db_session.flush()
    logger.info( <ANS> ",false,not subscription,not manager.app.conf.result_backend.startswith('redis') and manager.app.conf.result_,77.19
"def _safe_save(self, output_dir: str):
        
        state_dict = self.model.state_dict()
        if <IFMASK>:
            cpu_state_dict = {
                key: value.cpu()
                for key, value in state_dict.items()
            }
            del state_dict
            self._save(output_dir, state_dict=cpu_state_dict) <ANS> ",true,self.args.should_save,self.args.save_format == 'json' or self.args.save_format == 'yaml' or,71.05
"def __str__(self):
        
        error_message = ""({0})\n"" ""Reason: {1}\n"".format(self.status, self.reason)
        if <IFMASK>:
            error_message += ""HTTP response headers: {0}\n"".format(self.headers)

        if self.body:
            error_message += ""HTTP response body: {0}\n"".format(self.body)

        return error_message <ANS> ",true,self.headers,self.body is not None and self.body != '' and (not self.body) and (not self.,63.56
"def execute_method(self, method: Union[str, bytes], *args, **kwargs):
        
        if <IFMASK>:
            print(f""[DP={self.vllm_dp_rank},TP={self.vllm_tp_rank}] execute_method: {method if isinstance(method, str) else 'Callable'}"")
        return self.rollout.execute_method(method, *args, **kwargs) <ANS> ",false,"self.vllm_tp_rank == 0 and method != ""execute_model""",self.wait_for_completion is not None and self.deferrable and (not self.wait_for_completion,74.43
"def visualize_scroll(self, direction: str, clicks: int, img_base64: str) -> None:
        
        if (
            not self.agent.save_trajectory
            
            <IFMASK>
        ):
            return

        try:
            # Use the visualization utility
            img = visualize_scroll(direction, clicks, img_base64)

            # Save the visualization <ANS> ",true,not self.agent.save_trajectory,self.agent.save_trajectoryprepadded()'s call and self.agent.save_trajectory()'s call,50.72
"def format_search_results(self, results: list[tuple[BaseTool, str, float]]) -> str:
        

        # Only show top_k results
        results = results

        formatted_output = ""Search results\n\n""

        for i, (tool, server_name, score) in enumerate(results):
            # Format score as percentage
            if <IFMASK>:
                score_pct = f""{score * 100:.1f}%""
                logger.info(f""{i}: {tool.name} ({score_pct} match)"")
            formatted_output += f""[{i + 1}] Tool: {tool.name} ({score_pct} match)\n""
            formatted_output += f""    Server: {server_name}\n""
            formatted_output += f""    Description: {tool.description}\n\n""

        # Add footer with information about how to use the results
        formatted_output += ( <ANS> ",false,i < 5,len(results) == 1 and len(results) == 1 and (len(results) == 1) and (,69.36
"def _is_supported(self, file_path: str) -> bool:
        
        ext = Path(file_path).suffix.lower()
        if <IFMASK>:
            logger.warning(f""Unsupported file extension: {ext} for file: {file_path}"")
            return False
        return True <ANS> ",true,ext not in self.SUPPORTED_EXTENSIONS,not self.is_dir() or not self.is_symlink() or self.is_symlink() or self,59.38
"def generate_diff_html(a, b):
    
    seq_matcher = SequenceMatcher(None, a, b)
    output_html = """"
    for opcode, a0, a1, b0, b1 in seq_matcher.get_opcodes():
        if <IFMASK>:
            output_html += a[a0:a1]
        elif opcode == ""insert"":
            output_html += f""<span class='added'>{b[b0:b1]}</span>""
        elif opcode == ""delete"":
            output_html += f""<span class='removed'>{a[a0:a1]}</span>""
        elif opcode == ""replace"":
            output_html += f""<span class='removed'>{a[a0:a1]}</span><span class='added'>{b[b0:b1]}</span>""
    return output_html <ANS> ",false,"opcode == ""equal""","not isinstance(a, (list, tuple)) and isinstance(b, (list, tuple)) and (not isinstance",70.39
"def _build_docker_image(self):
    
    if <IFMASK>:
      raise ValueError('Docker path is not set.')
    if not os.path.exists(self.docker_path):
      raise FileNotFoundError(f'Invalid Docker path: {self.docker_path}')

    logger.info('Building Docker image...')
    self._client.images.build(
        path=self.docker_path,
        tag=self.image, <ANS> ",true,not self.docker_path,self.docker_path.is_dir() or self.docker_path.is_symlink()) or self.,73.00
"def _create_relevant_node(
        self, node: Dict[str, Any], relevance_score: float, reason: str
    ) -> Dict[str, Any]:
        
        relevant_node = {
            ""id"": node[""id""],
            ""name"": node[""name""],
            ""type"": node[""type""],
            ""relevance_score"": relevance_score,
            ""reason"": reason,
            ""children"": [],
        }

        for field in [""file_path"", ""start_line"", ""end_line"", ""relationship""]:
            if <IFMASK>:
                relevant_node[field] = node[field]

        return relevant_node <ANS> ",true,field in node,self.is_mapped(node) and self.is_mapped(node) and (not self.is_,54.30
"def get_speaker_selection_result(self, groupchat: ""GroupChat"") -> Optional[Union[Agent, str]]:
        
        if <IFMASK>:
            # Find the agent by name in the groupchat
            for agent in groupchat.agents:
                if agent.name == self.agent_name:
                    return agent
            raise ValueError(f""Agent '{self.agent_name}' not found in groupchat."")
        elif self.speaker_selection_method is not None:
            return self.speaker_selection_method
        elif self.terminate is not None and self.terminate: <ANS> ",true,self.agent_name is not None,self.wait_for_completion is not None and self.wait_for_completion.done() and (not,70.98
"def file_uri_to_path(file_uri: str) -> Tuple[str | None, str]:
    
    parsed = urlparse(file_uri)
    if <IFMASK>:
        raise ValueError(f""Not a file URL: {file_uri}"")

    netloc = parsed.netloc if parsed.netloc else None
    path = os.path.abspath(url2pathname(parsed.path))
    return netloc, path <ANS> ",false,"parsed.scheme != ""file""",not os.path.isfile(file_uri) or not os.path.isfile(path) or (not,71.58
"def context(self) -> ""Context"":
        
        # First try instance context
        if <IFMASK>:
            return self._context

        try:
            # Fall back to global context if available
            from mcp_agent.core.context import get_current_context

            return get_current_context()
        except Exception as e: <ANS> ",true,self._context is not None,self.context.closed is False and self.context.server.address is None and (self.context.server,62.24
"def update_presigned_url(self, presigned_url, base_url):
        
        #To Do: If Proxy URL has domain name how do we handle such cases
        
        presigned_parts = urlparse(presigned_url)
        base_parts = urlparse(base_url)
        # Check if base_url contains localhost or an IP address
        if <IFMASK>:
            new_netloc = base_parts.hostname  # Extract domain from base_url
            if base_parts.port:  # Add port if present in base_url
                new_netloc += f"":{base_parts.port}""
            updated_parts = presigned_parts._replace(netloc=new_netloc)
            return urlunparse(updated_parts)
        return presigned_url <ANS> ",false,"re.match(r'^(localhost|\d{1,3}(\.\d{1,3}){3})$', base_parts.hostname)",self.proxy_enabled_for_service_instance and self.proxy_enabled_for_service_instance.,56.09
"def resolve(self, override: ModelSettings | None) -> ModelSettings:
        
        if <IFMASK>:
            return self

        changes = {
            field.name: getattr(override, field.name)
            for field in fields(self)
            if getattr(override, field.name) is not None
        }
        return replace(self, **changes) <ANS> ",true,override is None,self.config.option.verbose >= 1 and override.verbose >= 1 and (not override.config.option.,46.63
"def _format_child_blocks(
        self, block_content: dict, block: dict, block_type: str, page_breadcrumbs: List[Breadcrumb]
    ) -> str:
        
        if <IFMASK>:
            title = block_content.get(""title"", ""Untitled Page"")
            return f""È¶ÉÊê´ **[{title}]** (Child Page)""
        else:  # child_database
            return self._format_child_database_block(block_content, block, page_breadcrumbs) <ANS> ",false,"block_type == ""child_page""","self.wait_for_completion is not None and self.wait_for_completion not in (None, False",65.52
"def get_api_version(cls) -> str:
        
        logger.debug(""Retrieving DeepSeek Azure API version from config"")
        provider_config = CONFIG.llm_endpoints.get(""deepseek_azure"")
        if <IFMASK>:
            logger.debug(f""DeepSeek Azure API version: {provider_config.api_version}"")
            return provider_config.api_version
        logger.warning(""DeepSeek Azure API version not found in config"")
        return None <ANS> ",false,provider_config and provider_config.api_version,not cls.metadata.name and (not cls.metadata.namespace) and (not cls.metadata.name_,57.23
"def _update_analyze_status_failed(self, doc_id: int) -> None:
        
        try:
            with self._repository._db.session() as session:
                document = session.get(self._repository.model, doc_id)
                if <IFMASK>:
                    document.analyze_status = ProcessStatus.FAILED
                    session.commit()
                    logger.debug(f""Updated analyze status for document {doc_id} to FAILED"")
                else:
                    logger.warning(f""Document not found with id: {doc_id}"")
        except Exception as e:
            logger.error(f""Error updating document analyze status: {str(e)}"") <ANS> ",false,document,"doc_id not in doc_ids_to_update_result.get(doc_id, False) or",63.14
"def matches(self, uri: str) -> dict[str, Any] | None:
        
        # Convert template to regex pattern
        pattern = self.uri_template.replace(""{"", ""(?P<"").replace(""}"", "">[^/]+)"")
        match = re.match(f""^{pattern}$"", uri)
        if <IFMASK>:
            return match.groupdict()
        return None <ANS> ",false,match,uri.startswith('mongodb://') and uri.endswith('mongodb:///') and (not uri.startswith('mongodb://')),60.63
"def _retry_if_not_cancelled_and_failed(retry_state):
    
    if <IFMASK>:
        exception = retry_state.outcome.exception()
        # Don't retry on CancelledError
        if isinstance(exception, asyncio.CancelledError):
            logger.debug(""Operation was cancelled, not retrying"")
            return False
        # Retry on network related errors
        if isinstance(
            exception, httpx.HTTPError | ConnectionError | ValueError | TimeoutError <ANS> ",false,retry_state.outcome.failed,"isinstance(exception, (aiohttp.ClientConnectorError, TimeoutError)) and exception.timeout is not None and (exception.",69.13
"def validate_config_values(self):
        
        for key, value in self.__dict__.items():
            if <IFMASK>:
                raise ValueError(f""Value for '{key}' must not be a dictionary (depth 0 only)"")
        return self <ANS> ",true,"isinstance(value, dict)","self.validate_assignment is not None and (not isinstance(self.validate_assignment, str)) and (not",64.69
"def save_report(report, filename=None):
    
    if <IFMASK>:
        filename = f""test_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt""
    with open(filename, 'w') as file:
        file.write(report)
    print(f""Report saved to {os.path.abspath(filename)}"") <ANS> ",true,filename is None,not os.path.exists(filename) or not os.path.isdir(filename) or (not os.,70.05
"def initialize_cache(self, num_gpu_blocks: int, num_cpu_blocks: int) -> None:
        

        # NOTE: We log here to avoid multiple logs when number of workers is
        # greater than one. We could log in the engine, but not all executors
        # have GPUs.
        logger.info(""# GPU blocks: %d, # CPU blocks: %d"", num_gpu_blocks, num_cpu_blocks)

        self.cache_config.num_gpu_blocks = num_gpu_blocks
        self.cache_config.num_cpu_blocks = num_cpu_blocks

        if <IFMASK>:
            print(
                f'before init cache memory allocated: {torch.cuda.memory_allocated() / 1e9}GB, reserved: {torch.cuda.memory_reserved() / 1e9}GB'
            )
        self.worker.initialize_cache(num_gpu_blocks=num_gpu_blocks, num_cpu_blocks=num_cpu_blocks)
        if torch.distributed.get_rank() == 0:
            print(
                f'after init cache memory allocated: {torch.cuda.memory_allocated() / 1e9}GB, reserved: {torch.cuda.memory_reserved() / 1e9}GB'
            ) <ANS> ",false,torch.distributed.get_rank() == 0,self.use_cuda or self.use_cpu or self.use_rng_state or self.use_,57.88
"def get_db_context():
    
    if <IFMASK>:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=""Database manager not initialized"",
        )
    try:
        yield _db_manager
    except Exception as e:
        logger.error(f""Database operation failed: {str(e)}"") <ANS> ",false,not _db_manager,not manager.app.conf.result_backend.startswith('redis') and manager.app.conf.result_,79.19
"def get_processor(cls, file_type: FileType) -> Type[BaseFileProcessor]:
        
        if <IFMASK>:
            cls.init()
        print(f""Current registered processors: {cls._processors}"")
        if file_type not in cls._processors:
            raise ValueError(f""No processor found for {file_type}"")
        return cls._processors[file_type] <ANS> ",false,not cls._initialized,not file_type.is_file() and (not file_type.is_symlink()) and (not file,73.15
"def tf_roll_out(
    agent: Agent, env: ScriptBrowserEnv, config_file: str
) -> list[Any]:
    
    obs, state_info = env.reset(options={""config_file"": config_file})

    trajectory: list[Any] = [{""observation"": obs, ""info"": state_info}]
    while True:
        action = agent.next_action(
            trajectory=trajectory, intent="""", meta_data={}
        )
        trajectory.append(action)
        if <IFMASK>:
            break

        # preceed to next action
        obs, reward, terminated, truncated, info = env.step(action)
        state_info = {""observation"": obs, ""info"": info}
        trajectory.append(state_info)

    return trajectory <ANS> ",false,"action[""action_type""] == ActionTypes.STOP",not config_file.endswith('.json') or not config_file.endswith('.xml') or not config_file,67.47
"def show_permission_dialog(code: str, action_description: str):
    
    if <IFMASK>:
        result = os.system(
            f'osascript -e \'display dialog ""Do you want to execute this action?\n\n{code} which will try to {action_description}"" with title ""Action Permission"" buttons {{""Cancel"", ""OK""}} default button ""OK"" cancel button ""Cancel""\''
        )
        return result == 0
    elif platform.system() == ""Linux"":
        result = os.system(
            f'zenity --question --title=""Action Permission"" --text=""Do you want to execute this action?\n\n{code}"" --width=400 --height=200'
        ) <ANS> ",false,"platform.system() == ""Darwin""",not result.startswith('Do you want to execute this action?\n\n{action_description}') and (not,56.56
"def __launch_kwargs(self):
        
        launch_kwargs = {'headless': self.headless, 'ignore_default_args': self.harmful_default_args, 'channel': 'chrome' if self.real_chrome else 'chromium'}
        if <IFMASK>:
            launch_kwargs.update({'args': self.__set_flags(), 'chromium_sandbox': True})

        return launch_kwargs <ANS> ",true,self.stealth,self.real_chrome and self.harmful_default_args and self.channel and self.kernel_,61.53
"def clean_response(cls, content: str) -> Dict[str, Any]:
        
        cleaned = re.sub(r""```(?:json)?\s*"", """", content).strip()
        match = re.search(r""(\{.*\})"", cleaned, re.S)
        if <IFMASK>:
            logger.error(""Failed to parse JSON from content: %r"", content)
            raise ValueError(""No JSON object found in response"")
        return json.loads(match.group(1)) <ANS> ",false,not match,not content or not content[0] or not content[1] or not content[2] or not content[,63.43
"def make_request(prompt):
    
    payload = {""input"": prompt}
    
    try:
        response = requests.post(
            API_URL,
            json=payload,
            stream=True
        )
        
        print(f""\nMaking request with prompt: '{prompt}'\n"")
        print(""Raw response:"")
        for line in response.iter_lines():
            if <IFMASK>:
                print(line.decode('utf-8'))
                
    except Exception as e:
        print(f""Error making request: {e}"") <ANS> ",false,line,not response.ok() or response.body or response.status_code != 200 or (not response.content or,51.99
"def _merge_audio(files, output: str) -> bool:
    
    try:
        # Create an empty audio segment
        combined = AudioSegment.empty()
        silence = AudioSegment.silent(duration=100)  # 100ms silence
        
        # Add audio files one by one
        for file in files:
            audio = AudioSegment.from_wav(file)
            combined += audio + silence
        combined += silence
        combined.export(output, format=""wav"", parameters=[""-acodec"", ""pcm_s16le"", ""-ar"", ""16000"", ""-ac"", ""1""])
        
        if <IFMASK>:
            rprint(f""[red]Output file size is 0"")
            return False
            
        rprint(f""[green]Successfully merged audio files"")
        return True
        
    except Exception as e:
        rprint(f""[red]Failed to merge audio: {str(e)}"") <ANS> ",false,os.path.getsize(output) == 0,"len(files) == 1 and len(files[0]) == 1 and (not isinstance(files[0],",64.50
"def pop_context():
    
    if <IFMASK>:
        if logger.isEnabledFor(logging.DEBUG):
            logger.debug(f""Popping context for atom {ctx.atom_name}"")
        _context_stack.pop()
    else:
        logger.warning(""[DAG] Attempted to pop context, but stack was empty"") <ANS> ",false,_context_stack,ctx.executor_memory_usage == 'memory' and ctx.executor_memory_usage != 'executor' and,61.56
"def remove_obsolete_ckpt(path: str, global_step: int, save_limit: int = -1, directory_format: str = ""global_step_{}""):
    
    if <IFMASK>:
        return

    if not os.path.exists(path):
        return

    pattern = re.escape(directory_format).replace(r""\{\}"", r""(\d+)"")
    ckpt_folders = []
    for folder in os.listdir(path): <ANS> ",true,save_limit <= 0,global_step < 0 or (global_step < 1) or (global_step < 2) or (global,65.54
"def _get_endpoint_config(self):
        
        endpoint_config = CONFIG.retrieval_endpoints.get(self.endpoint_name)
        
        if <IFMASK>:
            error_msg = f""No configuration found for endpoint {self.endpoint_name}""
            logger.error(error_msg)
            raise ValueError(error_msg)
        
        # Verify this is a Qdrant endpoint
        if endpoint_config.db_type != ""qdrant"":
            error_msg = f""Endpoint {self.endpoint_name} is not a Qdrant endpoint (type: {endpoint_config.db_type})""
            logger.error(error_msg) <ANS> ",true,not endpoint_config,self.endpoint_name == 'qdrant' and self.endpoint_config.get('db_type') != ',62.74
"def _safe_save(self, output_dir: str):
        
        if <IFMASK>:
            torch.cuda.synchronize()
            self.save_model(output_dir)
            return
    
        state_dict = self.model.state_dict()
        if self.args.should_save:
            cpu_state_dict = {
                key: value.cpu() <ANS> ",true,self.deepspeed,self.args.should_save and (not self.args.should_save) and (not self.args,74.76
"def validate_fold_input(fold_input: folding_input.Input):
  
  for i, chain in enumerate(fold_input.protein_chains):
    if <IFMASK>:
      raise ValueError(f'Protein chain {i + 1} is missing unpaired MSA.')
    if chain.paired_msa is None:
      raise ValueError(f'Protein chain {i + 1} is missing paired MSA.')
    if chain.templates is None:
      raise ValueError(f'Protein chain {i + 1} is missing Templates.')
  for i, chain in enumerate(fold_input.rna_chains):
    if chain.unpaired_msa is None:
      raise ValueError(f'RNA chain {i + 1} is missing unpaired MSA.') <ANS> ",false,chain.unpaired_msa is None,"isinstance(fold_input.input_specs, dict) and 'input_specs' in fold_input.input",58.87
"def get_unrealspeech_skill(
    name: str,
    store: SkillStoreABC,
) -> UnrealSpeechBaseTool:
    
    if <IFMASK>:
        if name not in _cache:
            _cache[name] = TextToSpeech(
                skill_store=store,
            )
        return _cache[name]
    else:
        raise ValueError(f""Unknown UnrealSpeech skill: {name}"") <ANS> ",true,"name == ""text_to_speech""",not name.startswith('s3') or name.startswith('s3-') or name.startswith('s3,55.23
"def handle_money(m: re.Match[str]) -> str:
    

    bill = ""dollar"" if m.group(2) == ""$"" else ""pound""
    coin = ""cent"" if m.group(2) == ""$"" else ""pence""
    number = m.group(3)

    multiplier = m.group(4)
    try:
        number = float(number)
    except:
        return m.group()

    if <IFMASK>:
        number *= -1

    if number % 1 == 0 or multiplier != """":
        text_number = f""{INFLECT_ENGINE.number_to_words(conditional_int(number))}{multiplier} {INFLECT_ENGINE.plural(bill, count=number)}""
    else:
        sub_number = int(str(number).split(""."")[-1].ljust(2, ""0""))

        text_number = f""{INFLECT_ENGINE.number_to_words(int(round(number)))} {INFLECT_ENGINE.plural(bill, count=number)} and {INFLECT_ENGINE.number_to_words(sub_number)} {INFLECT_ENGINE.plural(coin, count=sub_number)}"" <ANS> ",true,"m.group(1) == ""-""",not m.group(1) or not m.group(2) or not m.group(3) or,79.23
"def print_model_size(model: nn.Module, name: Optional[str] = None) -> None:
    
    if <IFMASK>:
        n_params, scale = _get_model_size(model, scale=""auto"")
        if name is None:
            name = model.__class__.__name__

        print(f""{name} contains {n_params:.2f}{scale} parameters."") <ANS> ",false,is_rank0(),name is None and len(model.parameters()) > 0 and (not name.startswith('_')) and (not name,54.01
"def _sanitize_input(self, args: tuple, kwargs: dict) -> dict:
            

            def sanitize_value(value):
                if <IFMASK>:
                    return value
                elif isinstance(value, list):
                    return [sanitize_value(item) for item in value]
                elif isinstance(value, dict):
                    return {key: sanitize_value(val) for key, val in value.items()}
                else:
                    return str(value)  # Convert non-standard types to string <ANS> ",false,"isinstance(value, (int, float, bool, str))",self.value is None and self.default is None and (self.default is not None) and (self.,59.08
"def cache_s3_files(dataset: Dataset, pdf_cache_location: str, num_proc: int = 32) -> Dataset:
    

    # Define the download function to use in parallel processing
    def cache_file(example):
        s3_path = example[""s3_path""]
        if <IFMASK>:
            # Download the file and cache it locally
            local_path = _cache_s3_file(s3_path, pdf_cache_location)
            return {""local_pdf_path"": local_path}
        return {""local_pdf_path"": None}

    # Map the caching function to the dataset (with parallelism if needed)
    dataset = dataset.map(cache_file, num_proc=num_proc, load_from_cache_file=False) <ANS> ",false,s3_path,not self.check_fn or self.check_fn(dataset) or self.check_fn(dataset_,66.42
"def delete_integration_credential(mapper, connection, target):
    
    if <IFMASK>:
        # Get the session
        session = Session.object_session(target)
        if session:
            # If we're in a session, use the session to delete the IntegrationCredential
            from airweave.models.integration_credential import IntegrationCredential

            credential = session.get(IntegrationCredential, target.integration_credential_id)
            if credential: <ANS> ",false,target.integration_credential_id,not credential and connection.login == '' and (not connection.password) and (not connection.host) and (,67.82
"def analyze_schema_types(filename: str) -> Counter:
    
    all_types = Counter()
    
    with open(filename) as f:
        for line in f:
            items = line.strip().split('\t')
            if (<IFMASK>):
                continue
            js = json.loads(items[1])
            try:
                all_types.update(extract_types(js))
            except json.JSONDecodeError:
                print(f""Warning: Could not parse JSON line: {line[:100]}..."")
                continue <ANS> ",false,len(items) < 2,not all_types and (not force_types) and (not all_types) and (not all_types,54.15
"def create_default_config(self) -> bool:
        
        try:
            if <IFMASK>:
                return True
                
            default_config = {
                ""mcpServers"": {},
                ""log_level"": ""INFO""
            }
            
            self.config_path.parent.mkdir(parents=True, exist_ok=True) <ANS> ",true,self.config_path.exists(),self.config_path.parent.exists() and self.config_path.parent.is_dir() and,80.63
"def save_fid_stats(paths, batch_size, device, dims, num_workers=1):
    
    if <IFMASK>:
        raise RuntimeError(""Invalid path: %s"" % paths[0])

    if os.path.exists(paths[1]):
        raise RuntimeError(""Existing output file: %s"" % paths[1])

    block_idx = InceptionV3.BLOCK_INDEX_BY_DIM[dims]

    model = InceptionV3([block_idx]).to(device) <ANS> ",false,not os.path.exists(paths[0]),batch_size < 0 or batch_size > 1 or batch_size > 2 or batch_size > 3 or,67.01
"def pytest_sessionfinish(session, exitstatus):
    
    cov_dir = os.path.abspath(""."")
    if <IFMASK>:
        try:
            cov = coverage.Coverage()
            cov.combine(data_paths=[cov_dir], strict=True)
            cov.save()
        except Exception as e:
            print(f""Error combining coverage data: {e}"", file=sys.stderr) <ANS> ",false,"exitstatus == 0 and os.environ.get(""COVERAGE_PROCESS_START"")","not hasattr(report, 'wasxfail') and hasattr(report, 'WARNING') and hasattr(report, 'Failed')",61.38
"def _build_param_buffer(self, pp_rank):
        
        if <IFMASK>:
            from verl.utils.memory_buffer import MemoryBuffer

            # The code here is very hard-coded, based on the following assumptions:
            # 1. `len(_this_rank_models) == 1`
            # 2. `_this_rank_models[0]` is a instance of `DistributedDataParallel` and `use_distributed_optimizer=True`
            # 3. Only bfloat16 data type is used in parameters
            source = self._this_rank_models[0].buffers[0].param_data
            self.memory_buffers[pp_rank] = {torch.bfloat16: MemoryBuffer(source.numel(), source.numel(), torch.bfloat16, source)} <ANS> ",true,pp_rank == self._pp_rank,self.use_distributed_optimizer and pp_rank == 1 and (not self.use_distributed_optimizer),64.21
"def MMMU_preproc(data):
    
    print(""Preprocessing MMMU dataset..."")
    cnt = 0
    As, Bs, Ans = list(data['A']), list(data['B']), list(data['answer'])
    lt = len(data)
    for i in range(lt):
        if <IFMASK>:
            As[i] = Ans[i]
            Bs[i] = 'Other Answers'
            cnt += 1
    print(f'During MMMU_preproc in Evaluation, {cnt} open questions are re-formulated to multi-choice ones.')
    data['A'] = As
    data['B'] = Bs
    return data <ANS> ",false,pd.isna(As[i]),len(data['A']) != len(data['B']) or data['A'] != data['B'] or,66.79
"def parse_expected_output(cls, v):
        
        if <IFMASK>:
            try:
                return json.loads(v)
            except json.JSONDecodeError as e:
                logger.error(f""Error parsing expected_output: {str(e)}"")
                raise ValueError(""Invalid JSON format for expected_output"")
        return v <ANS> ",true,"isinstance(v, str)","not isinstance(v, dict) or 'json' in v and 'orient' in v and 'records' in",56.54
"def get_latest_files(directory: str, file_types: list = ['.webm', '.zip']) -> Dict[str, Optional[str]]:
    
    latest_files: Dict[str, Optional[str]] = {ext: None for ext in file_types}

    if <IFMASK>:
        os.makedirs(directory, exist_ok=True)
        return latest_files

    for file_type in file_types:
        try:
            matches = list(Path(directory).rglob(f""*{file_type}""))
            if matches:
                latest = max(matches, key=lambda p: p.stat().st_mtime) <ANS> ",false,not os.path.exists(directory),not file_types and (not file_path_matches) and (not file_path_matches) and (,67.81
"def _trajectory_to_text(trajectory: Dict[str, Any]) -> str:
    
    text = ""AGENT TRAJECTORY\n"" + ""=""*50 + ""\n\n""
    
    # Add answers/responses
    if <IFMASK>:
        for i, answer in enumerate(trajectory[""answer""]):
            text += f""STEP {i+1}:\n""
            text += f""Agent Response:\n{answer}\n\n""
    
    # Add parsed responses if available
    if ""parsed_response"" in trajectory and isinstance(trajectory[""parsed_response""], list):
        text += ""\nPARSED RESPONSES\n"" + ""-""*50 + ""\n\n""
        for i, parsed in enumerate(trajectory[""parsed_response""]): <ANS> ",true,"""answer"" in trajectory and isinstance(trajectory[""answer""], list)","not isinstance(trajectory['parsed_response'], list) or not isinstance(trajectory['parsed_response'], list",77.00
"def _run(self, **kwargs) -> str:
        
        if <IFMASK>:
            return ""No MCP server is currently active, so there's nothing to disconnect from.""

        server_name = self.server_manager.active_server
        try:
            # Clear the active server
            self.server_manager.active_server = None

            # Note: We're not actually closing the session here, just 'deactivating' <ANS> ",true,not self.server_manager.active_server,self.is_alive() and self.is_alive() and (not self.is_alive()) and (,68.55
"def on_signal(self, signal_name: str) -> Callable:
        

        def decorator(func: Callable) -> Callable:
            unique_name = f""{signal_name}_{uuid.uuid4()}""

            async def wrapped(value: SignalValueT):
                try:
                    if <IFMASK>:
                        await func(value)
                    else:
                        func(value)
                except Exception as e:
                    # Log the error but don't fail the entire signal handling
                    print(f""Error in signal handler {signal_name}: {str(e)}"")

            self._handlers.setdefault(signal_name, []).append((unique_name, wrapped)) <ANS> ",false,asyncio.iscoroutinefunction(func),"not self._handlers and (not self._handlers.get(name, {}).get(name, {}).get(name",52.52
"def pdf_path():
    
    pdf_path = Path(PDF_FILE_PATH)
    if <IFMASK>:
        pytest.skip(f""Test PDF file not found at {pdf_path}"")
    return pdf_path <ANS> ",true,not pdf_path.exists(),not os.path.exists(PDF_FILE_PATH) or not os.path.exists(PDF_FILE,79.92
"def download_dataset(self) -> None:
        
        assert self.data_dir is not None
        if <IFMASK>:
            os.makedirs(self.data_dir, exist_ok=True)

        logging.info(f""[WebGames] Downloading dataset into '{self.data_dir}'..."")

        # Use pandas to download and save the dataset
        df = pd.read_json(  # type: ignore
        )
        output_path = os.path.join(self.data_dir, self.TEST_FILE) <ANS> ",true,not os.path.exists(self.data_dir),self.data_dir is None and self.data_dir.exists() and (not self.overwrite) and,60.80
"def get_ratelimit_middleware_instance(fastapi_app: FastAPI) -> RateLimitMiddleware:
    
    layer = fastapi_app.middleware_stack
    while layer is not None:
        if <IFMASK>:
            return cast(RateLimitMiddleware, layer)
        layer = getattr(layer, ""app"", None)

    assert False, f""{RateLimitMiddleware.__name__} instance not found"" <ANS> ",false,layer.__class__.__name__ == RateLimitMiddleware.__name__,"not isinstance(fastapi_app.config.USE_UVLOOP, Default) and fastapi_app.config.USE",62.08
"def plot_total_conversation_time(results, title, filename):
    
    df = pd.DataFrame(results)
    df = df[(df['elapsed'].notnull()) & (df['turn'] == 'ALL')]
    if <IFMASK>:
        print(f""No successful results to plot for {title}."")
        return
    plt.figure(figsize=(10, 6))
    ax = plt.gca()
    df.boxplot(column='elapsed', by=['provider'], ax=ax)
    plt.title(title)
    plt.suptitle("""")
    plt.ylabel('Total Conversation Time (s)') <ANS> ",false,df.empty,filename != 'total_conversation_time' or filename != 'input_ids' or filename != 'input_ids,52.23
"def _extract_tool_calls(self, trajectory: dict) -> list:
        
        tool_calls = []
        for message in trajectory.get(""messages"", []):
            if <IFMASK>:
                for tool_call in message.get(""tool_calls"", []):
                    if tool_call.get(""function"", {}).get(""name"") and tool_call.get(""function"", {}).get(""arguments""):
                        tool_calls.append({
                            ""name"": tool_call[""function""][""name""],
                            ""arguments"": json.loads(tool_call[""function""][""arguments""])
                        })
        return tool_calls <ANS> ",false,"message.get(""role"") == ""assistant"" and message.get(""tool_calls"")",self.project_id is None and self.location is None and (self.retry is None) and (self,64.06
"def close(self):
        
        try:
            # Milvus client doesn't have an explicit close method, but we can release the collection
            if <IFMASK>:
                self.client.release_collection(collection_name=self.collection_name)
        except Exception as e:
            logger.error(f""Error closing Milvus connection: {e}"") <ANS> ",true,"hasattr(self.client, ""release_collection"")",self.client.transport_protocol == 'tcp' and self.client.transport_protocol == 'both' and,71.38
"def print_summary(self):
        
        summary = self.get_summary()
        total = summary.get(""total_duration"", 0)
        
        print(""\n===== SPEED PROFILE SUMMARY ====="")
        print(f""Total execution time: {total:.2f} seconds"")
        print(""\n--- Component Breakdown ---"")
        
        # Print each component's timing
        for name, data in self.timings.items():
            if <IFMASK>:
                percent = data[""total""] / total * 100 if total > 0 else 0
                print(f""{name}: {data['total']:.2f}s ({percent:.1f}%) - ""
                      f""{data['count']} calls, avg {data['total'] / data['count']:.3f}s per call"")
        
        print(""\n=============================="") <ANS> ",false,"name != ""total""",self.get_logs is not None and self.get_logs_from_task_metadata is not None and,48.72
"def create_tool_result_message(tool_result, tool_name, status=""success""):
        
        from google.genai import types

        if <IFMASK>:
            function_response = {""result"": tool_result}
        else:
            function_response = {""error"": tool_result}

        return types.Content(
            role=""tool"",
            parts=[
                types.Part.from_function_response( <ANS> ",false,"status == ""success""",role == 'tool' and role == 'tool' and role == 'tool' and (role == 'tool',59.58
"def _merge_by_placement(self, tensors: list[torch.Tensor], placement: Placement) -> torch.Tensor:
        
        if <IFMASK>:
            return tensors[0]
        elif placement.is_partial():
            raise NotImplementedError(""Partial placement is not supported yet"")
        elif placement.is_shard():
            return torch.cat(tensors, dim=placement.dim).contiguous()

        raise NotImplementedError(f""Unsupported placement: {placement}"") <ANS> ",false,placement.is_replicate(),self.placement_strategy == 'stratified' and self.placement_strategy == 'stratified' and (not,57.84
"def analyze_news_sentiment(news_items: list) -> str:
    
    if <IFMASK>:
        return ""No news data available""
    
    # Just return a simple count for now - in a real implementation, this would use NLP
    return f""Qualitative review of {len(news_items)} recent news items would be needed"" <ANS> ",true,not news_items or len(news_items) == 0,len(news_items) == 1 and len(news_items) == 2 and (len(news_items,74.24
"def deepseek_fn(self, history, verbose=False):
        
        if <IFMASK>:
            raise Exception(""Deepseek (API) is not available for local use. Change config.ini"")
        try:
            response = client.chat.completions.create(
                model=""deepseek-chat"",
                messages=history,
                stream=False
            )
            thought = response.choices[0].message.content <ANS> ",true,self.is_local,verbose and self.verbose_mod is not None and self.verbose_mod(verbose) is not None and self,65.53
"def copy_old_config_names_to_new(self) -> Self:
		
		if <IFMASK>:
			self.window_size = self.window_size or {}
			self.window_size['width'] = (self.window_size or {}).get('width') or self.window_width or 1280
			self.window_size['height'] = (self.window_size or {}).get('height') or self.window_height or 1100
		return self <ANS> ",true,self.window_width or self.window_height,self.window_size is None or self.window_size.height is None or self.window_size.,74.17
"def _prepare_batch_get_documents_request(
        self, document_ids: List[str], folder_name: Optional[Union[str, List[str]]], end_user_id: Optional[str]
    ) -> Dict[str, Any]:
        
        if <IFMASK>:
            request = {""document_ids"": document_ids}
            if folder_name:
                request[""folder_name""] = folder_name
            if end_user_id:
                request[""end_user_id""] = end_user_id
            return request
        return document_ids <ANS> ",false,folder_name or end_user_id,self.batch_id is not None and self.batch_job_id is not None and (not self.,71.32
"def load_jsonl_files(input_dir):
    
    jsonl_files = list(Path(input_dir).glob(""*.jsonl""))
    if <IFMASK>:
        print(f""No JSONL files found in {input_dir}"")
        return []

    print(f""Found {len(jsonl_files)} JSONL files: {[f.name for f in jsonl_files]}"")
    return jsonl_files <ANS> ",false,not jsonl_files,not os.path.exists(input_dir) or not os.path.exists(input_dir) or,84.95
"def _load_environment(self) -> None:
        
        env_file = self.app_root / "".env""
        if <IFMASK>:
            # logger.info(f""Loading environment variables from {env_file}"")
            load_dotenv(str(env_file)) <ANS> ",false,env_file.exists(),self.use_managed_conn_id and self.use_managed_conn_secret_arn and (not,50.52
"def add_tool(self, tool: BaseTool):
        
        if <IFMASK>:
            logger.warning(f""Tool {tool.name} already exists in collection, skipping"")
            return self

        self.tools += (tool,)
        self.tool_map[tool.name] = tool
        return self <ANS> ",true,tool.name in self.tool_map,self.tool_map[tool.name] == 'airflow' and self.tool_map[tool.name,68.32
"def _generate_batch_response(self, text, system_message, provider, model_config, api_key, api_base):
        
        MAX_RETRIES = 3
        
        for attempt in range(MAX_RETRIES):
            try:
                if <IFMASK>:
                    messages = [{'role': 'user', 'content': system_message + text}]
                    response = proxy_api_completion(messages=messages, model=model_config[""model""], api_base=api_base)
                    # response = proxy_call.api_completion(messages=messages, model=model_config[""model""], api_base=api_base)
                    return pd.DataFrame(ast.literal_eval(response[0]))
                else:
                    return self._generate_llm_response(text, system_message, model_config, api_key)
            except (json.JSONDecodeError, ValueError) as e:
                if attempt == MAX_RETRIES - 1: <ANS> ",false,"provider == ""gemini"" and api_base",not self.supports_autocommit and self.autocommit_parameters and (not self.autocommit_parameters) and (not,53.22
"def model_kwargs():
    
    inference_url = settings.TEXT2VEC_INFERENCE_URL
    if <IFMASK>:
        raise ValueError(""TEXT2VEC_INFERENCE_URL environment variable is not set"")
    return {""inference_url"": inference_url} <ANS> ",false,not inference_url,"not isinstance(config['processor'], dict) or 'processor' not in config['processor'] or 'processor' not",55.06
"def handle_js_message(client_id, message_type, data):
        
        if <IFMASK>:
            asyncio.create_task(  # noqa: RUF006
                _service.handle_client_message(
                    client_id, {""type"": message_type, ""data"": data}
                )
            )
        return True <ANS> ",false,_service,"not isinstance(data, dict) or 'data' not in data or 'id' not in data['data']",60.09
"def rename_state_dict(
    old_prefix: str, new_prefix: str, state_dict: Dict[str, torch.Tensor]
):
    
    # need this list not to break the dict iterator
    old_keys = [k for k in state_dict if k.startswith(old_prefix)]
    if <IFMASK>:
        logging.warning(f""Rename: {old_prefix} -> {new_prefix}"")
    for k in old_keys:
        v = state_dict.pop(k)
        new_k = k.replace(old_prefix, new_prefix)
        state_dict[new_k] = v <ANS> ",true,len(old_keys) > 0,state_dict.get('num_shards') != 0 and old_state_dict.get('num_shards,61.54
"def _get_loggable_messages(self, messages: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        
        loggable_messages = []
        for msg in messages:
            if <IFMASK>:
                new_content = []
                for content in msg[""content""]:
                    if content.get(""type"") == ""image"":
                        new_content.append(
                            {""type"": ""image"", ""image_url"": {""url"": ""[BASE64_IMAGE_DATA]""}}
                        )
                    else:
                        new_content.append(content) <ANS> ",true,"isinstance(msg.get(""content""), list)",not content.get('filename') or not content.get('filename_jinja') or not content.get('filename,71.12
"def _build_split_documents(row, chunks: List[str]) -> List[dict[str, Any]]:
    
    documents: List[dict] = []

    for i, text in enumerate(chunks):
        if <IFMASK>:
            continue

        metadata = row.metadata if hasattr(row, ""metadata"") and isinstance(row.metadata, dict) else {}
        metadata = copy.deepcopy(metadata)

        metadata[""content""] = text

        documents.append({""document_type"": ContentTypeEnum.TEXT.value, ""metadata"": metadata, ""uuid"": str(uuid.uuid4())}) <ANS> ",false,text is None or not text.strip(),"not isinstance(document, dict) or not isinstance(document, list) or not isinstance(document, str) or",60.46
"def try_add_cuda_lib_path():
    
    required_submodules = [""cuda_runtime"", ""cublas""]
    cuda_versions = [""11"", ""12""]

    module_spec = find_spec(""nvidia"")
    if <IFMASK>:
        return

    nvidia_lib_root = Path(module_spec.submodule_search_locations[0])

    for submodule in required_submodules:
        for ver in cuda_versions:
            try:
                package_name = f""nvidia_{submodule}_cu{ver}"" <ANS> ",false,not module_spec,"not (installation_spec := os.environ.get('PATH_TO_PYTHON_MAJOR_MINOR_VERSION',",49.77
"def parse_size(size):
    
    units = {""KB"": 1024, ""MB"": 1024**2, ""GB"": 1024**3}
    if <IFMASK>:
        return int(size)
    unit = size[-2:].upper()
    number = size[:-2]
    if unit in units and number.isdigit():
        return int(number) * units[unit]
    raise ValueError(""Size must be in the format <number>[KB|MB|GB]."") <ANS> ",false,size.isdigit(),unit == 'KB' and unit == 'MB' and unit == 'GB' and unit == 'None' and,60.69
"def package_template_features(
    *,
    hit_features: Sequence[Mapping[str, Any]],
    include_ligand_features: bool,
) -> Mapping[str, Any]:
  

  features_to_include = set(_POLYMER_FEATURES)
  if <IFMASK>:
    features_to_include.update(_LIGAND_FEATURES)

  features = {
      feat: [single_hit_features[feat] for single_hit_features in hit_features]
      for feat in features_to_include
  }

  stacked_features = {} <ANS> ",false,include_ligand_features,"not isinstance(constraints, (list, tuple)) and len(constraints) != len(constraints) and (not all",63.47
"def rewrite_schema_for_sqlglot(
        cls, schema: str | SQLGlotSchemaType | BirdSampleType
    ) -> SQLGlotSchemaType:
        
        schema_dict = None
        if <IFMASK>:
            if isinstance(schema, str):
                schema = cls.extract_schema_from_ddls(schema)
                schema_dict = cls.format_schema(schema)
            elif _isinstance_sqlglot_schema_type(schema):
                schema_dict = schema
            elif _isinstance_bird_sample_type(schema):
                schema_dict = cls._get_schema_from_bird_sample(schema)
            elif _isinstance_ddl_schema_type(schema): <ANS> ",true,schema,"isinstance(schema, str) and schema_dict.get('pattern') == 'table' and schema_dict.",60.36
"def get_config_defaults() -> Dict[str, Any]:
    
    defaults = {
        'site': 'all',
        'model': 'gpt-4o-mini',
        'generate_mode': 'list', 
        'retrieval_backend': CONFIG.preferred_retrieval_endpoint,
        'prev': []
    }
    
    # Try to get preferred model from LLM config
    if <IFMASK>:
        llm_provider = CONFIG.get_llm_provider()
        if llm_provider and llm_provider.models:
            # Use the 'low' model as default for testing
            defaults['model'] = llm_provider.models.low or defaults['model']
    
    return defaults <ANS> ",false,"hasattr(CONFIG, 'preferred_llm_provider') and CONFIG.preferred_llm_provider",AIRFLOW_V_3_0_PLUS and (not AIRFLOW_V_3_0_PLUS.startswith(',79.62
"def format(self) -> str:
        
        if <IFMASK>:
            return ""There were no ingestion or query tasks detected.""

        instructions = ""Tasks:\n\n""
        order = 1

        if len(self.ingestions) > 0:
            instructions += ""Ingestions:\n""
            for ingestion in self.ingestions: <ANS> ",true,len(self.ingestions) == 0 and len(self.queries) == 0,len(self.ingestions) > 0 and self.query_ids and (not self.query_task_,64.85
"def _log_debug(self, message: str, **kwargs) -> None:
        
        if <IFMASK>:
            print(f""DEBUG: {message}"")
            if kwargs:
                print(json.dumps(kwargs, indent=2)) <ANS> ",true,self.server.debug,self.debug is not None and self.debug.name == 'log_debug' and (not self.debug,46.19
"def load_and_preprocess_image(image_path, target_img_size):
    
    image = cv2.imread(image_path)
    if <IFMASK>:
        raise ValueError(f""Failed to load image from path: {image_path}"")

    # Resize and pad the image to the target size
    resized_image = resize_image(image, target_img_size)

    # Normalize the image (assuming model expects normalized input)
    normalized_image = resized_image.astype(np.float32) / 255.0 <ANS> ",false,image is None,"not isinstance(target_img_size, collections.abc.Iterable) or not isinstance(image_size, collections",71.28
"def _prepare_toolset(self) -> None:
    
    # For each API, get the first version and the first spec of that version.
    spec_str = self._apihub_client.get_spec_content(self._apihub_resource_name)
    spec_dict = yaml.safe_load(spec_str)
    if <IFMASK>:
      return

    self.name = self.name or _to_snake_case(
        spec_dict.get('info', {}).get('title', 'unnamed')
    )
    self.description = self.description or spec_dict.get('info', {}).get(
        'description', ''
    ) <ANS> ",false,not spec_dict,self.name is not None and self.spec_type is not None and self.name != self.spec_,59.90
"def add_message_pair(self, user_message: str, assistant_message: str):
        
        if <IFMASK>:
            self.cache.append({""user"": user_message, ""assistant"": assistant_message})
            self._save() <ANS> ",false,"not any(entry[""user""] == user_message for entry in self.cache)",self.verbose_feature_names_out is None or self.verbose_feature_names_out == 'all,61.77
"def clean_response(cls, content: str) -> Dict[str, Any]:
        
        cleaned = re.sub(r""```(?:json)?\s*"", """", content).strip()
        match = re.search(r""(\{.*\})"", cleaned, re.S)
        if <IFMASK>:
            logger.error(""Failed to parse JSON from content: %r"", content)
            return {}
        return json.loads(match.group(1)) <ANS> ",false,not match,content == 'json' or content == 'yaml' or content == 'json_or_python' or content ==,62.91
"def _create_session(self) -> requests.Session:
        
        session = requests.Session()

        # Configure automatic retries with exponential backoff
        retry_strategy = Retry(
            total=self.max_retries,
            backoff_factor=self.retry_backoff_factor,
            status_forcelist=[429, 500, 502, 503, 504],
            allowed_methods={""HEAD"", ""GET"", ""POST"", ""OPTIONS""},
        )

        adapter = HTTPAdapter(max_retries=retry_strategy)

        # Set up headers
        headers = {""Accept"": ""application/json""}
        if <IFMASK>:
            headers[""x-api-key""] = self.api_key

        session.headers.update(headers)

        return session <ANS> ",true,self.api_key,self.wait_for_completion is not None and self.deferrable and (not self.wait_for_completion,75.03
"def _snowflake_to_iso(snowflake: str) -> str:
        
        if <IFMASK>:
            raise ValueError(f""Invalid snowflake ID: {snowflake}"")

        # Discord epoch (2015-01-01)
        discord_epoch = 1420070400000

        # Convert ID to int and shift right 22 bits to get timestamp
        timestamp_ms = (int(snowflake) >> 22) + discord_epoch <ANS> ",false,not DiscordRetrieveTool._is_snowflake(snowflake),notsnowflake.startswith('0') or notsnowflake.endswith('1') or (notsnowflake.startswith('0')),51.95
"def wait_for_health(url: str, timeout: int = 300) -> bool:
    
    print(f""Waiting for {url} to be healthy..."")
    start_time = time.time()

    while time.time() - start_time < timeout:
        try:
            response = requests.get(f""{url}/health"", timeout=5)
            if <IFMASK>:
                print(""‚úì Service is healthy"")
                return True
        except requests.exceptions.RequestException:
            pass

        time.sleep(2)
        print(""."", end="""", flush=True) <ANS> ",true,response.status_code == 200,not response.ok() or not response.status_code >= 500 or (response.status_code == 200),57.96
"def configure(cls, **kwargs):
        
        for key, value in kwargs.items():
            key = key.strip().lower()
            if <IFMASK>:
                if key in cls.parser_keywords:
                    setattr(cls, key, value)
                else:
                    # Yup, no fun allowed LOL
                    raise AttributeError(f'Unknown parser argument: ""{key}""; maybe you meant {cls.parser_keywords}?')
            else:
                raise ValueError(f'Unknown parser argument: ""{key}""; maybe you meant {cls.parser_keywords}?') <ANS> ",true,"hasattr(cls, key)",cls.parser_defaults is not None and cls.parser_defaults[0] is not None and (cls.,58.30
"def list_available(cls) -> List[str]:
        
        keys = list(Registrable._registry[cls].keys())
        default = cls.default_implementation

        if <IFMASK>:
            return keys
        elif default not in keys:
            raise ConfigurationError(
                f""Default implementation {default} is not registered""
            )
        else:
            return [default] + [k for k in keys if k != default] <ANS> ",false,default is None,"not isinstance(cls, Enum) or not isinstance(cls, Enum) or (not isinstance(cls, Enum))",57.29
"def __str__(self) -> str:
        
        plan_str = """"
        if <IFMASK>:
            plan_str += f""Task: {self.task}\n""
        for i, step in enumerate(self.steps):
            plan_str += f""{i}. {step.agent_name}: {step.title}\n   {step.details}\n""
        return plan_str <ANS> ",true,self.task is not None,not self.task.is_teardown and self.task.is_setup and (not self.task.is,67.49
"def cleanup_memory(self, force: bool = False) -> None:
        
        # Run Python garbage collection
        gc.collect()
        
        # Empty CUDA cache if available
        if <IFMASK>:
            torch.cuda.empty_cache()
            
        # Log memory status after cleanup
        if force:
            info = self.get_memory_info()
            logger.info(
                f""Memory after cleanup: RAM: {info['ram_used_gb']:.2f}GB / {info['ram_total_gb']:.2f}GB, ""
                f""VRAM: {info.get('vram_used_gb', 0):.2f}GB / {info.get('vram_total_gb', 0):.2f}GB"" <ANS> ",true,self.cuda_available,self.is_alive() and (not self.is_alive()) and (not self.is_alive()),65.43
"def add_prompt(
        self,
        prompt: Prompt,
    ) -> Prompt:
        

        # Check for duplicates
        existing = self._prompts.get(prompt.name)
        if <IFMASK>:
            if self.warn_on_duplicate_prompts:
                logger.warning(f""Prompt already exists: {prompt.name}"")
            return existing

        self._prompts[prompt.name] = prompt
        return prompt <ANS> ",false,existing,self.config.option.verbose >= 1 and self.config.option.debug and (not self.config.,51.77
"def save_model_settings(model, state):
    
    if <IFMASK>:
        yield (""Not saving the settings because no model is selected in the menu."")
        return

    user_config = shared.load_user_config()
    model_regex = model + '$'  # For exact matches
    if model_regex not in user_config:
        user_config[model_regex] = {} <ANS> ",false,model == 'None',state in user_config['Not Inherited'] or state in user_config['Not Inherited'] or (state,59.37
"def get_target_module(obj: object) -> Optional[str]:
    
    if <IFMASK>:
        return None

    fqn = f""{obj.__module__}.{obj.__name__}""
    return _PDOC_MODULE_EXPORT_MAPPINGS.get(fqn) <ANS> ",true,"not hasattr(obj, ""__module__"")","isinstance(obj, str) and obj.endswith('modules/generated/') and (not obj.endswith('modules/",60.13
"def validate_graph_fields(self) -> ""GraphPromptOverrides"":
        
        allowed_fields = {""entity_extraction"", ""entity_resolution""}
        for field in self.model_fields:
            if <IFMASK>:
                raise ValueError(f""Field '{field}' is not allowed in graph prompt overrides"")
        return self <ANS> ",true,"field not in allowed_fields and getattr(self, field, None) is not None",self.entity_extraction is not None and self.entity_resolution is not None and (not self.entity_,68.98
"def _truncate_multimodal_text(self, contents: list[dict[str, Any]], n_tokens: int) -> list[dict[str, Any]]:
        
        tmp_contents = []
        for content in contents:
            if <IFMASK>:
                truncated_text = self._truncate_tokens(content[""text""], n_tokens)
                tmp_contents.append({""type"": ""text"", ""text"": truncated_text})
            else:
                tmp_contents.append(content)
        return tmp_contents <ANS> ",false,"content[""type""] == ""text""",self.do_truncate_text and self.truncate_text and (not self.do_truncate_text),53.87
"def format_validation_errors(errors: list) -> str:
    
    formatted_errors = []

    for error in errors:
        loc = error.get(""loc"", [])
        msg = error.get(""msg"", """")
        error_type = error.get(""type"", """")

        # Build field path
        field_path = "" -> "".join(str(part) for part in loc if part != ""body"")

        # Format the error message with type information
        if <IFMASK>:
            if error_type:
                formatted_error = f""Field '{field_path}' ({error_type}): {msg}""
            else:
                formatted_error = f""Field '{field_path}': {msg}""
        else:
            formatted_error = msg

        formatted_errors.append(formatted_error) <ANS> ",true,field_path,field_path not in errors and errors[field_path] != f'Field '{field_path}' and field,56.97
"def check_hub_revision_exists(training_args: SFTConfig | GRPOConfig):
    
    if <IFMASK>:
        if training_args.push_to_hub_revision is True:
            # First check if the revision exists
            revisions = [rev.name for rev in list_repo_refs(training_args.hub_model_id).branches]
            # If the revision exists, we next check it has a README file
            if training_args.hub_model_revision in revisions:
                repo_files = list_repo_files(
                    repo_id=training_args.hub_model_id, revision=training_args.hub_model_revision
                ) <ANS> ",true,repo_exists(training_args.hub_model_id),not (args.hub_model_id and args.hub_model_revision) and (not args.push,63.14
"def save_partial_results(self, agent, task):
        
        try:
            if <IFMASK>:
                task_client = self.tasks[task]
                overall = task_client.calculate_overall(self.completions[agent][task])
                output_dir = self.get_output_dir(agent, task)
                os.makedirs(output_dir, exist_ok=True)
                with open(os.path.join(output_dir, ""overall.json""), ""w"") as f:
                    f.write(json.dumps(overall, indent=4, ensure_ascii=False))
                return overall
        except Exception as e: <ANS> ",false,task in self.tasks and agent in self.completions and task in self.completions[agent],not self.wait_for_completion or self.deferrable and self.wait_for_completion not in (None,63.69
"def do_Tm(
        self,
        a: PDFStackT,
        b: PDFStackT,
        c: PDFStackT,
        d: PDFStackT,
        e: PDFStackT,
        f: PDFStackT,
    ) -> None:
        
        values = (a, b, c, d, e, f)
        matrix = safe_matrix(*values)

        if <IFMASK>:
            log.warning(
                f""Could not set text matrix because not all values in {values!r} can be parsed as floats""
            )
        else:
            self.textstate.matrix = matrix
            self.textstate.linematrix = (0, 0) <ANS> ",false,matrix is None,self.text_format == 'CSV' and self.text_format == 'CSVRender' and (not self,51.91
"def get_tool_metadata(tool_name_or_schema: str | ToolSchema) -> ToolMetadata:
    
    tool_name: str | None = (
        tool_name_or_schema
        if isinstance(tool_name_or_schema, str)
        else tool_name_or_schema.get(""name"")
    )

    metadata = _tool_metadata.get(tool_name)

    if <IFMASK>:
        raise ValueError(f""Tool {tool_name} not found in metadata."")

    return metadata <ANS> ",false,metadata is None,"not isinstance(tool_name_or_schema, str) or not isinstance(tool_name_or_schema",85.07
"def fetch_branches(owner: str, repo: str):
        

        response = requests.get(url, headers=headers)

        if <IFMASK>:
            if not token:
                print(f""Error 404: Repository not found or is private.\n""
                      f""If this is a private repository, please provide a valid GitHub token via the 'token' argument or set the GITHUB_TOKEN environment variable."")
            else:
                print(f""Error 404: Repository not found or insufficient permissions with the provided token.\n""
                      f""Please verify the repository exists and the token has access to this repository."")
            return [] <ANS> ",false,response.status_code == 404,not token or token.startswith('https') or token.endswith('https://') or token.endswith('https://,66.18
"def get_model(cls, base_id, task_type):
        
        if <IFMASK>:
            return cls.registry[""model_specific""][base_id]
        elif task_type in cls.registry[""task_type""]:
            return cls.registry[""task_type""][task_type]
        else:
            raise ValueError(f""No model class found for model '{base_id}' or task type '{task_type}'"") <ANS> ",false,"base_id in cls.registry[""model_specific""]",not task_type.startswith('task') or not task_type.endswith('task') or task_type.,64.43
"def _compile_dependencies():
    
    if <IFMASK>:
        start_time = time.time()
        logger.info(""> Compiling dataset index builder..."")
        try:
            from core.datasets.utils import compile_helpers
            compile_helpers()
            logger.info(
                f"">>> Done with dataset index builder. Compilation time: {time.time() - start_time:.3f} seconds""
            ) <ANS> ",false,torch.distributed.get_rank() == 0,"not isinstance(dataset_range, range) or not isinstance(dataset_range, range) or (dataset_range",61.38
"def crawl(self) -> List[Dict]:
        
        to_visit = [self.base_url]
        results = []
        
        while to_visit and len(self.visited) < self.max_pages:
            url = to_visit.pop(0)
            
            if <IFMASK>:
                continue
                
            print(f""Crawling: {url}"")
            content = self.extract_page_content(url)
            
            if content:
                self.visited.add(url)
                results.append(content) <ANS> ",true,url in self.visited,self.visited and (not self.visited) and (not results) and (not self.visited) and (,54.62
"def clean_response(cls, content: str) -> Dict[str, Any]:
        
        cleaned = re.sub(r""```(?:json)?\s*"", """", content).strip()
        match = re.search(r""(\{.*\})"", cleaned, re.S)
        if <IFMASK>:
            return {}
        return json.loads(match.group(1)) <ANS> ",false,not match,content == 'json' or content == 'yaml' or content == 'json_or_python' or content ==,61.63
"def replace_message_at(
        self, index, text_content, image_content=None, image_detail=""high""
    ):
        
        if <IFMASK>:
            self.messages[index] = {
                ""role"": self.messages[index][""role""],
                ""content"": [{""type"": ""text"", ""text"": text_content}],
            }
            if image_content:
                base64_image = self.encode_image(image_content)
                self.messages[index][""content""].append(
                    { <ANS> ",false,index < len(self.messages),image_content is None or image_content == base64_image) or image_content == base64_image,76.71
"def _make_output(
        self,
        file_content: str,
        file_descriptor: str,
        init_line: int = 1,
        expand_tabs: bool = True,
    ) -> str:
        
        file_content = maybe_truncate(file_content)
        if <IFMASK>:
            file_content = file_content.expandtabs()
        file_content = ""\n"".join(
            [f""{i + init_line:6}\t{line}"" for i, line in enumerate(file_content.split(""\n""))]
        )
        return (
            f""Here's the result of running `cat -n` on {file_descriptor}:\n"" + file_content + ""\n""
        ) <ANS> ",false,expand_tabs,not self.is_versioned and (not self.is_helm) and (not self.is_helm),55.76
"def get_transcript_object(self) -> aai.Transcript:
        
        logger.info(""Starting video transcription"")
        transcript = self.transcriber.transcribe(self.video_path)
        if <IFMASK>:
            logger.error(f""Transcription failed: {transcript.error}"")
            raise ValueError(f""Transcription failed: {transcript.error}"")
        if not transcript.words:
            logger.warning(""No words found in transcript"")
        logger.info(""Transcription completed successfully!"")

        return transcript <ANS> ",false,"transcript.status == ""error""",self.use_redshift_data_quality_ruleset_kwargs or self.redshift_data_quality_ruleset_,66.32
"def update_status(self, agent_name: str, ticker: Optional[str] = None, status: str = """", analysis: Optional[str] = None):
        
        if <IFMASK>:
            self.agent_status[agent_name] = {""status"": """", ""ticker"": None}

        if ticker:
            self.agent_status[agent_name][""ticker""] = ticker
        if status:
            self.agent_status[agent_name][""status""] = status
        if analysis:
            self.agent_status[agent_name][""analysis""] = analysis <ANS> ",true,agent_name not in self.agent_status,self.wait_for_completion is not None and self.wait_for_completion not in self.completion_,71.10
"def register_mimetype_component_type(mimetype: str, component_type: Optional[str] = None):
    
    if <IFMASK>:
        logger.warning(f""[registry] Suspicious mimetype format: {mimetype}"")
    _mimetype_to_component_type[mimetype] = component_type or ""generic"" <ANS> ",false,"not re.match(r""^[^/]+/[^/]+$"", mimetype)",not component_type or not _is_subtype_of(mimetype) or _is_subtype_of(component,58.86
"def is_unsubscribe_allowed(
    mcp: FastMCP, sns_client: Any, kwargs: Dict[str, Any]
) -> Tuple[bool, str]:
    
    subscription_arn = kwargs.get('SubscriptionArn')

    if <IFMASK>:
        return False, 'SubscriptionArn is not passed to the tool'

    try:
        # Get subscription attributes to find the TopicArn
        attributes = sns_client.get_subscription_attributes(SubscriptionArn=subscription_arn)
        topic_arn = attributes.get('Attributes', {}).get('TopicArn')

        return is_mutative_action_allowed(mcp, sns_client, {'TopicArn': topic_arn}) <ANS> ",false,subscription_arn is None or subscription_arn == '',"not isinstance(mcp, LivyHook) or not isinstance(mcp. LivyHook, LivyHook) or not isinstance(m",63.88
"def increment_and_enforce_llm_calls_limit(
      self, run_config: Optional[RunConfig]
  ):
    
    # We first increment the counter and then check the conditions.
    self._number_of_llm_calls += 1

    if (
        run_config
        
        <IFMASK>
    ):
      # We only enforce the limit if the limit is a positive number.
      raise LlmCallsLimitExceededError(
          ""Max number of llm calls limit of""
          f"" `{run_config.max_llm_calls}` exceeded""
      ) <ANS> ",true,run_config,self._number_of_llm_calls == 0 and run_config.llm_limit_size > 0 and,64.28
"def process_llm_request(self, llm_request: LlmRequest) -> None:
    
    if <IFMASK>:
      llm_request.config = llm_request.config or types.GenerateContentConfig()
      llm_request.config.tools = llm_request.config.tools or []
      llm_request.config.tools.append(
          types.Tool(code_execution=types.ToolCodeExecution())
      )
      return
    raise ValueError(
        ""Gemini code execution tool is not supported for model"" <ANS> ",false,"llm_request.model and llm_request.model.startswith(""gemini-2"")",not self.check_model_type(type_) or self.check_model_type(type_.from_,49.32
"def _megatron_calc_global_rank(tp_rank: int = 0, dp_rank: int = 0, pp_rank: int = 0):
    

    args = get_args()
    tp_size = mpu.get_tensor_model_parallel_world_size()
    dp_size = mpu.get_data_parallel_world_size()
    pp_size = mpu.get_pipeline_model_parallel_world_size()
    assert (tp_size * dp_size * pp_size == torch.distributed.get_world_size()
           ), f""{tp_size} x {dp_size} x {pp_size} != {torch.distributed.get_world_size()}""
    if <IFMASK>:
        # TP-PP-DP grouping
        return (dp_rank * pp_size + pp_rank) * tp_size + tp_rank
    else:
        # TP-DP-PP grouping
        return (pp_rank * dp_size + dp_rank) * tp_size + tp_rank <ANS> ",false,args.switch_dp_and_pp_grouping,tp_rank == 0 and dp_rank == 0 and (tp_size * dp_size) and ( pp,64.07
"def infer_transport_type_from_url(
    url: str | AnyUrl,
) -> Literal[""streamable-http"", ""sse""]:
    
    url = str(url)
    if <IFMASK>:
        raise ValueError(f""Invalid URL: {url}"")

    parsed_url = urlparse(url)
    path = parsed_url.path

    if ""/sse/"" in path or path.rstrip(""/"").endswith(""/sse""):
        return ""sse""
    else: <ANS> ",true,"not url.startswith(""http"")","not isinstance(url, str) or not url.startswith('https://') or (not url.startswith('https://",62.94
"def log_trainable_parameters(model: torch.nn.Module, logger: Optional[Logger] = None):
    
    trainable_params = 0
    all_param = 0
    for name, param in model.named_parameters():
        all_param += param.numel()
        if <IFMASK>:
            (logger or get_logger(__name__)).info(f""training with {name}"")
            trainable_params += param.numel()

    (logger or get_logger(__name__)).info(
        ""trainable params: %s || all params: %s || trainable%%: %s"",
        f""{trainable_params:,}"",
        f""{all_param:,}"",
        f""{trainable_params / all_param:.2%}"", <ANS> ",false,param.requires_grad,not logger or not is_enabled(logger) or not is_enabled(logger) or not is_enabled(,48.82
"def _get_user_query(self, task_send_params: TaskSendParams) -> str | None:
        
        if <IFMASK>:
            logger.warning(f""No message parts found for task {task_send_params.id}"")
            return None
        for part in task_send_params.message.parts:
            # Ensure part is treated as a dictionary if it came from JSON
            part_dict = part if isinstance(part, dict) else part.model_dump()
            if part_dict.get(""type"") == ""text"" and ""text"" in part_dict:
                 return part_dict[""text""]
        logger.warning(f""No text part found in message for task {task_send_params.id}"") <ANS> ",false,not task_send_params.message or not task_send_params.message.parts,self.task_queue is not None and self.task_queue.queue_type == 'task' and (,61.16
"def calculate_cost(input_tokens: int, output_tokens: int, model_id: str) -> float:
    
    if <IFMASK>:
        input_cost_per_k, output_cost_per_k = PRICES_PER_K_TOKENS[model_id]
        input_cost = (input_tokens / 1000) * input_cost_per_k
        output_cost = (output_tokens / 1000) * output_cost_per_k
        return input_cost + output_cost
    else:
        warnings.warn(
            f'Cannot get the costs for {model_id}. The cost will be 0. In your config_list, add field {{""price"" : [prompt_price_per_1k, completion_token_price_per_1k]}} for customized pricing.',
            UserWarning, <ANS> ",true,model_id in PRICES_PER_K_TOKENS,model_id == 'input-ids' and input_ids.shape[1] == 1 and input_ids,71.45
"def get_step_footnote_content(step_log: ActionStep | PlanningStep, step_name: str) -> str:
    
    step_footnote = f""**{step_name}**""
    if <IFMASK>:
        step_footnote += f"" | Input tokens: {step_log.token_usage.input_tokens:,} | Output tokens: {step_log.token_usage.output_tokens:,}""
    step_footnote += f"" | Duration: {round(float(step_log.timing.duration), 2)}s"" if step_log.timing.duration else """"
    step_footnote_content = f
    return step_footnote_content <ANS> ",true,step_log.token_usage is not None,step_log.level != logging.NOTSET and (not step_log.level.value) and (not step,62.08
"def _cancel_interruptible_tasks(self) -> None:
		
		current_task = asyncio.current_task(self.loop)
		for task in asyncio.all_tasks(self.loop):
			if <IFMASK>:
				task_name = task.get_name() if hasattr(task, 'get_name') else str(task)
				# Cancel tasks that match certain patterns
				if any(pattern in task_name for pattern in self.interruptible_task_patterns):
					logger.debug(f'Cancelling task: {task_name}')
					task.cancel()
					# Add exception handler to silence ""Task exception was never retrieved"" warnings
					task.add_done_callback(lambda t: t.exception() if t.cancelled() else None) <ANS> ",true,task != current_task and not task.done(),self.task_log_fetcher and self.task_log_fetcher.done() and (not self.task,65.66
"def compare_hashes_gcs(blob, local_file_path: str) -> bool:
    
    if <IFMASK>:
        remote_md5_base64 = blob.md5_hash
        hash_md5 = hashlib.md5()
        with open(local_file_path, ""rb"") as f:
            for chunk in iter(lambda: f.read(8192), b""""):
                hash_md5.update(chunk)
        local_md5 = hash_md5.digest()
        remote_md5 = base64.b64decode(remote_md5_base64)
        if remote_md5 == local_md5: <ANS> ",false,os.path.exists(local_file_path),not local_file_path.is_dir() or not local_file_path.resolve().is_file,75.10
"def __str__(self) -> str:
        
        info = """"
        info += ""Audio Extraction Task:\n""

        if <IFMASK>:
            info += ""  auth_token: [redacted]\n""
        if self._grpc_endpoint:
            info += f""  grpc_endpoint: {self._grpc_endpoint}\n""
        if self._infer_protocol:
            info += f""  infer_protocol: {self._infer_protocol}\n""
        if self._function_id:
            info += ""  function_id: [redacted]\n""
        if self._use_ssl: <ANS> ",true,self._auth_token,self._use_ssl and self._is_ssl_ca_certs() and self._is_ssl_ca,63.21
"def trim_voice_tensor(tensor):
    
    if <IFMASK>:
        raise ValueError(f""Expected tensor with first dimension 511, got {tensor.shape[0]}"")
    
    # Analyze variance contribution of each row
    variance = analyze_voice_content(tensor)
    
    # Determine which end has lower variance (less information)
    start_var = variance[:5].mean().item()
    end_var = variance[-5:].mean().item() <ANS> ",false,tensor.shape[0] != 511,"isinstance(tensor, np.ndarray) and (not np.isnan(tensor)) and (not np.isinf(",66.41
"def clear_all(cls, cache_dir: Path | None = None) -> None:
        
        cache_dir = cache_dir or default_cache_dir()
        if <IFMASK>:
            return

        file_types: list[Literal[""client_info"", ""tokens""]] = [""client_info"", ""tokens""]
        for file_type in file_types:
            for file in cache_dir.glob(f""*_{file_type}.json""):
                file.unlink(missing_ok=True)
        logger.info(""Cleared all OAuth client cache data."") <ANS> ",false,not cache_dir.exists(),not file_types or not file_data.get('client_info') or not file_data.get(',59.45
"def convert_to_regular_types(obj):
    
    from omegaconf import ListConfig, DictConfig
    if <IFMASK>:
        return {k: convert_to_regular_types(v) for k, v in obj.items()} if isinstance(obj, DictConfig) else list(obj)
    elif isinstance(obj, (list, tuple)):
        return [convert_to_regular_types(x) for x in obj]
    elif isinstance(obj, dict):
        return {k: convert_to_regular_types(v) for k, v in obj.items()}
    return obj <ANS> ",true,"isinstance(obj, (ListConfig, DictConfig))","isinstance(obj, DictConfig) and isinstance(obj, DictConfig) and (not isinstance(obj, DictConfig",76.94
"def load_environment():
    
    try:
        # Read and execute set_keys.sh
        with open('set_keys.sh', 'r') as f:
            lines = f.readlines()
        
        for line in lines:
            line = line.strip()
            if <IFMASK>:
                # Parse export statements
                line = line.replace('export ', '')
                key, value = line.split('=', 1)
                # Remove quotes if present
                value = value.strip('""').strip(""'"")
                os.environ[key] = value
                print(f""Loaded: {key}"") <ANS> ",false,line.startswith('export ') and '=' in line,not os.path.isfile(path) or not os.path.isfile(path) or os.path.,64.95
"def _run(self, **kwargs) -> str:
        
        servers = self.server_manager.client.get_server_names()
        if <IFMASK>:
            return ""No MCP servers are currently defined.""

        result = ""Available MCP servers:\n""
        for i, server_name in enumerate(servers):
            active_marker = "" (ACTIVE)"" if server_name == self.server_manager.active_server else """"
            result += f""{i + 1}. {server_name}{active_marker}\n""

            tools: list = [] <ANS> ",false,not servers,not self.server_manager.is_alive() and self.is_alive() and (not self.is,61.42
"def update_presigned_url(presigned_url, base_url):
    
    #To Do: If Proxy URL has domain name how do we handle such cases? Engineering Dependency.
    
    presigned_parts = urlparse(presigned_url)
    base_parts = urlparse(base_url)
    # Check if base_url contains localhost or an IP address
    if <IFMASK>:
        new_netloc = base_parts.hostname  # Extract domain from base_url
        if base_parts.port:  # Add port if present in base_url
            new_netloc += f"":{base_parts.port}""
        updated_parts = presigned_parts._replace(netloc=new_netloc)
        return urlunparse(updated_parts)
    return presigned_url <ANS> ",false,"re.match(r'^(localhost|\d{1,3}(\.\d{1,3}){3})$', base_parts.hostname)","isinstance(presigned_url, URL) and isinstance(base_url, URL) and (not isinstance(pres",69.55
"def get_aws_session(region_name=None):
    
    profile_name = os.environ.get('AWS_PROFILE')
    region = region_name or get_region()

    if <IFMASK>:
        logger.debug(f'Using AWS profile: {profile_name}')
        return boto3.Session(profile_name=profile_name, region_name=region)
    else:
        logger.debug('Using default AWS credential chain')
        return boto3.Session(region_name=region) <ANS> ",true,profile_name,region_name is None and region is None and (not region) and (not profile_name) and (not,63.89
"def handle_phue_error(
    light_or_group: str, operation: str, error: Exception
) -> dict[str, Any]:
    
    base_info = {""target"": light_or_group, ""operation"": operation, ""success"": False}
    if <IFMASK>:
        base_info[""error""] = f""Target '{light_or_group}' not found""
    elif isinstance(error, PhueException):
        base_info[""error""] = f""phue2 error during {operation}: {error}""
    else:
        base_info[""error""] = f""Unexpected error during {operation}: {error}""
    return base_info <ANS> ",true,"isinstance(error, KeyError)","not isinstance(error, PhueException) or not isinstance(operation, str) or (isinstance(operation, str",64.70
"def to_dict(self) -> Dict:
        
        task_properties = {}

        if <IFMASK>:
            task_properties[""api_key""] = self._api_key

        if self._endpoint_url:
            task_properties[""endpoint_url""] = self._endpoint_url

        if self._prompt:
            task_properties[""prompt""] = self._prompt <ANS> ",true,self._api_key,self._default_branch != 'main' or self._default_branch != 'main' or self._default_,68.06
"def _exists(file_path: str):
    
    if <IFMASK>:
        return _run_cmd(_hdfs_cmd(f""-test -e {file_path}"")) == 0
    return os.path.exists(file_path) <ANS> ",true,"file_path.startswith(""hdfs"")",not file_path.startswith('pytest.') or file_path.startswith('pytest.') or file_path.startswith,70.90
"def language_token(self) -> int:
        
        if <IFMASK>:
            raise ValueError(f""This tokenizer does not have language token configured"")

        additional_tokens = dict(
            zip(
                self.tokenizer.additional_special_tokens,
                self.tokenizer.additional_special_tokens_ids,
            )
        ) <ANS> ",true,self.language is None,self.language_cache and self.language_cache.issubset(additional_tokens) and (not self.additional,63.20
"def _format_search_results_as_context(self, search_results):
        
        context_snippets = []

        for i, result in enumerate(
            search_results[:10]
        ):  # Limit to prevent context overflow
            title = result.get(""title"", ""Untitled"")
            snippet = result.get(""snippet"", """")
            url = result.get(""link"", """")

            if <IFMASK>:
                context_snippets.append(
                    f""Source {i + 1}: {title}\nURL: {url}\nSnippet: {snippet}""
                )

        return ""\n\n"".join(context_snippets) <ANS> ",false,snippet,self.verbose > 0 and self.verbose_feature_names_out is not None and self.verbose_feature,60.96
"def create_html_redline(text1: str, text2: str) -> str:
    
    d = dmp.diff_match_patch()
    diffs = d.diff_main(text2, text1)
    d.diff_cleanupSemantic(diffs)

    html_output = """"
    for op, text in diffs:
        if <IFMASK>:  # Deletion
            html_output += (
                f'<del style=""background-color: #ffcccc;"">{text}</del>'
            )
        elif op == 1:  # Insertion
            html_output += (
                f'<ins style=""background-color: #ccffcc;"">{text}</ins>'
            )
        else:  # Unchanged <ANS> ",false,op == -1,len(text1) != len(text2) and (not text1.startswith(f'x{len,57.64
"def __init__(self, config, actor_module: nn.Module, actor_optimizer: torch.optim.Optimizer = None):
        
        super().__init__(config)
        self.actor_module = actor_module
        self.actor_optimizer = actor_optimizer

        self.use_remove_padding = self.config.get(""use_remove_padding"", False)
        if <IFMASK>:
            print(f""Actor use_remove_padding={self.use_remove_padding}"")
        self.use_fused_kernels = self.config.get(""use_fused_kernels"", False)
        if torch.distributed.get_rank() == 0:
            print(f""Actor use_fused_kernels={self.use_fused_kernels}"")

        self.ulysses_sequence_parallel_size = self.config.ulysses_sequence_parallel_size
        self.use_ulysses_sp = self.ulysses_sequence_parallel_size > 1 <ANS> ",false,torch.distributed.get_rank() == 0,self.use_remove_padding and config.get('use_remove_padding') and (not config.get,64.68
"def collate_fn(data_list: list[dict]) -> dict:
    
    tensors = defaultdict(list)
    non_tensors = defaultdict(list)

    for data in data_list:
        for key, val in data.items():
            if <IFMASK>:
                tensors[key].append(val)
            else:
                non_tensors[key].append(val)

    for key, val in tensors.items():
        tensors[key] = torch.stack(val, dim=0)

    for key, val in non_tensors.items(): <ANS> ",false,"isinstance(val, torch.Tensor)","not isinstance(data, dict) or not isinstance(data_list, dict) or not isinstance(data, list",67.66
"def format_category(
        self, category: RouterCategory, index: int | None = None
    ) -> str:
        

        index_str = f""{index}. "" if index is not None else "" ""
        category_str = """"

        if <IFMASK>:
            category_str = self._format_server_category(category)
        elif isinstance(category, AgentRouterCategory):
            category_str = self._format_agent_category(category)
        else:
            category_str = self._format_function_category(category)

        return f""{index_str}{category_str}"" <ANS> ",true,"isinstance(category, ServerRouterCategory)",self.description is not None and self.description.category != category_str and (self.description.category !=,58.65
"def add_prompt(self, prompt: Prompt, key: str | None = None) -> Prompt:
        
        key = key or prompt.name

        # Check for duplicates
        existing = self._prompts.get(key)
        if <IFMASK>:
            if self.duplicate_behavior == ""warn"":
                logger.warning(f""Prompt already exists: {key}"")
                self._prompts[key] = prompt
            elif self.duplicate_behavior == ""replace"":
                self._prompts[key] = prompt
            elif self.duplicate_behavior == ""error"":
                raise ValueError(f""Prompt already exists: {key}"")
            elif self.duplicate_behavior == ""ignore"": <ANS> ",false,existing,self.config.option.verbose > 1 and (not self.config.option.no_color) and (,58.37
"def database_setup_and_cleanup(db_session: Session) -> Generator[None, None, None]:
    

    inspector = cast(Inspector, inspect(db_session.bind))

    # Check if all tables defined in models are created in the db
    for table in Base.metadata.tables.values():
        if <IFMASK>:
            pytest.exit(f""Table {table} does not exist in the database."")

    clear_database(db_session)
    yield  # This allows the test to run
    clear_database(db_session) <ANS> ",false,not inspector.has_table(table.name),not tables_to_clear and tables_to_clear.exists() and (not tables_to_clear.,69.62
"def decode_unicode_escapes_to_utf8(text: str) -> str:
	

	if <IFMASK>:
		# doesn't have any escape sequences that need to be decoded
		return text

	try:
		# Try to decode Unicode escape sequences
		return text.encode('latin1').decode('unicode_escape')
	except (UnicodeEncodeError, UnicodeDecodeError):
		# logger.debug(f""Failed to decode unicode escape sequences while generating gif text: {text}"") <ANS> ",true,r'\u' not in text,not text.startswith('utf-8') or text.startswith('utf-8') or text.startswith('utf,71.22
"def check_prime(nums: list[int]) -> str:
  
  primes = set()
  for number in nums:
    number = int(number)
    if <IFMASK>:
      continue
    is_prime = True
    for i in range(2, int(number**0.5) + 1):
      if number % i == 0:
        is_prime = False
        break
    if is_prime:
      primes.add(number) <ANS> ",false,number <= 1,not is_prime and (not is_integer_dtype(nums[0])) and (not is_float_,62.40
"def worker_task(thread_id, args):
    
    for i in range(args.iterations):
        iteration = i + 1
        test_id = f""{thread_id:02d}_{iteration:02d}""
        text = generate_test_sentence(thread_id, iteration)
        success = request_tts(
            args.url, test_id, text, args.voice, args.output_dir, args.debug
        )

        if <IFMASK>:
            log_message(
                f""Thread {thread_id}: Iteration {iteration} failed"",
                args.debug,
                is_error=True,
            )

        # Small delay between iterations to avoid overwhelming the API
        time.sleep(0.1) <ANS> ",false,not success,args.verbose_feature_names_out is False and args.verbose_feature_names_out != 'all,55.07
"def _annotate_span_for_generation_message(
        self,
        span: trace.Span,
        message: MessageParamT | str | List[MessageParamT],
    ) -> None:
        
        if <IFMASK>:
            return
        if isinstance(message, str):
            span.set_attribute(""message.content"", message)
        elif isinstance(message, list):
            for i, msg in enumerate(message):
                if isinstance(msg, str):
                    span.set_attribute(f""message.{i}.content"", msg)
                else: <ANS> ",false,not self.context.tracing_enabled,"isinstance(span, (list, tuple)) and len(span) != len(message) and (not isinstance(",65.88
"def __str__(self) -> str:
        
        base = f""{self.action.ljust(11)}. {self.target}""
        if <IFMASK>:
            base += f"" - {self.details}""
        if self.agent_name:
            base = f""[{self.agent_name}] {base}""
        return base <ANS> ",true,self.details,self.wait_for_completion is not None and self.wait_for_completion != 'true' or self,68.06
"def normalize_model_name(name, pp_rank, vpp_rank, transformer_config, layer_name=""layers""):
    
    from verl.utils.megatron_utils import get_transformer_layer_offset

    layer_offset = get_transformer_layer_offset(pp_rank, vpp_rank, transformer_config)

    if <IFMASK>:  # belong to an intermediate layer
        split_name = name.split(""."")
        # find the num next to split_name
        for i, name in enumerate(split_name):
            if name == layer_name:
                break
        layer_num_idx = i + 1
        # check the name
        assert len(split_name) >= layer_num_idx + 1, f""split_name = {split_name}"" <ANS> ",false,layer_name in name,"hasattr(transformer_config, 'get_output_config') and 'layers' in transformer_config.get_",66.99
"def interpreter_feedback(self, output:str) -> str:
        
        if <IFMASK>:
            feedback = f""[failure] Error in execution:\n{output}""
        else:
            feedback = ""[success] Execution success, code output:\n"" + output
        return feedback <ANS> ",true,self.execution_failure_check(output),self.verbose > 0 and self.verbose_mod is not None and (not self.verbose_mod.is,58.03
"def __repr__(self) -> str:
        
        output_str = f""sub question: {self.sub_question}""
        if <IFMASK>:
            output_str += f""\nretrieved chunks:\n{self.chunks}""
        if self.spo:
            output_str += f""\nretrieved spo:\n{self.spo}""
        if self.summary:
            output_str += f""\nsummary:\n{self.summary}""
        return output_str <ANS> ",true,self.chunks,self.sub_process_ids and self.sub_process_ids != self.sub_process_ids and,60.37
"def setup_data_dir():
    
    # Get the project root directory (3 levels up from this file)
    current_dir = os.path.dirname(os.path.abspath(__file__))
    project_root = os.path.abspath(os.path.join(current_dir, "".."", ""..""))

    # Define the data directory path
    data_dir = os.path.join(project_root, ""data"")

    # Create the data directory if it doesn't exist
    if <IFMASK>:
        os.makedirs(data_dir)
        print(f""Created data directory at: {data_dir}"")
    else:
        print(f""Data directory already exists at: {data_dir}"")

    # Return the path to the data directory
    return data_dir <ANS> ",true,not os.path.exists(data_dir),not os.path.exists(data_dir) or not os.path.exists(os.path.join,77.35
"def create_empty_state(
    agent: BaseAgent, initialized_states: Optional[dict[str, Any]] = None
) -> dict[str, Any]:
  
  non_initialized_states = {}
  _create_empty_state(agent, non_initialized_states)
  for key in initialized_states or {}:
    if <IFMASK>:
      del non_initialized_states[key]
  return non_initialized_states <ANS> ",false,key in non_initialized_states,"not isinstance(agent, BaseAgent) or not isinstance(agent, BaseAgent) or not isinstance(agent, Base",67.10
"def _display_quality_summary(self, metrics: Dict[str, Any]):
        
        overall_score = metrics.get(""overall_score"", 0)
        issues = metrics.get(""issues"", [])

        # Use formatter for consistent display
        quality_display = self.formatter.format_quality_score(overall_score, issues)
        self.console.print(f""[dim]{quality_display}[/dim]"")

        # Highlight specific test concerns
        if <IFMASK>:
            self.console.print(
                ""  [yellow]‚ö† Test detected premature answer attempt[/yellow]""
            )

        verbosity = metrics.get(""verbosity"", 0)
        if verbosity > 0.7:
            self.console.print(
                f""  [yellow]‚ö† High verbosity detected ({verbosity:.0%})[/yellow]"" <ANS> ",false,"metrics.get(""premature_attempt"")",self.console.print_as_current_console and self.console.print_as_current_console.,57.14
"def get_markdown(research_id):
    
    conn = get_db_connection()
    conn.row_factory = lambda cursor, row: {
        column[0]: row[idx] for idx, column in enumerate(cursor.description)
    }
    cursor = conn.cursor()
    cursor.execute(""SELECT * FROM research_history WHERE id = ?"", (research_id,))
    result = cursor.fetchone()
    conn.close()

    if <IFMASK>:
        return jsonify({""status"": ""error"", ""message"": ""Report not found""}), 404

    try:
        with open(result[""report_path""], ""r"", encoding=""utf-8"") as f:
            content = f.read()
        return jsonify({""status"": ""success"", ""content"": content})
    except Exception as e:
        return jsonify({""status"": ""error"", ""message"": str(e)}), 500 <ANS> ",false,"not result or not result.get(""report_path"")","isinstance(research_id, str) andresearch_id in self.data_columns and isinstance(self.data",47.83
"def connect(self) -> bool:
        
        if <IFMASK>:
            return True
            
        try:
            self.sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            self.sock.connect((self.host, self.port))
            logger.info(f""Connected to Blender at {self.host}:{self.port}"")
            return True
        except Exception as e: <ANS> ",true,self.sock,self.transport is not None and self.transport.is_closing() and (not self.transport.is_,71.40
"def append_dims(x, target_dims):
    
    dims_to_append = target_dims - x.ndim
    if <IFMASK>:
        raise ValueError(f""input has {x.ndim} dims but target_dims is {target_dims}, which is less"")
    return x[(...,) + (None,) * dims_to_append] <ANS> ",false,dims_to_append < 0,"not isinstance(x, (np.ndarray, list, tuple)) or not isinstance(target_dims, (np",62.93
"def api_delete_resource(research_id, resource_id):
    
    try:
        # Delete the resource
        success = delete_resource(resource_id)

        if <IFMASK>:
            return jsonify(
                {""status"": ""success"", ""message"": ""Resource deleted successfully""}
            )
        else:
            return jsonify({""status"": ""error"", ""message"": ""Resource not found""}), 404
    except Exception as e:
        logger.error(f""Error deleting resource: {str(e)}"")
        return jsonify({""status"": ""error"", ""message"": str(e)}), 500 <ANS> ",false,success,self.delete_on_status_change and self.delete_on_status_callback and self.delete_,70.02
