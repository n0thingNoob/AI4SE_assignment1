id,code,code_tokens,docstring,docstring_tokens
1,"def register_component_producer(self, component_id: str, atom_name: str):
        
        logger.info(f""[DAG] Registering component producer {component_id=} {atom_name=}"")
        if atom_name in self.atoms:
            self._component_producers[component_id] = atom_name
            if self._current_atom:
                logger.info(f""[DAG] Component registered while atom was active self._current_atom={self._current_atom}"")
        else:
            logger.warning(f""[DAG] Skipping producer registration for unknown atom {atom_name=}"")","['def', 'register_component_producer', '(', 'self', ',', 'component_id', ':', 'str', ',', 'atom_name', ':', 'str', ')', ':', 'logger', '.', 'info', '(', 'f', '""', '[DAG] Registering component producer ', '{', 'component_id', '=}', '{', 'atom_name', '=}', '""', ')', 'if', 'atom_name', 'in', 'self', '.', 'atoms', ':', 'self', '.', '_component_producers', '[', 'component_id', ']', '=', 'atom_name', 'if', 'self', '.', '_current_atom', ':', 'logger', '.', 'info', '(', 'f', '""', '[DAG] Component registered while atom was active self._current_atom=', '{', 'self', '.', '_current_atom', '}', '""', ')', 'else', ':', 'logger', '.', 'warning', '(', 'f', '""', '[DAG] Skipping producer registration for unknown atom ', '{', 'atom_name', '=}', '""', ')']","Register a component producer if the atom is recognized, logging the process.","['Register', 'a', 'component', 'producer', 'if', 'the', 'atom', 'is', 'recognized', ',', 'logging', 'the', 'process', '.']"
2,"def process_in_batches(tx, query, data, batch_size):
    
    for i in range(0, len(data), batch_size):
        batch = data[i : i + batch_size]
        tx.run(query, {""nodes"": batch} if ""nodes"" in query else {""edges"": batch})","['def', 'process_in_batches', '(', 'tx', ',', 'query', ',', 'data', ',', 'batch_size', ')', ':', 'for', 'i', 'in', 'range', '(', '0', ',', 'len', '(', 'data', ')', ',', 'batch_size', ')', ':', 'batch', '=', 'data', '[', 'i', ':', 'i', '+', 'batch_size', ']', 'tx', '.', 'run', '(', 'query', ',', '{', '""', 'nodes', '""', ':', 'batch', '}', 'if', '""', 'nodes', '""', 'in', 'query', 'else', '{', '""', 'edges', '""', ':', 'batch', '}', ')']",Execute database queries in specified data batches,"['Execute', 'database', 'queries', 'in', 'specified', 'data', 'batches']"
3,"def filter_property(self, siid: int, piid: int) -> bool:
        
        if (self._cache and 'properties' in self._cache and
            (f'{siid}.{piid}' in self._cache['properties'] or
             f'{siid}.*' in self._cache['properties'])):
            return True
        return False","['def', 'filter_property', '(', 'self', ',', 'siid', ':', 'int', ',', 'piid', ':', 'int', ')', '-', '>', 'bool', ':', 'if', '(', 'self', '.', '_cache', 'and', ""'"", 'properties', ""'"", 'in', 'self', '.', '_cache', 'and', '(', 'f', ""'"", '{', 'siid', '}', '.', '{', 'piid', '}', ""'"", 'in', 'self', '.', '_cache', '[', ""'"", 'properties', ""'"", ']', 'or', 'f', ""'"", '{', 'siid', '}', '.*', ""'"", 'in', 'self', '.', '_cache', '[', ""'"", 'properties', ""'"", ']', ')', ')', ':', 'return', 'True', 'return', 'False']",Check if a property identifier exists in a cached properties list.,"['Check', 'if', 'a', 'property', 'identifier', 'exists', 'in', 'a', 'cached', 'properties', 'list', '.']"
4,"def _save_ontology_to_db(self, ontology: ""Ontology"") -> None:  # type: ignore[no-any-unimported]
        
        if self.ontology_table_name in self.falkordb.list_graphs():
            raise ValueError(f""Knowledge graph {self.name} is already created."")
        graph = self.__get_ontology_storage_graph()
        ontology.save_to_graph(graph)","['def', '_save_ontology_to_db', '(', 'self', ',', 'ontology', ':', '""', 'Ontology', '""', ')', '-', '>', 'None', ':', '# type: ignore[no-any-unimported]', 'if', 'self', '.', 'ontology_table_name', 'in', 'self', '.', 'falkordb', '.', 'list_graphs', '(', ')', ':', 'raise', 'ValueError', '(', 'f', '""', 'Knowledge graph ', '{', 'self', '.', 'name', '}', ' is already created.', '""', ')', 'graph', '=', 'self', '.', '__get_ontology_storage_graph', '(', ')', 'ontology', '.', 'save_to_graph', '(', 'graph', ')']",Save ontology data to a database if not already present.,"['Save', 'ontology', 'data', 'to', 'a', 'database', 'if', 'not', 'already', 'present', '.']"
5,"def _build_merge_function(self):
        

        def merge_fn(ctrl_msg: IngestControlMessage):
            do_trace_tagging = ctrl_msg.get_metadata(""config::add_trace_tagging"") is True
            if do_trace_tagging:
                ts_exit = datetime.now()
                ctrl_msg.set_timestamp(f""trace::exit::{self._task_desc}"", ts_exit)
                ctrl_msg.set_timestamp(""latency::ts_send"", ts_exit)
            return ctrl_msg

        return merge_fn","['def', '_build_merge_function', '(', 'self', ')', ':', 'def', 'merge_fn', '(', 'ctrl_msg', ':', 'IngestControlMessage', ')', ':', 'do_trace_tagging', '=', 'ctrl_msg', '.', 'get_metadata', '(', '""', 'config::add_trace_tagging', '""', ')', 'is', 'True', 'if', 'do_trace_tagging', ':', 'ts_exit', '=', 'datetime', '.', 'now', '(', ')', 'ctrl_msg', '.', 'set_timestamp', '(', 'f', '""', 'trace::exit::', '{', 'self', '.', '_task_desc', '}', '""', ',', 'ts_exit', ')', 'ctrl_msg', '.', 'set_timestamp', '(', '""', 'latency::ts_send', '""', ',', 'ts_exit', ')', 'return', 'ctrl_msg', 'return', 'merge_fn']",Create a function to add trace timestamps to control messages if configured.,"['Create', 'a', 'function', 'to', 'add', 'trace', 'timestamps', 'to', 'control', 'messages', 'if', 'configured', '.']"
6,"def filter_paths(openapi_schema: Dict[str, Any]) -> Dict[str, Any]:
    
    if ""paths"" not in openapi_schema:
        return openapi_schema

    filtered_paths = {}
    for path, path_item in openapi_schema[""paths""].items():
        include_path = False
        filtered_operations = {}

        for method, operation in path_item.items():
            if method.lower() in [""get"", ""post"", ""put"", ""delete"", ""patch""] and is_included_endpoint(
                path, method
            ):
                filtered_operations[method] = operation
                include_path = True

        if include_path:
            filtered_paths[path] = filtered_operations

    openapi_schema[""paths""] = filtered_paths
    return openapi_schema","['def', 'filter_paths', '(', 'openapi_schema', ':', 'Dict', '[', 'str', ',', 'Any', ']', ')', '-', '>', 'Dict', '[', 'str', ',', 'Any', ']', ':', 'if', '""', 'paths', '""', 'not', 'in', 'openapi_schema', ':', 'return', 'openapi_schema', 'filtered_paths', '=', '{', '}', 'for', 'path', ',', 'path_item', 'in', 'openapi_schema', '[', '""', 'paths', '""', ']', '.', 'items', '(', ')', ':', 'include_path', '=', 'False', 'filtered_operations', '=', '{', '}', 'for', 'method', ',', 'operation', 'in', 'path_item', '.', 'items', '(', ')', ':', 'if', 'method', '.', 'lower', '(', ')', 'in', '[', '""', 'get', '""', ',', '""', 'post', '""', ',', '""', 'put', '""', ',', '""', 'delete', '""', ',', '""', 'patch', '""', ']', 'and', 'is_included_endpoint', '(', 'path', ',', 'method', ')', ':', 'filtered_operations', '[', 'method', ']', '=', 'operation', 'include_path', '=', 'True', 'if', 'include_path', ':', 'filtered_paths', '[', 'path', ']', '=', 'filtered_operations', 'openapi_schema', '[', '""', 'paths', '""', ']', '=', 'filtered_paths', 'return', 'openapi_schema']",Filter OpenAPI schema paths to include specific HTTP methods.,"['Filter', 'OpenAPI', 'schema', 'paths', 'to', 'include', 'specific', 'HTTP', 'methods', '.']"
7,"def _handle_task_completion(self, task: asyncio.Task) -> None:
        
        task_id = getattr(task, ""task_id"", ""unknown"")
        self.pending_tasks.discard(task)

        if task.cancelled():
            logger.warning(f""ðŸš« WORKER_CANCELLED [{task_id}] Task was cancelled"")
        elif task.exception() is not None:
            logger.error(
                f""ðŸ’¥ WORKER_EXCEPTION [{task_id}] Task completed with exception: {task.exception()}""
            )
        else:
            logger.info(f""ðŸ WORKER_CLEANUP [{task_id}] Task cleaned up successfully"")","['def', '_handle_task_completion', '(', 'self', ',', 'task', ':', 'asyncio', '.', 'Task', ')', '-', '>', 'None', ':', 'task_id', '=', 'getattr', '(', 'task', ',', '""', 'task_id', '""', ',', '""', 'unknown', '""', ')', 'self', '.', 'pending_tasks', '.', 'discard', '(', 'task', ')', 'if', 'task', '.', 'cancelled', '(', ')', ':', 'logger', '.', 'warning', '(', 'f', '""', 'ðŸš« WORKER_CANCELLED [', '{', 'task_id', '}', '] Task was cancelled', '""', ')', 'elif', 'task', '.', 'exception', '(', ')', 'is', 'not', 'None', ':', 'logger', '.', 'error', '(', 'f', '""', 'ðŸ’¥ WORKER_EXCEPTION [', '{', 'task_id', '}', '] Task completed with exception: ', '{', 'task', '.', 'exception', '(', ')', '}', '""', ')', 'else', ':', 'logger', '.', 'info', '(', 'f', '""', 'ðŸ WORKER_CLEANUP [', '{', 'task_id', '}', '] Task cleaned up successfully', '""', ')']","Handle task completion by logging cancellation, exceptions, or successful cleanup","['Handle', 'task', 'completion', 'by', 'logging', 'cancellation', ',', 'exceptions', ',', 'or', 'successful', 'cleanup']"
8,"def _get_mime_type(self) -> str:
        
        if self._format:
            return f""image/{self._format.lower()}""

        if self.path:
            suffix = self.path.suffix.lower()
            return {
                "".png"": ""image/png"",
                "".jpg"": ""image/jpeg"",
                "".jpeg"": ""image/jpeg"",
                "".gif"": ""image/gif"",
                "".webp"": ""image/webp"",
            }.get(suffix, ""application/octet-stream"")
        return ""image/png""","['def', '_get_mime_type', '(', 'self', ')', '-', '>', 'str', ':', 'if', 'self', '.', '_format', ':', 'return', 'f', '""', 'image/', '{', 'self', '.', '_format', '.', 'lower', '(', ')', '}', '""', 'if', 'self', '.', 'path', ':', 'suffix', '=', 'self', '.', 'path', '.', 'suffix', '.', 'lower', '(', ')', 'return', '{', '""', '.png', '""', ':', '""', 'image/png', '""', ',', '""', '.jpg', '""', ':', '""', 'image/jpeg', '""', ',', '""', '.jpeg', '""', ':', '""', 'image/jpeg', '""', ',', '""', '.gif', '""', ':', '""', 'image/gif', '""', ',', '""', '.webp', '""', ':', '""', 'image/webp', '""', ',', '}', '.', 'get', '(', 'suffix', ',', '""', 'application/octet-stream', '""', ')', 'return', '""', 'image/png', '""']",Determine the MIME type of an image based on its format or file extension.,"['Determine', 'the', 'MIME', 'type', 'of', 'an', 'image', 'based', 'on', 'its', 'format', 'or', 'file', 'extension', '.']"
10,"def check_and_install_dependencies():
    
    required_packages = [
        ""gunicorn"",
        ""tiktoken"",
        ""psutil"",
        # Add other required packages here
    ]

    for package in required_packages:
        if not pm.is_installed(package):
            print(f""Installing {package}..."")
            pm.install(package)
            print(f""{package} installed successfully"")","['def', 'check_and_install_dependencies', '(', ')', ':', 'required_packages', '=', '[', '""', 'gunicorn', '""', ',', '""', 'tiktoken', '""', ',', '""', 'psutil', '""', ',', '# Add other required packages here', ']', 'for', 'package', 'in', 'required_packages', ':', 'if', 'not', 'pm', '.', 'is_installed', '(', 'package', ')', ':', 'print', '(', 'f', '""', 'Installing ', '{', 'package', '}', '...', '""', ')', 'pm', '.', 'install', '(', 'package', ')', 'print', '(', 'f', '""', '{', 'package', '}', ' installed successfully', '""', ')']","Ensure required software packages are installed, installing if necessary.","['Ensure', 'required', 'software', 'packages', 'are', 'installed', ',', 'installing', 'if', 'necessary', '.']"
11,"def get(cls, name: str, raise_on_missing: bool = True) -> Optional[T]:
        

        matches = [registered for registered in cls._get_storage() if re.match(registered, name)]

        if len(matches) > 1:
            raise ValueError(f""Multiple taggers match {name}: {', '.join(matches)}"")

        elif len(matches) == 0:
            if raise_on_missing:
                tagger_names = "", "".join([tn for tn, _ in cls.items()])
                raise ValueError(f""Unknown tagger {name}; available taggers: {tagger_names}"")
            return None

        else:
            name = matches[0]
            t, _ = cls._get_storage()[name]
            return t","['def', 'get', '(', 'cls', ',', 'name', ':', 'str', ',', 'raise_on_missing', ':', 'bool', '=', 'True', ')', '-', '>', 'Optional', '[', 'T', ']', ':', 'matches', '=', '[', 'registered', 'for', 'registered', 'in', 'cls', '.', '_get_storage', '(', ')', 'if', 're', '.', 'match', '(', 'registered', ',', 'name', ')', ']', 'if', 'len', '(', 'matches', ')', '>', '1', ':', 'raise', 'ValueError', '(', 'f', '""', 'Multiple taggers match ', '{', 'name', '}', ': ', '{', ""'"", ', ', ""'"", '.', 'join', '(', 'matches', ')', '}', '""', ')', 'elif', 'len', '(', 'matches', ')', '==', '0', ':', 'if', 'raise_on_missing', ':', 'tagger_names', '=', '""', ', ', '""', '.', 'join', '(', '[', 'tn', 'for', 'tn', ',', '_', 'in', 'cls', '.', 'items', '(', ')', ']', ')', 'raise', 'ValueError', '(', 'f', '""', 'Unknown tagger ', '{', 'name', '}', '; available taggers: ', '{', 'tagger_names', '}', '""', ')', 'return', 'None', 'else', ':', 'name', '=', 'matches', '[', '0', ']', 't', ',', '_', '=', 'cls', '.', '_get_storage', '(', ')', '[', 'name', ']', 'return', 't']",Retrieve or raise error for a specific tagger based on name matching,"['Retrieve', 'or', 'raise', 'error', 'for', 'a', 'specific', 'tagger', 'based', 'on', 'name', 'matching']"
12,"def get_single_param_type_from_schema(param_schema: Dict[str, Any]) -> str:
    
    if ""anyOf"" in param_schema:
        types = {schema.get(""type"") for schema in param_schema[""anyOf""] if schema.get(""type"")}
        if ""null"" in types:
            types.remove(""null"")
        if types:
            return next(iter(types))
        return ""string""
    return param_schema.get(""type"", ""string"")","['def', 'get_single_param_type_from_schema', '(', 'param_schema', ':', 'Dict', '[', 'str', ',', 'Any', ']', ')', '-', '>', 'str', ':', 'if', '""', 'anyOf', '""', 'in', 'param_schema', ':', 'types', '=', '{', 'schema', '.', 'get', '(', '""', 'type', '""', ')', 'for', 'schema', 'in', 'param_schema', '[', '""', 'anyOf', '""', ']', 'if', 'schema', '.', 'get', '(', '""', 'type', '""', ')', '}', 'if', '""', 'null', '""', 'in', 'types', ':', 'types', '.', 'remove', '(', '""', 'null', '""', ')', 'if', 'types', ':', 'return', 'next', '(', 'iter', '(', 'types', ')', ')', 'return', '""', 'string', '""', 'return', 'param_schema', '.', 'get', '(', '""', 'type', '""', ',', '""', 'string', '""', ')']","Determine parameter type from schema, defaulting to ""string"" if unspecified","['Determine', 'parameter', 'type', 'from', 'schema', ',', 'defaulting', 'to', '``', 'string', ""''"", 'if', 'unspecified']"
13,"def start(self):
        
        try:
            self.reset_trace()
            self.current_trace[""start_time""] = datetime.now()
            self._active = True
            self._monkey_patch()

            if self.save_interval:
                loop = asyncio.get_event_loop()
                self._save_task = loop.create_task(self._periodic_save())

            logger.info(""Tracing started"")
        except Exception as e:
            logger.error(f""Error starting tracer: {e}"")
            self.on_error(e, context=""start"")
            raise","['def', 'start', '(', 'self', ')', ':', 'try', ':', 'self', '.', 'reset_trace', '(', ')', 'self', '.', 'current_trace', '[', '""', 'start_time', '""', ']', '=', 'datetime', '.', 'now', '(', ')', 'self', '.', '_active', '=', 'True', 'self', '.', '_monkey_patch', '(', ')', 'if', 'self', '.', 'save_interval', ':', 'loop', '=', 'asyncio', '.', 'get_event_loop', '(', ')', 'self', '.', '_save_task', '=', 'loop', '.', 'create_task', '(', 'self', '.', '_periodic_save', '(', ')', ')', 'logger', '.', 'info', '(', '""', 'Tracing started', '""', ')', 'except', 'Exception', 'as', 'e', ':', 'logger', '.', 'error', '(', 'f', '""', 'Error starting tracer: ', '{', 'e', '}', '""', ')', 'self', '.', 'on_error', '(', 'e', ',', 'context', '=', '""', 'start', '""', ')', 'raise']",Initialize and start a tracing process with error handling and periodic saving.,"['Initialize', 'and', 'start', 'a', 'tracing', 'process', 'with', 'error', 'handling', 'and', 'periodic', 'saving', '.']"
14,"def generate_audio_from_phonemes(phonemes: str, voice: str = ""af_bella""):
    
    response = requests.post(
        json={""phonemes"": phonemes, ""voice"": voice},
        headers={""Accept"": ""audio/wav""}
    )
    if response.status_code != 200:
        print(f""Error: {response.text}"")
        return None
    return response.content","['def', 'generate_audio_from_phonemes', '(', 'phonemes', ':', 'str', ',', 'voice', ':', 'str', '=', '""', 'af_bella', '""', ')', ':', 'response', '=', 'requests', '.', 'post', '(', 'json', '=', '{', '""', 'phonemes', '""', ':', 'phonemes', ',', '""', 'voice', '""', ':', 'voice', '}', ',', 'headers', '=', '{', '""', 'Accept', '""', ':', '""', 'audio/wav', '""', '}', ')', 'if', 'response', '.', 'status_code', '!=', '200', ':', 'print', '(', 'f', '""', 'Error: ', '{', 'response', '.', 'text', '}', '""', ')', 'return', 'None', 'return', 'response', '.', 'content']",Convert phonetic input into audio using specified voice settings.,"['Convert', 'phonetic', 'input', 'into', 'audio', 'using', 'specified', 'voice', 'settings', '.']"
15,"def _build_conversation_context(self) -> str:
        
        context_parts = []

        # Include recent messages (last 5 for now)
        recent_messages = (
            self.state.messages[-5:]
            if len(self.state.messages) > 5
            else self.state.messages
        )

        for msg in recent_messages:
            if msg.role != ""system"":  # Skip system message in context
                role_label = ""User"" if msg.role == ""user"" else ""Assistant""
                context_parts.append(f""{role_label}: {msg.content}"")

        return (
            ""\n"".join(context_parts)
            if context_parts
            else ""This is the start of our conversation.""
        )","['def', '_build_conversation_context', '(', 'self', ')', '-', '>', 'str', ':', 'context_parts', '=', '[', ']', '# Include recent messages (last 5 for now)', 'recent_messages', '=', '(', 'self', '.', 'state', '.', 'messages', '[', '-', '5', ':', ']', 'if', 'len', '(', 'self', '.', 'state', '.', 'messages', ')', '>', '5', 'else', 'self', '.', 'state', '.', 'messages', ')', 'for', 'msg', 'in', 'recent_messages', ':', 'if', 'msg', '.', 'role', '!=', '""', 'system', '""', ':', '# Skip system message in context', 'role_label', '=', '""', 'User', '""', 'if', 'msg', '.', 'role', '==', '""', 'user', '""', 'else', '""', 'Assistant', '""', 'context_parts', '.', 'append', '(', 'f', '""', '{', 'role_label', '}', ': ', '{', 'msg', '.', 'content', '}', '""', ')', 'return', '(', '""', '\\n', '""', '.', 'join', '(', 'context_parts', ')', 'if', 'context_parts', 'else', '""', 'This is the start of our conversation.', '""', ')']",Constructs a conversation context string from recent non-system messages.,"['Constructs', 'a', 'conversation', 'context', 'string', 'from', 'recent', 'non-system', 'messages', '.']"
16,"def to_api_params(self) -> Dict[str, JsonValue]:
        
        params: Dict[str, JsonValue] = {
            ""includeResolvedCases"": cast(JsonValue, self.include_resolved_cases),
            ""includeCommunications"": cast(JsonValue, self.include_communications),
            ""language"": cast(JsonValue, self.language),
        }

        if self.case_id_list:
            params[""caseIdList""] = cast(JsonValue, self.case_id_list)
        if self.display_id:
            params[""displayId""] = cast(JsonValue, self.display_id)
        if self.after_time:
            params[""afterTime""] = cast(JsonValue, self.after_time)
        if self.before_time:
            params[""beforeTime""] = cast(JsonValue, self.before_time)
        if self.max_results:
            params[""maxResults""] = cast(JsonValue, self.max_results)
        if self.next_token:
            params[""nextToken""] = cast(JsonValue, self.next_token)

        return params","['def', 'to_api_params', '(', 'self', ')', '-', '>', 'Dict', '[', 'str', ',', 'JsonValue', ']', ':', 'params', ':', 'Dict', '[', 'str', ',', 'JsonValue', ']', '=', '{', '""', 'includeResolvedCases', '""', ':', 'cast', '(', 'JsonValue', ',', 'self', '.', 'include_resolved_cases', ')', ',', '""', 'includeCommunications', '""', ':', 'cast', '(', 'JsonValue', ',', 'self', '.', 'include_communications', ')', ',', '""', 'language', '""', ':', 'cast', '(', 'JsonValue', ',', 'self', '.', 'language', ')', ',', '}', 'if', 'self', '.', 'case_id_list', ':', 'params', '[', '""', 'caseIdList', '""', ']', '=', 'cast', '(', 'JsonValue', ',', 'self', '.', 'case_id_list', ')', 'if', 'self', '.', 'display_id', ':', 'params', '[', '""', 'displayId', '""', ']', '=', 'cast', '(', 'JsonValue', ',', 'self', '.', 'display_id', ')', 'if', 'self', '.', 'after_time', ':', 'params', '[', '""', 'afterTime', '""', ']', '=', 'cast', '(', 'JsonValue', ',', 'self', '.', 'after_time', ')', 'if', 'self', '.', 'before_time', ':', 'params', '[', '""', 'beforeTime', '""', ']', '=', 'cast', '(', 'JsonValue', ',', 'self', '.', 'before_time', ')', 'if', 'self', '.', 'max_results', ':', 'params', '[', '""', 'maxResults', '""', ']', '=', 'cast', '(', 'JsonValue', ',', 'self', '.', 'max_results', ')', 'if', 'self', '.', 'next_token', ':', 'params', '[', '""', 'nextToken', '""', ']', '=', 'cast', '(', 'JsonValue', ',', 'self', '.', 'next_token', ')', 'return', 'params']",Convert object attributes into a dictionary of API query parameters.,"['Convert', 'object', 'attributes', 'into', 'a', 'dictionary', 'of', 'API', 'query', 'parameters', '.']"
17,"def interpreter_feedback(self, output: str) -> str:
        
        if not output:
            raise ValueError(""No output to interpret."")
        return f","['def', 'interpreter_feedback', '(', 'self', ',', 'output', ':', 'str', ')', '-', '>', 'str', ':', 'if', 'not', 'output', ':', 'raise', 'ValueError', '(', '""', 'No output to interpret.', '""', ')', 'return', 'f']",Function checks for empty output and raises an error if none is found.,"['Function', 'checks', 'for', 'empty', 'output', 'and', 'raises', 'an', 'error', 'if', 'none', 'is', 'found', '.']"
18,"def _load_model(self):
        
        try:
            # Check if adaptive-classifier is installed
            try:
                import adaptive_classifier
            except ImportError:
                logger.info(""Installing adaptive-classifier library..."")
                os.system(f""{sys.executable} -m pip install adaptive-classifier"")
                import adaptive_classifier
            
            # Import the AdaptiveClassifier class
            from adaptive_classifier import AdaptiveClassifier
            
            logger.info(f""Loading complexity classifier model: {self.model_name}"")
            self.classifier = AdaptiveClassifier.from_pretrained(self.model_name)
            logger.info(""Classifier loaded successfully"")
            
        except Exception as e:
            logger.error(f""Error loading complexity classifier: {e}"")
            # Fallback to basic classification if model fails to load
            self.classifier = None","['def', '_load_model', '(', 'self', ')', ':', 'try', ':', '# Check if adaptive-classifier is installed', 'try', ':', 'import', 'adaptive_classifier', 'except', 'ImportError', ':', 'logger', '.', 'info', '(', '""', 'Installing adaptive-classifier library...', '""', ')', 'os', '.', 'system', '(', 'f', '""', '{', 'sys', '.', 'executable', '}', ' -m pip install adaptive-classifier', '""', ')', 'import', 'adaptive_classifier', '# Import the AdaptiveClassifier class', 'from', 'adaptive_classifier', 'import', 'AdaptiveClassifier', 'logger', '.', 'info', '(', 'f', '""', 'Loading complexity classifier model: ', '{', 'self', '.', 'model_name', '}', '""', ')', 'self', '.', 'classifier', '=', 'AdaptiveClassifier', '.', 'from_pretrained', '(', 'self', '.', 'model_name', ')', 'logger', '.', 'info', '(', '""', 'Classifier loaded successfully', '""', ')', 'except', 'Exception', 'as', 'e', ':', 'logger', '.', 'error', '(', 'f', '""', 'Error loading complexity classifier: ', '{', 'e', '}', '""', ')', '# Fallback to basic classification if model fails to load', 'self', '.', 'classifier', '=', 'None']","Load or install and initialize an adaptive complexity classifier model, with error handling.","['Load', 'or', 'install', 'and', 'initialize', 'an', 'adaptive', 'complexity', 'classifier', 'model', ',', 'with', 'error', 'handling', '.']"
20,"def __init__(self, agent: SwarmAgent, context_variables: Dict[str, str] = None):
        
        super().__init__(agent=agent)
        self.context_variables = defaultdict(str, context_variables or {})
        self.instruction = (
            agent.instruction(self.context_variables)
            if isinstance(agent.instruction, Callable)
            else agent.instruction
        )
        logger.debug(
            f""Swarm initialized with agent {agent.name}"",
            data={
                ""context_variables"": self.context_variables,
                ""instruction"": self.instruction,
            },
        )","['def', '__init__', '(', 'self', ',', 'agent', ':', 'SwarmAgent', ',', 'context_variables', ':', 'Dict', '[', 'str', ',', 'str', ']', '=', 'None', ')', ':', 'super', '(', ')', '.', '__init__', '(', 'agent', '=', 'agent', ')', 'self', '.', 'context_variables', '=', 'defaultdict', '(', 'str', ',', 'context_variables', 'or', '{', '}', ')', 'self', '.', 'instruction', '=', '(', 'agent', '.', 'instruction', '(', 'self', '.', 'context_variables', ')', 'if', 'isinstance', '(', 'agent', '.', 'instruction', ',', 'Callable', ')', 'else', 'agent', '.', 'instruction', ')', 'logger', '.', 'debug', '(', 'f', '""', 'Swarm initialized with agent ', '{', 'agent', '.', 'name', '}', '""', ',', 'data', '=', '{', '""', 'context_variables', '""', ':', 'self', '.', 'context_variables', ',', '""', 'instruction', '""', ':', 'self', '.', 'instruction', ',', '}', ',', ')']",Initialize a swarm agent with context variables and log the setup details.,"['Initialize', 'a', 'swarm', 'agent', 'with', 'context', 'variables', 'and', 'log', 'the', 'setup', 'details', '.']"
21,"def auto_discover_processors():
    
    # get processors directory path
    processors_path = Path(__file__).parent.parent / ""processors""

    # iterate over all subdirectories in processors directory
    for _, name, _ in pkgutil.iter_modules([str(processors_path)]):
        # if it is a directory and contains processor.py
        processor_file = processors_path / name / ""processor.py""
        if processor_file.exists():
            module_path = f""lpm_kernel.file_data.processors.{name}.processor""
            try:
                importlib.import_module(module_path)
            except ImportError as e:
                print(f""Failed to load processor module {module_path}: {e}"")","['def', 'auto_discover_processors', '(', ')', ':', '# get processors directory path', 'processors_path', '=', 'Path', '(', '__file__', ')', '.', 'parent', '.', 'parent', '/', '""', 'processors', '""', '# iterate over all subdirectories in processors directory', 'for', '_', ',', 'name', ',', '_', 'in', 'pkgutil', '.', 'iter_modules', '(', '[', 'str', '(', 'processors_path', ')', ']', ')', ':', '# if it is a directory and contains processor.py', 'processor_file', '=', 'processors_path', '/', 'name', '/', '""', 'processor.py', '""', 'if', 'processor_file', '.', 'exists', '(', ')', ':', 'module_path', '=', 'f', '""', 'lpm_kernel.file_data.processors.', '{', 'name', '}', '.processor', '""', 'try', ':', 'importlib', '.', 'import_module', '(', 'module_path', ')', 'except', 'ImportError', 'as', 'e', ':', 'print', '(', 'f', '""', 'Failed to load processor module ', '{', 'module_path', '}', ': ', '{', 'e', '}', '""', ')']",Automatically discover and import processor modules from a specified directory path.,"['Automatically', 'discover', 'and', 'import', 'processor', 'modules', 'from', 'a', 'specified', 'directory', 'path', '.']"
24,"def download_youtube_video(video_url, output_path):
    
    # video_url, output_path = info
    try:
        if not os.path.exists(os.path.dirname(output_path)):
            os.makedirs(os.path.dirname(output_path), exist_ok=True)
        # download command
        command = ['yt-dlp', '-f', 'bestvideo[ext=mp4]+bestaudio[ext=m4a]/best[ext=mp4]', '--merge-output-format',
               'mp4', '--output', output_path , video_url]
        # subprocess.run
        result = subprocess.run(command, capture_output=True, text=True, encoding='utf-8')

        if result.returncode == 0:
            print('Download {:s} successfully!'.format(video_url))
        else:
            print(""Fail to download {:s}, error info:\n{:s}"".format(video_url, result.stderr))
    except Exception as e:
        print(f""error: {e}"")","['def', 'download_youtube_video', '(', 'video_url', ',', 'output_path', ')', ':', '# video_url, output_path = info', 'try', ':', 'if', 'not', 'os', '.', 'path', '.', 'exists', '(', 'os', '.', 'path', '.', 'dirname', '(', 'output_path', ')', ')', ':', 'os', '.', 'makedirs', '(', 'os', '.', 'path', '.', 'dirname', '(', 'output_path', ')', ',', 'exist_ok', '=', 'True', ')', '# download command', 'command', '=', '[', ""'"", 'yt-dlp', ""'"", ',', ""'"", '-f', ""'"", ',', ""'"", 'bestvideo[ext=mp4]+bestaudio[ext=m4a]/best[ext=mp4]', ""'"", ',', ""'"", '--merge-output-format', ""'"", ',', ""'"", 'mp4', ""'"", ',', ""'"", '--output', ""'"", ',', 'output_path', ',', 'video_url', ']', '# subprocess.run', 'result', '=', 'subprocess', '.', 'run', '(', 'command', ',', 'capture_output', '=', 'True', ',', 'text', '=', 'True', ',', 'encoding', '=', ""'"", 'utf-8', ""'"", ')', 'if', 'result', '.', 'returncode', '==', '0', ':', 'print', '(', ""'"", 'Download ', '{:s}', ' successfully!', ""'"", '.', 'format', '(', 'video_url', ')', ')', 'else', ':', 'print', '(', '""', 'Fail to download ', '{:s}', ', error info:', '\\n', '{:s}', '""', '.', 'format', '(', 'video_url', ',', 'result', '.', 'stderr', ')', ')', 'except', 'Exception', 'as', 'e', ':', 'print', '(', 'f', '""', 'error: ', '{', 'e', '}', '""', ')']",Download YouTube video to specified path using yt-dlp command.,"['Download', 'YouTube', 'video', 'to', 'specified', 'path', 'using', 'yt-dlp', 'command', '.']"
25,"def sub_device_state(
        self, did: str, handler: Callable[[str, MIoTDeviceState, Any], None],
        handler_ctx: Any = None
    ) -> bool:
        
        if did not in self._device_list_cache:
            raise MIoTClientError(f'did not exist, {did}')
        self._sub_device_state[did] = MipsDeviceState(
            did=did, handler=handler, handler_ctx=handler_ctx)
        _LOGGER.debug('client sub device state, %s', did)
        return True","['def', 'sub_device_state', '(', 'self', ',', 'did', ':', 'str', ',', 'handler', ':', 'Callable', '[', '[', 'str', ',', 'MIoTDeviceState', ',', 'Any', ']', ',', 'None', ']', ',', 'handler_ctx', ':', 'Any', '=', 'None', ')', '-', '>', 'bool', ':', 'if', 'did', 'not', 'in', 'self', '.', '_device_list_cache', ':', 'raise', 'MIoTClientError', '(', 'f', ""'"", 'did not exist, ', '{', 'did', '}', ""'"", ')', 'self', '.', '_sub_device_state', '[', 'did', ']', '=', 'MipsDeviceState', '(', 'did', '=', 'did', ',', 'handler', '=', 'handler', ',', 'handler_ctx', '=', 'handler_ctx', ')', '_LOGGER', '.', 'debug', '(', ""'"", 'client sub device state, ', '%s', ""'"", ',', 'did', ')', 'return', 'True']","Subscribe to device state updates, raising error if device ID is invalid.","['Subscribe', 'to', 'device', 'state', 'updates', ',', 'raising', 'error', 'if', 'device', 'ID', 'is', 'invalid', '.']"
26,"def generate_audio_from_phonemes(phonemes: str, voice: str = ""af_bella"") -> Optional[bytes]:
    
    response = requests.post(
        json={""phonemes"": phonemes, ""voice"": voice},
        headers={""Accept"": ""audio/wav""}
    )
    
    print(f""Response status: {response.status_code}"")
    print(f""Response headers: {dict(response.headers)}"")
    print(f""Response content type: {response.headers.get('Content-Type')}"")
    print(f""Response length: {len(response.content)} bytes"")
    
    if response.status_code != 200:
        print(f""Error response: {response.text}"")
        return None
        
    if not response.content:
        print(""Error: Empty response content"")
        return None
        
    return response.content","['def', 'generate_audio_from_phonemes', '(', 'phonemes', ':', 'str', ',', 'voice', ':', 'str', '=', '""', 'af_bella', '""', ')', '-', '>', 'Optional', '[', 'bytes', ']', ':', 'response', '=', 'requests', '.', 'post', '(', 'json', '=', '{', '""', 'phonemes', '""', ':', 'phonemes', ',', '""', 'voice', '""', ':', 'voice', '}', ',', 'headers', '=', '{', '""', 'Accept', '""', ':', '""', 'audio/wav', '""', '}', ')', 'print', '(', 'f', '""', 'Response status: ', '{', 'response', '.', 'status_code', '}', '""', ')', 'print', '(', 'f', '""', 'Response headers: ', '{', 'dict', '(', 'response', '.', 'headers', ')', '}', '""', ')', 'print', '(', 'f', '""', 'Response content type: ', '{', 'response', '.', 'headers', '.', 'get', '(', ""'"", 'Content-Type', ""'"", ')', '}', '""', ')', 'print', '(', 'f', '""', 'Response length: ', '{', 'len', '(', 'response', '.', 'content', ')', '}', ' bytes', '""', ')', 'if', 'response', '.', 'status_code', '!=', '200', ':', 'print', '(', 'f', '""', 'Error response: ', '{', 'response', '.', 'text', '}', '""', ')', 'return', 'None', 'if', 'not', 'response', '.', 'content', ':', 'print', '(', '""', 'Error: Empty response content', '""', ')', 'return', 'None', 'return', 'response', '.', 'content']",Convert phonemes to audio using a specified voice and return the audio data.,"['Convert', 'phonemes', 'to', 'audio', 'using', 'a', 'specified', 'voice', 'and', 'return', 'the', 'audio', 'data', '.']"
27,"def _setup_user_dir(self):
        
        if '--user-data-dir' not in [
            arg.split('=')[0] for arg in self.options.arguments
        ]:
            # For all browsers, use a temporary directory
            temp_dir = self._temp_directory_manager.create_temp_dir()
            self.options.arguments.append(f'--user-data-dir={temp_dir.name}')","['def', '_setup_user_dir', '(', 'self', ')', ':', 'if', ""'"", '--user-data-dir', ""'"", 'not', 'in', '[', 'arg', '.', 'split', '(', ""'"", '=', ""'"", ')', '[', '0', ']', 'for', 'arg', 'in', 'self', '.', 'options', '.', 'arguments', ']', ':', '# For all browsers, use a temporary directory', 'temp_dir', '=', 'self', '.', '_temp_directory_manager', '.', 'create_temp_dir', '(', ')', 'self', '.', 'options', '.', 'arguments', '.', 'append', '(', 'f', ""'"", '--user-data-dir=', '{', 'temp_dir', '.', 'name', '}', ""'"", ')']",Ensure user data directory is set for browser options,"['Ensure', 'user', 'data', 'directory', 'is', 'set', 'for', 'browser', 'options']"
28,"def __post_init__(
        self,
    ) -> ""BaseSecurity"":  # dataclasses uses __post_init__ instead of model_validator
        
        valid_in_values = {
            ""apiKey"": [""header"", ""query"", ""cookie""],
            ""http"": [""bearer"", ""basic""],
            ""oauth2"": [""bearer""],
            ""openIdConnect"": [""bearer""],
            ""mutualTLS"": [""tls""],
            ""unsupported"": [""unsupported""],
        }
        if self.in_value not in valid_in_values[self.type]:
            raise ValueError(f""Invalid in_value '{self.in_value}' for type '{self.type}'"")
        return self","['def', '__post_init__', '(', 'self', ',', ')', '-', '>', '""', 'BaseSecurity', '""', ':', '# dataclasses uses __post_init__ instead of model_validator', 'valid_in_values', '=', '{', '""', 'apiKey', '""', ':', '[', '""', 'header', '""', ',', '""', 'query', '""', ',', '""', 'cookie', '""', ']', ',', '""', 'http', '""', ':', '[', '""', 'bearer', '""', ',', '""', 'basic', '""', ']', ',', '""', 'oauth2', '""', ':', '[', '""', 'bearer', '""', ']', ',', '""', 'openIdConnect', '""', ':', '[', '""', 'bearer', '""', ']', ',', '""', 'mutualTLS', '""', ':', '[', '""', 'tls', '""', ']', ',', '""', 'unsupported', '""', ':', '[', '""', 'unsupported', '""', ']', ',', '}', 'if', 'self', '.', 'in_value', 'not', 'in', 'valid_in_values', '[', 'self', '.', 'type', ']', ':', 'raise', 'ValueError', '(', 'f', '""', 'Invalid in_value ', ""'"", '{', 'self', '.', 'in_value', '}', ""'"", ' for type ', ""'"", '{', 'self', '.', 'type', '}', ""'"", '""', ')', 'return', 'self']",Validate security configuration based on type and location constraints.,"['Validate', 'security', 'configuration', 'based', 'on', 'type', 'and', 'location', 'constraints', '.']"
31,"def __call__(
      self, query_sequence: str, chain_polymer_type: str
  ) -> tuple[msa.Msa, MsaErrors]:
    
    if chain_polymer_type != self._chain_polymer_type:
      raise ValueError(
          f'EmptyMsaProvider of type {self._chain_polymer_type} called with '
          f'sequence of {chain_polymer_type=}, {query_sequence=}.'
      )
    return (
        msa.Msa.from_empty(
            query_sequence=query_sequence,
            chain_poly_type=self._chain_polymer_type,
        ),
        (),
    )","['def', '__call__', '(', 'self', ',', 'query_sequence', ':', 'str', ',', 'chain_polymer_type', ':', 'str', ')', '-', '>', 'tuple', '[', 'msa', '.', 'Msa', ',', 'MsaErrors', ']', ':', 'if', 'chain_polymer_type', '!=', 'self', '.', '_chain_polymer_type', ':', 'raise', 'ValueError', '(', 'f', ""'"", 'EmptyMsaProvider of type ', '{', 'self', '.', '_chain_polymer_type', '}', ' called with ', ""'"", 'f', ""'"", 'sequence of ', '{', 'chain_polymer_type', '=}', ', ', '{', 'query_sequence', '=}', '.', ""'"", ')', 'return', '(', 'msa', '.', 'Msa', '.', 'from_empty', '(', 'query_sequence', '=', 'query_sequence', ',', 'chain_poly_type', '=', 'self', '.', '_chain_polymer_type', ',', ')', ',', '(', ')', ',', ')']",Validate polymer type and return empty MSA for a query sequence.,"['Validate', 'polymer', 'type', 'and', 'return', 'empty', 'MSA', 'for', 'a', 'query', 'sequence', '.']"
32,"def interpreter_feedback(self, output: str) -> str:
        
        if self.execution_failure_check(output):
            return f""Web search failed: {output}""
        return f""Web search result:\n{output}""","['def', 'interpreter_feedback', '(', 'self', ',', 'output', ':', 'str', ')', '-', '>', 'str', ':', 'if', 'self', '.', 'execution_failure_check', '(', 'output', ')', ':', 'return', 'f', '""', 'Web search failed: ', '{', 'output', '}', '""', 'return', 'f', '""', 'Web search result:', '\\n', '{', 'output', '}', '""']",The function evaluates execution success and formats web search feedback.,"['The', 'function', 'evaluates', 'execution', 'success', 'and', 'formats', 'web', 'search', 'feedback', '.']"
34,"def initialize_cache(self, num_gpu_blocks: int, num_cpu_blocks: int) -> None:
        

        # NOTE: We log here to avoid multiple logs when number of workers is
        # greater than one. We could log in the engine, but not all executors
        # have GPUs.
        logger.info(""# GPU blocks: %d, # CPU blocks: %d"", num_gpu_blocks, num_cpu_blocks)

        self.cache_config.num_gpu_blocks = num_gpu_blocks
        self.cache_config.num_cpu_blocks = num_cpu_blocks

        if torch.distributed.get_rank() == 0:
            print(f""before init cache memory allocated: {torch.cuda.memory_allocated() / 1e9}GB, reserved: {torch.cuda.memory_reserved() / 1e9}GB"")
        self.worker.initialize_cache(num_gpu_blocks=num_gpu_blocks, num_cpu_blocks=num_cpu_blocks)
        if torch.distributed.get_rank() == 0:
            print(f""after init cache memory allocated: {torch.cuda.memory_allocated() / 1e9}GB, reserved: {torch.cuda.memory_reserved() / 1e9}GB"")","['def', 'initialize_cache', '(', 'self', ',', 'num_gpu_blocks', ':', 'int', ',', 'num_cpu_blocks', ':', 'int', ')', '-', '>', 'None', ':', '# NOTE: We log here to avoid multiple logs when number of workers is', '# greater than one. We could log in the engine, but not all executors', '# have GPUs.', 'logger', '.', 'info', '(', '""', '# GPU blocks: ', '%d', ', # CPU blocks: ', '%d', '""', ',', 'num_gpu_blocks', ',', 'num_cpu_blocks', ')', 'self', '.', 'cache_config', '.', 'num_gpu_blocks', '=', 'num_gpu_blocks', 'self', '.', 'cache_config', '.', 'num_cpu_blocks', '=', 'num_cpu_blocks', 'if', 'torch', '.', 'distributed', '.', 'get_rank', '(', ')', '==', '0', ':', 'print', '(', 'f', '""', 'before init cache memory allocated: ', '{', 'torch', '.', 'cuda', '.', 'memory_allocated', '(', ')', '/', '1e9', '}', 'GB, reserved: ', '{', 'torch', '.', 'cuda', '.', 'memory_reserved', '(', ')', '/', '1e9', '}', 'GB', '""', ')', 'self', '.', 'worker', '.', 'initialize_cache', '(', 'num_gpu_blocks', '=', 'num_gpu_blocks', ',', 'num_cpu_blocks', '=', 'num_cpu_blocks', ')', 'if', 'torch', '.', 'distributed', '.', 'get_rank', '(', ')', '==', '0', ':', 'print', '(', 'f', '""', 'after init cache memory allocated: ', '{', 'torch', '.', 'cuda', '.', 'memory_allocated', '(', ')', '/', '1e9', '}', 'GB, reserved: ', '{', 'torch', '.', 'cuda', '.', 'memory_reserved', '(', ')', '/', '1e9', '}', 'GB', '""', ')']","Initialize cache with specified GPU and CPU blocks, logging memory usage.","['Initialize', 'cache', 'with', 'specified', 'GPU', 'and', 'CPU', 'blocks', ',', 'logging', 'memory', 'usage', '.']"
35,"def _build_service_characteristics(self) -> Dict[str, Any]:
        
        characteristics: Dict[str, Any] = {
            'write_throughput_limitation': 'Amazon Keyspaces has specific throughput characteristics that differ from self-managed Cassandra',
            'implementation_notes': 'The service architecture imposes a 1MB item size limit and throughput constraints different from open-source Cassandra',
        }

        response_guidance = {
            'do_not_mention': ['DynamoDB', 'underlying implementation', 'AWS storage layer'],
            'preferred_terminology': [
                'Keyspaces architecture',
                'managed service design',
                'AWS distributed systems',
            ],
        }

        characteristics['response_guidance'] = response_guidance

        return characteristics","['def', '_build_service_characteristics', '(', 'self', ')', '-', '>', 'Dict', '[', 'str', ',', 'Any', ']', ':', 'characteristics', ':', 'Dict', '[', 'str', ',', 'Any', ']', '=', '{', ""'"", 'write_throughput_limitation', ""'"", ':', ""'"", 'Amazon Keyspaces has specific throughput characteristics that differ from self-managed Cassandra', ""'"", ',', ""'"", 'implementation_notes', ""'"", ':', ""'"", 'The service architecture imposes a 1MB item size limit and throughput constraints different from open-source Cassandra', ""'"", ',', '}', 'response_guidance', '=', '{', ""'"", 'do_not_mention', ""'"", ':', '[', ""'"", 'DynamoDB', ""'"", ',', ""'"", 'underlying implementation', ""'"", ',', ""'"", 'AWS storage layer', ""'"", ']', ',', ""'"", 'preferred_terminology', ""'"", ':', '[', ""'"", 'Keyspaces architecture', ""'"", ',', ""'"", 'managed service design', ""'"", ',', ""'"", 'AWS distributed systems', ""'"", ',', ']', ',', '}', 'characteristics', '[', ""'"", 'response_guidance', ""'"", ']', '=', 'response_guidance', 'return', 'characteristics']",Constructs a dictionary detailing Amazon Keyspaces service characteristics and response guidelines.,"['Constructs', 'a', 'dictionary', 'detailing', 'Amazon', 'Keyspaces', 'service', 'characteristics', 'and', 'response', 'guidelines', '.']"
37,"def _get_camoufox_options(self):
        
        return {
            ""geoip"": self.geoip,
            ""proxy"": self.proxy,
            ""enable_cache"": True,
            ""addons"": self.addons,
            ""exclude_addons"": [] if self.disable_ads else [DefaultAddons.UBO],
            ""headless"": self.headless,
            ""humanize"": self.humanize,
            ""i_know_what_im_doing"": True,  # To turn warnings off with the user configurations
            ""allow_webgl"": self.allow_webgl,
            ""block_webrtc"": self.block_webrtc,
            ""block_images"": self.block_images,  # Careful! it makes some websites doesn't finish loading at all like stackoverflow even in headful
            ""os"": None if self.os_randomize else get_os_name(),
            **self.additional_arguments
        }","['def', '_get_camoufox_options', '(', 'self', ')', ':', 'return', '{', '""', 'geoip', '""', ':', 'self', '.', 'geoip', ',', '""', 'proxy', '""', ':', 'self', '.', 'proxy', ',', '""', 'enable_cache', '""', ':', 'True', ',', '""', 'addons', '""', ':', 'self', '.', 'addons', ',', '""', 'exclude_addons', '""', ':', '[', ']', 'if', 'self', '.', 'disable_ads', 'else', '[', 'DefaultAddons', '.', 'UBO', ']', ',', '""', 'headless', '""', ':', 'self', '.', 'headless', ',', '""', 'humanize', '""', ':', 'self', '.', 'humanize', ',', '""', 'i_know_what_im_doing', '""', ':', 'True', ',', '# To turn warnings off with the user configurations', '""', 'allow_webgl', '""', ':', 'self', '.', 'allow_webgl', ',', '""', 'block_webrtc', '""', ':', 'self', '.', 'block_webrtc', ',', '""', 'block_images', '""', ':', 'self', '.', 'block_images', ',', ""# Careful! it makes some websites doesn't finish loading at all like stackoverflow even in headful"", '""', 'os', '""', ':', 'None', 'if', 'self', '.', 'os_randomize', 'else', 'get_os_name', '(', ')', ',', '*', '*', 'self', '.', 'additional_arguments', '}']",Configure browser options for privacy and customization settings.,"['Configure', 'browser', 'options', 'for', 'privacy', 'and', 'customization', 'settings', '.']"
38,"def validate_dimensions(dimension: Union[int, float], attribute_name: str) -> int:
    
    dimension = int(dimension)
    if dimension <= 0 or dimension % 64 != 0:
        raise ValueError(f""The '{attribute_name}' must be a multiple of 64."")
    return dimension","['def', 'validate_dimensions', '(', 'dimension', ':', 'Union', '[', 'int', ',', 'float', ']', ',', 'attribute_name', ':', 'str', ')', '-', '>', 'int', ':', 'dimension', '=', 'int', '(', 'dimension', ')', 'if', 'dimension', '<', '=', '0', 'or', 'dimension', '%', '64', '!=', '0', ':', 'raise', 'ValueError', '(', 'f', '""', 'The ', ""'"", '{', 'attribute_name', '}', ""'"", ' must be a multiple of 64.', '""', ')', 'return', 'dimension']","Ensure dimension is a positive multiple of 64, else raise error.","['Ensure', 'dimension', 'is', 'a', 'positive', 'multiple', 'of', '64', ',', 'else', 'raise', 'error', '.']"
40,"def _format_comment_body(self, comment: GitHubPRComment) -> str:
        
        body = comment.comment_body

        # Add code snippet reference if provided
        if comment.code_snippet:
            body += f""\n\n```\n{comment.code_snippet}\n```""

        # Add suggestion if provided
        if comment.suggestion:
            body += f""\n\n```suggestion\n{comment.suggestion}\n```""

        return body","['def', '_format_comment_body', '(', 'self', ',', 'comment', ':', 'GitHubPRComment', ')', '-', '>', 'str', ':', 'body', '=', 'comment', '.', 'comment_body', '# Add code snippet reference if provided', 'if', 'comment', '.', 'code_snippet', ':', 'body', '+', '=', 'f', '""', '\\n', '\\n', '```', '\\n', '{', 'comment', '.', 'code_snippet', '}', '\\n', '```', '""', '# Add suggestion if provided', 'if', 'comment', '.', 'suggestion', ':', 'body', '+', '=', 'f', '""', '\\n', '\\n', '```suggestion', '\\n', '{', 'comment', '.', 'suggestion', '}', '\\n', '```', '""', 'return', 'body']",Formats GitHub PR comment with optional code and suggestion snippets.,"['Formats', 'GitHub', 'PR', 'comment', 'with', 'optional', 'code', 'and', 'suggestion', 'snippets', '.']"
41,"def _save_credentials(self) -> None:
        
        if not self.credentials:
            logger.error(f""Attempted to save null credentials for user {self.user_morphik_id}."")
            return

        creds_path = self._get_user_credentials_path()
        try:
            with open(creds_path, ""w"") as creds_file:
                json.dump(self.credentials, creds_file)
            logger.info(f""Successfully saved Zotero credentials for user {self.user_morphik_id}"")
        except Exception as e:
            logger.error(f""Failed to save Zotero credentials for user {self.user_morphik_id}: {e}"")","['def', '_save_credentials', '(', 'self', ')', '-', '>', 'None', ':', 'if', 'not', 'self', '.', 'credentials', ':', 'logger', '.', 'error', '(', 'f', '""', 'Attempted to save null credentials for user ', '{', 'self', '.', 'user_morphik_id', '}', '.', '""', ')', 'return', 'creds_path', '=', 'self', '.', '_get_user_credentials_path', '(', ')', 'try', ':', 'with', 'open', '(', 'creds_path', ',', '""', 'w', '""', ')', 'as', 'creds_file', ':', 'json', '.', 'dump', '(', 'self', '.', 'credentials', ',', 'creds_file', ')', 'logger', '.', 'info', '(', 'f', '""', 'Successfully saved Zotero credentials for user ', '{', 'self', '.', 'user_morphik_id', '}', '""', ')', 'except', 'Exception', 'as', 'e', ':', 'logger', '.', 'error', '(', 'f', '""', 'Failed to save Zotero credentials for user ', '{', 'self', '.', 'user_morphik_id', '}', ': ', '{', 'e', '}', '""', ')']","Safely store user credentials to a file, logging success or failure.","['Safely', 'store', 'user', 'credentials', 'to', 'a', 'file', ',', 'logging', 'success', 'or', 'failure', '.']"
42,"def extract_json_from_response(content: str) -> dict | None:
    
    try:
        json_start = content.find(""```json"")
        if json_start != -1:
            json_text = content[json_start + 7 :]  # Skip past ```json
            json_end = json_text.find(""```"")
            if json_end != -1:
                json_text = json_text[:json_end].strip()
                return json.loads(json_text)
    except Exception as e:
        print(f""Error extracting JSON from response: {e}"")
    return None","['def', 'extract_json_from_response', '(', 'content', ':', 'str', ')', '-', '>', 'dict', '|', 'None', ':', 'try', ':', 'json_start', '=', 'content', '.', 'find', '(', '""', '```json', '""', ')', 'if', 'json_start', '!=', '-', '1', ':', 'json_text', '=', 'content', '[', 'json_start', '+', '7', ':', ']', '# Skip past ```json', 'json_end', '=', 'json_text', '.', 'find', '(', '""', '```', '""', ')', 'if', 'json_end', '!=', '-', '1', ':', 'json_text', '=', 'json_text', '[', ':', 'json_end', ']', '.', 'strip', '(', ')', 'return', 'json', '.', 'loads', '(', 'json_text', ')', 'except', 'Exception', 'as', 'e', ':', 'print', '(', 'f', '""', 'Error extracting JSON from response: ', '{', 'e', '}', '""', ')', 'return', 'None']",Extract JSON data from a string response containing markdown code blocks.,"['Extract', 'JSON', 'data', 'from', 'a', 'string', 'response', 'containing', 'markdown', 'code', 'blocks', '.']"
43,"def extract_target_uri_s3(bucket, key, s3_client, head_bytes=1048576):
    
    target_uri = None
    try:
        response = s3_client.get_object(Bucket=bucket, Key=key, Range=f""bytes=0-{head_bytes-1}"")
        stream = response[""Body""]
        for record in ArchiveIterator(stream):
            for name, value in record.rec_headers.headers:
                if name == ""WARC-Target-URI"":
                    target_uri = value
                    break
            if target_uri:
                break  # Only use the first valid response record
    except Exception as e:
    return target_uri","['def', 'extract_target_uri_s3', '(', 'bucket', ',', 'key', ',', 's3_client', ',', 'head_bytes', '=', '1048576', ')', ':', 'target_uri', '=', 'None', 'try', ':', 'response', '=', 's3_client', '.', 'get_object', '(', 'Bucket', '=', 'bucket', ',', 'Key', '=', 'key', ',', 'Range', '=', 'f', '""', 'bytes=0-', '{', 'head_bytes', '-', '1', '}', '""', ')', 'stream', '=', 'response', '[', '""', 'Body', '""', ']', 'for', 'record', 'in', 'ArchiveIterator', '(', 'stream', ')', ':', 'for', 'name', ',', 'value', 'in', 'record', '.', 'rec_headers', '.', 'headers', ':', 'if', 'name', '==', '""', 'WARC-Target-URI', '""', ':', 'target_uri', '=', 'value', 'break', 'if', 'target_uri', ':', 'break', '# Only use the first valid response record', 'except', 'Exception', 'as', 'e', ':', 'return', 'target_uri']",Extracts the target URI from an S3 object using WARC headers.,"['Extracts', 'the', 'target', 'URI', 'from', 'an', 'S3', 'object', 'using', 'WARC', 'headers', '.']"
46,"def define_log_level(print_level=""INFO"", logfile_level=""DEBUG"", name: str = None):
    
    global _print_level
    _print_level = print_level

    current_date = datetime.now()
    formatted_date = current_date.strftime(""%Y%m%d%H%M%S"")
    log_name = (
        f""{name}_{formatted_date}"" if name else formatted_date
    )  # name a log with prefix name

    _logger.remove()
    _logger.add(sys.stderr, level=print_level)
    _logger.add(PROJECT_ROOT / f""logs/{log_name}.log"", level=logfile_level)
    return _logger","['def', 'define_log_level', '(', 'print_level', '=', '""', 'INFO', '""', ',', 'logfile_level', '=', '""', 'DEBUG', '""', ',', 'name', ':', 'str', '=', 'None', ')', ':', 'global', '_print_level', '_print_level', '=', 'print_level', 'current_date', '=', 'datetime', '.', 'now', '(', ')', 'formatted_date', '=', 'current_date', '.', 'strftime', '(', '""', '%', 'Y', '%', 'm', '%d', '%', 'H', '%', 'M', '%', 'S', '""', ')', 'log_name', '=', '(', 'f', '""', '{', 'name', '}', '_', '{', 'formatted_date', '}', '""', 'if', 'name', 'else', 'formatted_date', ')', '# name a log with prefix name', '_logger', '.', 'remove', '(', ')', '_logger', '.', 'add', '(', 'sys', '.', 'stderr', ',', 'level', '=', 'print_level', ')', '_logger', '.', 'add', '(', 'PROJECT_ROOT', '/', 'f', '""', 'logs/', '{', 'log_name', '}', '.log', '""', ',', 'level', '=', 'logfile_level', ')', 'return', '_logger']",Configure logging levels and file naming for application logs.,"['Configure', 'logging', 'levels', 'and', 'file', 'naming', 'for', 'application', 'logs', '.']"
47,"def normalize_pandas(obj):
    
    if isinstance(obj, pd.Series):
        return obj.tolist()
    elif isinstance(obj, pd.DataFrame):
        return obj.to_dict(""records"")
    elif isinstance(obj, dict):
        return {k: normalize_pandas(v) for k, v in obj.items()}
    elif isinstance(obj, (list, tuple)):
        return [normalize_pandas(item) for item in obj]
    return obj","['def', 'normalize_pandas', '(', 'obj', ')', ':', 'if', 'isinstance', '(', 'obj', ',', 'pd', '.', 'Series', ')', ':', 'return', 'obj', '.', 'tolist', '(', ')', 'elif', 'isinstance', '(', 'obj', ',', 'pd', '.', 'DataFrame', ')', ':', 'return', 'obj', '.', 'to_dict', '(', '""', 'records', '""', ')', 'elif', 'isinstance', '(', 'obj', ',', 'dict', ')', ':', 'return', '{', 'k', ':', 'normalize_pandas', '(', 'v', ')', 'for', 'k', ',', 'v', 'in', 'obj', '.', 'items', '(', ')', '}', 'elif', 'isinstance', '(', 'obj', ',', '(', 'list', ',', 'tuple', ')', ')', ':', 'return', '[', 'normalize_pandas', '(', 'item', ')', 'for', 'item', 'in', 'obj', ']', 'return', 'obj']",Convert pandas objects to native Python data structures,"['Convert', 'pandas', 'objects', 'to', 'native', 'Python', 'data', 'structures']"
49,"def get_client_and_project():
        
        is_dev_mode = os.getenv(""isDevelopmentMode"", ""enabled"") == ""enabled""
        if is_dev_mode:
            return None, None

        project_id = os.environ.get(""GCP_PROJECT"")
        if not project_id:
            raise HTTPException(
                status_code=500, detail=""GCP_PROJECT environment variable is not set""
            )

        try:
            client = secretmanager.SecretManagerServiceClient()
            return client, project_id
        except Exception as e:
            raise HTTPException(
                status_code=500,
                detail=f""Failed to initialize Secret Manager client: {str(e)}"",
            )","['def', 'get_client_and_project', '(', ')', ':', 'is_dev_mode', '=', 'os', '.', 'getenv', '(', '""', 'isDevelopmentMode', '""', ',', '""', 'enabled', '""', ')', '==', '""', 'enabled', '""', 'if', 'is_dev_mode', ':', 'return', 'None', ',', 'None', 'project_id', '=', 'os', '.', 'environ', '.', 'get', '(', '""', 'GCP_PROJECT', '""', ')', 'if', 'not', 'project_id', ':', 'raise', 'HTTPException', '(', 'status_code', '=', '500', ',', 'detail', '=', '""', 'GCP_PROJECT environment variable is not set', '""', ')', 'try', ':', 'client', '=', 'secretmanager', '.', 'SecretManagerServiceClient', '(', ')', 'return', 'client', ',', 'project_id', 'except', 'Exception', 'as', 'e', ':', 'raise', 'HTTPException', '(', 'status_code', '=', '500', ',', 'detail', '=', 'f', '""', 'Failed to initialize Secret Manager client: ', '{', 'str', '(', 'e', ')', '}', '""', ',', ')']","Determine client and project ID, handling development mode and errors","['Determine', 'client', 'and', 'project', 'ID', ',', 'handling', 'development', 'mode', 'and', 'errors']"
50,"def get_dataset_info(cls) -> Dict[str, str]:
        
        return {
            ""id"": ""custom"",  # Unique identifier for the dataset
            ""name"": ""Custom Dataset"",  # Human-readable name
            ""description"": ""Template for a custom benchmark dataset"",  # Description
            ""url"": cls.get_default_dataset_path(),  # Default URL or path
        }","['def', 'get_dataset_info', '(', 'cls', ')', '-', '>', 'Dict', '[', 'str', ',', 'str', ']', ':', 'return', '{', '""', 'id', '""', ':', '""', 'custom', '""', ',', '# Unique identifier for the dataset', '""', 'name', '""', ':', '""', 'Custom Dataset', '""', ',', '# Human-readable name', '""', 'description', '""', ':', '""', 'Template for a custom benchmark dataset', '""', ',', '# Description', '""', 'url', '""', ':', 'cls', '.', 'get_default_dataset_path', '(', ')', ',', '# Default URL or path', '}']","Provide metadata for a custom benchmark dataset including ID, name, description, and URL.","['Provide', 'metadata', 'for', 'a', 'custom', 'benchmark', 'dataset', 'including', 'ID', ',', 'name', ',', 'description', ',', 'and', 'URL', '.']"
52,"def is_fastTrack_eligible(self):
        
        if (self.handler.context_url != ''):
            logger.debug(""Fast track not eligible: context_url present"")
            return False
        if (len(self.handler.prev_queries) > 0):
            logger.debug(f""Fast track not eligible: {len(self.handler.prev_queries)} previous queries present"")
            return False
        logger.info(""Query is eligible for fast track"")
        return True","['def', 'is_fastTrack_eligible', '(', 'self', ')', ':', 'if', '(', 'self', '.', 'handler', '.', 'context_url', '!=', ""'"", ""'"", ')', ':', 'logger', '.', 'debug', '(', '""', 'Fast track not eligible: context_url present', '""', ')', 'return', 'False', 'if', '(', 'len', '(', 'self', '.', 'handler', '.', 'prev_queries', ')', '>', '0', ')', ':', 'logger', '.', 'debug', '(', 'f', '""', 'Fast track not eligible: ', '{', 'len', '(', 'self', '.', 'handler', '.', 'prev_queries', ')', '}', ' previous queries present', '""', ')', 'return', 'False', 'logger', '.', 'info', '(', '""', 'Query is eligible for fast track', '""', ')', 'return', 'True']",Determine fast track eligibility based on context URL and previous queries.,"['Determine', 'fast', 'track', 'eligibility', 'based', 'on', 'context', 'URL', 'and', 'previous', 'queries', '.']"
56,"def load_server_config(cls) -> Dict[str, MCPServerConfig]:
        
        config_path = PROJECT_ROOT / ""config"" / ""mcp.json""

        try:
            config_file = config_path if config_path.exists() else None
            if not config_file:
                return {}

            with config_file.open() as f:
                data = json.load(f)
                servers = {}

                for server_id, server_config in data.get(""mcpServers"", {}).items():
                    servers[server_id] = MCPServerConfig(
                        type=server_config[""type""],
                        url=server_config.get(""url""),
                        command=server_config.get(""command""),
                        args=server_config.get(""args"", []),
                    )
                return servers
        except Exception as e:
            raise ValueError(f""Failed to load MCP server config: {e}"")","['def', 'load_server_config', '(', 'cls', ')', '-', '>', 'Dict', '[', 'str', ',', 'MCPServerConfig', ']', ':', 'config_path', '=', 'PROJECT_ROOT', '/', '""', 'config', '""', '/', '""', 'mcp.json', '""', 'try', ':', 'config_file', '=', 'config_path', 'if', 'config_path', '.', 'exists', '(', ')', 'else', 'None', 'if', 'not', 'config_file', ':', 'return', '{', '}', 'with', 'config_file', '.', 'open', '(', ')', 'as', 'f', ':', 'data', '=', 'json', '.', 'load', '(', 'f', ')', 'servers', '=', '{', '}', 'for', 'server_id', ',', 'server_config', 'in', 'data', '.', 'get', '(', '""', 'mcpServers', '""', ',', '{', '}', ')', '.', 'items', '(', ')', ':', 'servers', '[', 'server_id', ']', '=', 'MCPServerConfig', '(', 'type', '=', 'server_config', '[', '""', 'type', '""', ']', ',', 'url', '=', 'server_config', '.', 'get', '(', '""', 'url', '""', ')', ',', 'command', '=', 'server_config', '.', 'get', '(', '""', 'command', '""', ')', ',', 'args', '=', 'server_config', '.', 'get', '(', '""', 'args', '""', ',', '[', ']', ')', ',', ')', 'return', 'servers', 'except', 'Exception', 'as', 'e', ':', 'raise', 'ValueError', '(', 'f', '""', 'Failed to load MCP server config: ', '{', 'e', '}', '""', ')']",Load and parse MCP server configurations from a JSON file into a dictionary.,"['Load', 'and', 'parse', 'MCP', 'server', 'configurations', 'from', 'a', 'JSON', 'file', 'into', 'a', 'dictionary', '.']"
57,"def api_get_types():
    
    try:
        # Get all setting types
        types = [t.value for t in SettingType]
        return jsonify({""types"": types})
    except Exception as e:
        logger.exception(""Error getting types"")
        return jsonify({""error"": str(e)}), 500","['def', 'api_get_types', '(', ')', ':', 'try', ':', '# Get all setting types', 'types', '=', '[', 't', '.', 'value', 'for', 't', 'in', 'SettingType', ']', 'return', 'jsonify', '(', '{', '""', 'types', '""', ':', 'types', '}', ')', 'except', 'Exception', 'as', 'e', ':', 'logger', '.', 'exception', '(', '""', 'Error getting types', '""', ')', 'return', 'jsonify', '(', '{', '""', 'error', '""', ':', 'str', '(', 'e', ')', '}', ')', ',', '500']","Retrieve and return setting types as JSON, handling errors gracefully.","['Retrieve', 'and', 'return', 'setting', 'types', 'as', 'JSON', ',', 'handling', 'errors', 'gracefully', '.']"
58,"def set_default_attn_processor(self):
        
        if all(
            proc.__class__ in ADDED_KV_ATTENTION_PROCESSORS
            for proc in self.attn_processors.values()
        ):
            processor = AttnAddedKVProcessor()
        elif all(
            proc.__class__ in CROSS_ATTENTION_PROCESSORS
            for proc in self.attn_processors.values()
        ):
            processor = AttnProcessor()
        else:
            raise ValueError(
                f""Cannot call `set_default_attn_processor` when attention processors are of type {next(iter(self.attn_processors.values()))}""
            )

        self.set_attn_processor(processor, _remove_lora=True)","['def', 'set_default_attn_processor', '(', 'self', ')', ':', 'if', 'all', '(', 'proc', '.', '__class__', 'in', 'ADDED_KV_ATTENTION_PROCESSORS', 'for', 'proc', 'in', 'self', '.', 'attn_processors', '.', 'values', '(', ')', ')', ':', 'processor', '=', 'AttnAddedKVProcessor', '(', ')', 'elif', 'all', '(', 'proc', '.', '__class__', 'in', 'CROSS_ATTENTION_PROCESSORS', 'for', 'proc', 'in', 'self', '.', 'attn_processors', '.', 'values', '(', ')', ')', ':', 'processor', '=', 'AttnProcessor', '(', ')', 'else', ':', 'raise', 'ValueError', '(', 'f', '""', 'Cannot call `set_default_attn_processor` when attention processors are of type ', '{', 'next', '(', 'iter', '(', 'self', '.', 'attn_processors', '.', 'values', '(', ')', ')', ')', '}', '""', ')', 'self', '.', 'set_attn_processor', '(', 'processor', ',', '_remove_lora', '=', 'True', ')']",Configure default attention processor based on current processor types.,"['Configure', 'default', 'attention', 'processor', 'based', 'on', 'current', 'processor', 'types', '.']"
59,"def state(self, value: JobStateEnum) -> None:
        
        if self._state in _TERMINAL_STATES:
            logger.error(f""Attempt to change state from {self._state.name} to {value.name} denied."")
            raise ValueError(f""Cannot change state from {self._state.name} to {value.name}."")
        if value.value < self._state.value:
            logger.error(f""Invalid state transition attempt from {self._state.name} to {value.name}."")
            raise ValueError(f""State can only transition forward, from {self._state.name} to {value.name} not allowed."")
        self._state = value","['def', 'state', '(', 'self', ',', 'value', ':', 'JobStateEnum', ')', '-', '>', 'None', ':', 'if', 'self', '.', '_state', 'in', '_TERMINAL_STATES', ':', 'logger', '.', 'error', '(', 'f', '""', 'Attempt to change state from ', '{', 'self', '.', '_state', '.', 'name', '}', ' to ', '{', 'value', '.', 'name', '}', ' denied.', '""', ')', 'raise', 'ValueError', '(', 'f', '""', 'Cannot change state from ', '{', 'self', '.', '_state', '.', 'name', '}', ' to ', '{', 'value', '.', 'name', '}', '.', '""', ')', 'if', 'value', '.', 'value', '<', 'self', '.', '_state', '.', 'value', ':', 'logger', '.', 'error', '(', 'f', '""', 'Invalid state transition attempt from ', '{', 'self', '.', '_state', '.', 'name', '}', ' to ', '{', 'value', '.', 'name', '}', '.', '""', ')', 'raise', 'ValueError', '(', 'f', '""', 'State can only transition forward, from ', '{', 'self', '.', '_state', '.', 'name', '}', ' to ', '{', 'value', '.', 'name', '}', ' not allowed.', '""', ')', 'self', '.', '_state', '=', 'value']","Enforces valid state transitions for a job, logging errors on invalid attempts.","['Enforces', 'valid', 'state', 'transitions', 'for', 'a', 'job', ',', 'logging', 'errors', 'on', 'invalid', 'attempts', '.']"
60,"def _get_hash(identifier: str) -> str:
        
        identifier = identifier.lower().strip()
        if isinstance(identifier, str):
            # Hash functions have to take bytes
            identifier = identifier.encode('utf-8')

        hash_value = sha256(identifier).hexdigest()
        return f""{hash_value}_{len(identifier)}""","['def', '_get_hash', '(', 'identifier', ':', 'str', ')', '-', '>', 'str', ':', 'identifier', '=', 'identifier', '.', 'lower', '(', ')', '.', 'strip', '(', ')', 'if', 'isinstance', '(', 'identifier', ',', 'str', ')', ':', '# Hash functions have to take bytes', 'identifier', '=', 'identifier', '.', 'encode', '(', ""'"", 'utf-8', ""'"", ')', 'hash_value', '=', 'sha256', '(', 'identifier', ')', '.', 'hexdigest', '(', ')', 'return', 'f', '""', '{', 'hash_value', '}', '_', '{', 'len', '(', 'identifier', ')', '}', '""']",Generate a unique hash for a given string identifier.,"['Generate', 'a', 'unique', 'hash', 'for', 'a', 'given', 'string', 'identifier', '.']"
62,"def is_valid_tool_call_item(call_item: dict) -> bool:
    
    if ""name"" not in call_item or not isinstance(call_item[""name""], str):
        return False

    if set(call_item.keys()) - {""name"", ""arguments""}:  # noqa: SIM103
        return False

    return True","['def', 'is_valid_tool_call_item', '(', 'call_item', ':', 'dict', ')', '-', '>', 'bool', ':', 'if', '""', 'name', '""', 'not', 'in', 'call_item', 'or', 'not', 'isinstance', '(', 'call_item', '[', '""', 'name', '""', ']', ',', 'str', ')', ':', 'return', 'False', 'if', 'set', '(', 'call_item', '.', 'keys', '(', ')', ')', '-', '{', '""', 'name', '""', ',', '""', 'arguments', '""', '}', ':', '# noqa: SIM103', 'return', 'False', 'return', 'True']",Check if a dictionary represents a valid tool call item.,"['Check', 'if', 'a', 'dictionary', 'represents', 'a', 'valid', 'tool', 'call', 'item', '.']"
64,"def _handle_content(self, content):
        
        if content.get(""type"") == ""text"":
            text = content.get(""text"", """")
            if text == ""<DONE>"":
                return
            logger.info(f""Assistant: {text}"")","['def', '_handle_content', '(', 'self', ',', 'content', ')', ':', 'if', 'content', '.', 'get', '(', '""', 'type', '""', ')', '==', '""', 'text', '""', ':', 'text', '=', 'content', '.', 'get', '(', '""', 'text', '""', ',', '""', '""', ')', 'if', 'text', '==', '""', '<DONE>', '""', ':', 'return', 'logger', '.', 'info', '(', 'f', '""', 'Assistant: ', '{', 'text', '}', '""', ')']",Process content to log text messages unless marked as done,"['Process', 'content', 'to', 'log', 'text', 'messages', 'unless', 'marked', 'as', 'done']"
65,"def add_prompt(self, prompt: Prompt) -> Prompt:
        
        logger.debug(f""Adding prompt: {prompt.name}"")
        existing = self._prompts.get(prompt.name)
        if existing:
            if self.warn_on_duplicate_prompts:
                logger.warning(f""Prompt already exists: {prompt.name}"")
            return existing
        self._prompts[prompt.name] = prompt
        return prompt","['def', 'add_prompt', '(', 'self', ',', 'prompt', ':', 'Prompt', ')', '-', '>', 'Prompt', ':', 'logger', '.', 'debug', '(', 'f', '""', 'Adding prompt: ', '{', 'prompt', '.', 'name', '}', '""', ')', 'existing', '=', 'self', '.', '_prompts', '.', 'get', '(', 'prompt', '.', 'name', ')', 'if', 'existing', ':', 'if', 'self', '.', 'warn_on_duplicate_prompts', ':', 'logger', '.', 'warning', '(', 'f', '""', 'Prompt already exists: ', '{', 'prompt', '.', 'name', '}', '""', ')', 'return', 'existing', 'self', '.', '_prompts', '[', 'prompt', '.', 'name', ']', '=', 'prompt', 'return', 'prompt']",Add or log existing prompt in collection with optional warning,"['Add', 'or', 'log', 'existing', 'prompt', 'in', 'collection', 'with', 'optional', 'warning']"
66,"def _create_visualizations(self):
        
        if not PLOTTING_AVAILABLE:
            logger.warning(""Matplotlib not available, skipping visualization creation"")
            return

        if not self.study or len(self.study.trials) < 2:
            logger.warning(""Not enough trials to create visualizations"")
            return

        # Create directory for visualizations
        viz_dir = os.path.join(self.output_dir, ""visualizations"")
        os.makedirs(viz_dir, exist_ok=True)

        # Create Optuna visualizations
        self._create_optuna_visualizations(viz_dir)

        # Create custom visualizations
        self._create_custom_visualizations(viz_dir)

        logger.info(f""Visualizations saved to {viz_dir}"")","['def', '_create_visualizations', '(', 'self', ')', ':', 'if', 'not', 'PLOTTING_AVAILABLE', ':', 'logger', '.', 'warning', '(', '""', 'Matplotlib not available, skipping visualization creation', '""', ')', 'return', 'if', 'not', 'self', '.', 'study', 'or', 'len', '(', 'self', '.', 'study', '.', 'trials', ')', '<', '2', ':', 'logger', '.', 'warning', '(', '""', 'Not enough trials to create visualizations', '""', ')', 'return', '# Create directory for visualizations', 'viz_dir', '=', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'output_dir', ',', '""', 'visualizations', '""', ')', 'os', '.', 'makedirs', '(', 'viz_dir', ',', 'exist_ok', '=', 'True', ')', '# Create Optuna visualizations', 'self', '.', '_create_optuna_visualizations', '(', 'viz_dir', ')', '# Create custom visualizations', 'self', '.', '_create_custom_visualizations', '(', 'viz_dir', ')', 'logger', '.', 'info', '(', 'f', '""', 'Visualizations saved to ', '{', 'viz_dir', '}', '""', ')']",Generate and save visualizations if conditions are met and dependencies are available.,"['Generate', 'and', 'save', 'visualizations', 'if', 'conditions', 'are', 'met', 'and', 'dependencies', 'are', 'available', '.']"
68,"def delete_all_output_files() -> bool:
    
    try:
        for filename in os.listdir(OUTPUTS_DIR):
            if any(filename.endswith(ext) for ext in AUDIO_FORMATS):
                file_path = os.path.join(OUTPUTS_DIR, filename)
                os.remove(file_path)
        return True
    except Exception as e:
        print(f""Error deleting output files: {e}"")
        return False","['def', 'delete_all_output_files', '(', ')', '-', '>', 'bool', ':', 'try', ':', 'for', 'filename', 'in', 'os', '.', 'listdir', '(', 'OUTPUTS_DIR', ')', ':', 'if', 'any', '(', 'filename', '.', 'endswith', '(', 'ext', ')', 'for', 'ext', 'in', 'AUDIO_FORMATS', ')', ':', 'file_path', '=', 'os', '.', 'path', '.', 'join', '(', 'OUTPUTS_DIR', ',', 'filename', ')', 'os', '.', 'remove', '(', 'file_path', ')', 'return', 'True', 'except', 'Exception', 'as', 'e', ':', 'print', '(', 'f', '""', 'Error deleting output files: ', '{', 'e', '}', '""', ')', 'return', 'False']","Remove all audio files from the specified output directory, handling errors.","['Remove', 'all', 'audio', 'files', 'from', 'the', 'specified', 'output', 'directory', ',', 'handling', 'errors', '.']"
73,"def _respect_rate_limit(self):
        
        current_time = time.time()
        time_since_last_request = current_time - self.last_request_time

        if time_since_last_request < self.delay_between_requests:
            wait_time = self.delay_between_requests - time_since_last_request
            logger.info(f""Rate limiting: waiting {wait_time:.2f} seconds"")
            time.sleep(wait_time)

        self.last_request_time = time.time()","['def', '_respect_rate_limit', '(', 'self', ')', ':', 'current_time', '=', 'time', '.', 'time', '(', ')', 'time_since_last_request', '=', 'current_time', '-', 'self', '.', 'last_request_time', 'if', 'time_since_last_request', '<', 'self', '.', 'delay_between_requests', ':', 'wait_time', '=', 'self', '.', 'delay_between_requests', '-', 'time_since_last_request', 'logger', '.', 'info', '(', 'f', '""', 'Rate limiting: waiting ', '{', 'wait_time', ':', '.2f', '}', ' seconds', '""', ')', 'time', '.', 'sleep', '(', 'wait_time', ')', 'self', '.', 'last_request_time', '=', 'time', '.', 'time', '(', ')']",Enforces API rate limits by delaying requests if needed.,"['Enforces', 'API', 'rate', 'limits', 'by', 'delaying', 'requests', 'if', 'needed', '.']"
74,"def _cleanup_existing_alembic(self) -> None:
        
        # logger.info(""Cleaning up existing Alembic configuration..."")

        # Remove entire alembic directory if it exists
        if self.alembic_dir.exists():
            import shutil

            shutil.rmtree(self.alembic_dir)
            logger.info(f""Removed alembic directory: {self.alembic_dir}"")

        # Remove alembic.ini if it exists
        if self.alembic_ini_path.exists():
            self.alembic_ini_path.unlink()
            logger.info(""Removed alembic.ini"")","['def', '_cleanup_existing_alembic', '(', 'self', ')', '-', '>', 'None', ':', '# logger.info(""Cleaning up existing Alembic configuration..."")', '# Remove entire alembic directory if it exists', 'if', 'self', '.', 'alembic_dir', '.', 'exists', '(', ')', ':', 'import', 'shutil', 'shutil', '.', 'rmtree', '(', 'self', '.', 'alembic_dir', ')', 'logger', '.', 'info', '(', 'f', '""', 'Removed alembic directory: ', '{', 'self', '.', 'alembic_dir', '}', '""', ')', '# Remove alembic.ini if it exists', 'if', 'self', '.', 'alembic_ini_path', '.', 'exists', '(', ')', ':', 'self', '.', 'alembic_ini_path', '.', 'unlink', '(', ')', 'logger', '.', 'info', '(', '""', 'Removed alembic.ini', '""', ')']",Remove existing Alembic configuration by deleting directory and configuration file.,"['Remove', 'existing', 'Alembic', 'configuration', 'by', 'deleting', 'directory', 'and', 'configuration', 'file', '.']"
76,"def _separate_create_config(self, config: dict[str, Any]) -> tuple[dict[str, Any], dict[str, Any]]:
        
        create_config = {k: v for k, v in config.items() if k not in self.extra_kwargs}
        extra_kwargs = {k: v for k, v in config.items() if k in self.extra_kwargs}
        return create_config, extra_kwargs","['def', '_separate_create_config', '(', 'self', ',', 'config', ':', 'dict', '[', 'str', ',', 'Any', ']', ')', '-', '>', 'tuple', '[', 'dict', '[', 'str', ',', 'Any', ']', ',', 'dict', '[', 'str', ',', 'Any', ']', ']', ':', 'create_config', '=', '{', 'k', ':', 'v', 'for', 'k', ',', 'v', 'in', 'config', '.', 'items', '(', ')', 'if', 'k', 'not', 'in', 'self', '.', 'extra_kwargs', '}', 'extra_kwargs', '=', '{', 'k', ':', 'v', 'for', 'k', ',', 'v', 'in', 'config', '.', 'items', '(', ')', 'if', 'k', 'in', 'self', '.', 'extra_kwargs', '}', 'return', 'create_config', ',', 'extra_kwargs']",Split configuration into main settings and additional parameters.,"['Split', 'configuration', 'into', 'main', 'settings', 'and', 'additional', 'parameters', '.']"
77,"def _format_server_info(self, server_name: str) -> str:
        
        server_config = self.server_registry.get_server_config(server_name)
        server_str = f""Server Name: {server_name}""
        if not server_config:
            return server_str

        description = server_config.description
        if description:
            server_str = f""{server_str}\nDescription: {description}""

        return server_str","['def', '_format_server_info', '(', 'self', ',', 'server_name', ':', 'str', ')', '-', '>', 'str', ':', 'server_config', '=', 'self', '.', 'server_registry', '.', 'get_server_config', '(', 'server_name', ')', 'server_str', '=', 'f', '""', 'Server Name: ', '{', 'server_name', '}', '""', 'if', 'not', 'server_config', ':', 'return', 'server_str', 'description', '=', 'server_config', '.', 'description', 'if', 'description', ':', 'server_str', '=', 'f', '""', '{', 'server_str', '}', '\\n', 'Description: ', '{', 'description', '}', '""', 'return', 'server_str']",Format server details into a descriptive string output,"['Format', 'server', 'details', 'into', 'a', 'descriptive', 'string', 'output']"
78,"def format_question(category: str, question: str, answer: str) -> Dict[str, Any]:
    
    # Basic sanity checks
    if not question or not answer:
        raise ValueError(f""Empty question or answer in {category}"")
        
    return {
        ""id"": f""{category}_{random.getrandbits(32):08x}"",
        ""category"": category,
        ""question"": clean_text(question),
        ""answer"": clean_text(answer),
        ""metadata"": {
            ""source"": SOURCES[category][""name""],
            ""type"": category,
            ""difficulty"": ""challenging""  # All examples are chosen to be challenging
        }
    }","['def', 'format_question', '(', 'category', ':', 'str', ',', 'question', ':', 'str', ',', 'answer', ':', 'str', ')', '-', '>', 'Dict', '[', 'str', ',', 'Any', ']', ':', '# Basic sanity checks', 'if', 'not', 'question', 'or', 'not', 'answer', ':', 'raise', 'ValueError', '(', 'f', '""', 'Empty question or answer in ', '{', 'category', '}', '""', ')', 'return', '{', '""', 'id', '""', ':', 'f', '""', '{', 'category', '}', '_', '{', 'random', '.', 'getrandbits', '(', '32', ')', ':', '08x', '}', '""', ',', '""', 'category', '""', ':', 'category', ',', '""', 'question', '""', ':', 'clean_text', '(', 'question', ')', ',', '""', 'answer', '""', ':', 'clean_text', '(', 'answer', ')', ',', '""', 'metadata', '""', ':', '{', '""', 'source', '""', ':', 'SOURCES', '[', 'category', ']', '[', '""', 'name', '""', ']', ',', '""', 'type', '""', ':', 'category', ',', '""', 'difficulty', '""', ':', '""', 'challenging', '""', '# All examples are chosen to be challenging', '}', '}']",Generate a structured question-answer dictionary with metadata validation.,"['Generate', 'a', 'structured', 'question-answer', 'dictionary', 'with', 'metadata', 'validation', '.']"
79,"def get_tool(self, key: str) -> Tool:
        
        if key in self._tools:
            return self._tools[key]
        raise NotFoundError(f""Unknown tool: {key}"")","['def', 'get_tool', '(', 'self', ',', 'key', ':', 'str', ')', '-', '>', 'Tool', ':', 'if', 'key', 'in', 'self', '.', '_tools', ':', 'return', 'self', '.', '_tools', '[', 'key', ']', 'raise', 'NotFoundError', '(', 'f', '""', 'Unknown tool: ', '{', 'key', '}', '""', ')']",Retrieve a tool by key or raise an error if not found.,"['Retrieve', 'a', 'tool', 'by', 'key', 'or', 'raise', 'an', 'error', 'if', 'not', 'found', '.']"
80,"def stable_serialize(obj: Any) -> Any:
    
    if isinstance(obj, dict):
        return {k: stable_serialize(v) for k, v in sorted(obj.items())}
    elif isinstance(obj, (list, tuple)):
        return [stable_serialize(x) for x in obj]
    elif isinstance(obj, (str, int, float, bool, type(None))):
        return obj
    else:
        return str(obj)","['def', 'stable_serialize', '(', 'obj', ':', 'Any', ')', '-', '>', 'Any', ':', 'if', 'isinstance', '(', 'obj', ',', 'dict', ')', ':', 'return', '{', 'k', ':', 'stable_serialize', '(', 'v', ')', 'for', 'k', ',', 'v', 'in', 'sorted', '(', 'obj', '.', 'items', '(', ')', ')', '}', 'elif', 'isinstance', '(', 'obj', ',', '(', 'list', ',', 'tuple', ')', ')', ':', 'return', '[', 'stable_serialize', '(', 'x', ')', 'for', 'x', 'in', 'obj', ']', 'elif', 'isinstance', '(', 'obj', ',', '(', 'str', ',', 'int', ',', 'float', ',', 'bool', ',', 'type', '(', 'None', ')', ')', ')', ':', 'return', 'obj', 'else', ':', 'return', 'str', '(', 'obj', ')']","Recursively serialize data structures into a stable, sorted format.","['Recursively', 'serialize', 'data', 'structures', 'into', 'a', 'stable', ',', 'sorted', 'format', '.']"
81,"def register_mcp_tools(self):
        
        if self.mcp_client:
            for server_name in self.mcp_client.server_name_to_tools:
                for tool in self.mcp_client.server_name_to_tools[server_name]:
                    tool_name = f""mcp.{server_name}.{tool.name}""
                    self.registry.registry.actions[tool_name] = RegisteredAction(
                        name=tool_name,
                        description=tool.description,
                        function=tool,
                        param_model=create_tool_param_model(tool),
                    )
                    logger.info(f""Add mcp tool: {tool_name}"")
                logger.debug(
                    f""Registered {len(self.mcp_client.server_name_to_tools[server_name])} mcp tools for {server_name}"")
        else:
            logger.warning(f""MCP client not started."")","['def', 'register_mcp_tools', '(', 'self', ')', ':', 'if', 'self', '.', 'mcp_client', ':', 'for', 'server_name', 'in', 'self', '.', 'mcp_client', '.', 'server_name_to_tools', ':', 'for', 'tool', 'in', 'self', '.', 'mcp_client', '.', 'server_name_to_tools', '[', 'server_name', ']', ':', 'tool_name', '=', 'f', '""', 'mcp.', '{', 'server_name', '}', '.', '{', 'tool', '.', 'name', '}', '""', 'self', '.', 'registry', '.', 'registry', '.', 'actions', '[', 'tool_name', ']', '=', 'RegisteredAction', '(', 'name', '=', 'tool_name', ',', 'description', '=', 'tool', '.', 'description', ',', 'function', '=', 'tool', ',', 'param_model', '=', 'create_tool_param_model', '(', 'tool', ')', ',', ')', 'logger', '.', 'info', '(', 'f', '""', 'Add mcp tool: ', '{', 'tool_name', '}', '""', ')', 'logger', '.', 'debug', '(', 'f', '""', 'Registered ', '{', 'len', '(', 'self', '.', 'mcp_client', '.', 'server_name_to_tools', '[', 'server_name', ']', ')', '}', ' mcp tools for ', '{', 'server_name', '}', '""', ')', 'else', ':', 'logger', '.', 'warning', '(', 'f', '""', 'MCP client not started.', '""', ')']",Register and log MCP tools in the action registry if the MCP client is active.,"['Register', 'and', 'log', 'MCP', 'tools', 'in', 'the', 'action', 'registry', 'if', 'the', 'MCP', 'client', 'is', 'active', '.']"
83,"def get_model_config(version: str):
    
    if version not in MODEL_CONFIGS:
        raise ValueError(f""Unsupported model version '{version}'. Supported versions are: {list(MODEL_CONFIGS.keys())}"")
    return MODEL_CONFIGS[version]","['def', 'get_model_config', '(', 'version', ':', 'str', ')', ':', 'if', 'version', 'not', 'in', 'MODEL_CONFIGS', ':', 'raise', 'ValueError', '(', 'f', '""', 'Unsupported model version ', ""'"", '{', 'version', '}', ""'"", '. Supported versions are: ', '{', 'list', '(', 'MODEL_CONFIGS', '.', 'keys', '(', ')', ')', '}', '""', ')', 'return', 'MODEL_CONFIGS', '[', 'version', ']']","Retrieve configuration for specified model version, raising error if unsupported.","['Retrieve', 'configuration', 'for', 'specified', 'model', 'version', ',', 'raising', 'error', 'if', 'unsupported', '.']"
84,"def get_cortex_search_service(cfg: RetrievalProviderConfig) -> Tuple[str,str,str]:
    
    if not cfg:
        raise snowflake.ConfigurationError(""Unable to determine Snowflake configuration"")
    index_name = cfg.index_name
    if not index_name:
        raise snowflake.ConfigurationError(""Unable to determine Snowflake Cortex Search Service, is SNOWFLAKE_CORTEX_SEARCH_SERVICE set?"")
    parts = index_name.split(""."")
    if len(parts) != 3:
        raise snowflake.ConfigurationError(f""Invalid SNOWFLAKE_CORTEX_SEARCH_SERVICE, expected format:<database>.<schema>.<service>, got {index_name}"")
    return (parts[0], parts[1], parts[2])","['def', 'get_cortex_search_service', '(', 'cfg', ':', 'RetrievalProviderConfig', ')', '-', '>', 'Tuple', '[', 'str', ',', 'str', ',', 'str', ']', ':', 'if', 'not', 'cfg', ':', 'raise', 'snowflake', '.', 'ConfigurationError', '(', '""', 'Unable to determine Snowflake configuration', '""', ')', 'index_name', '=', 'cfg', '.', 'index_name', 'if', 'not', 'index_name', ':', 'raise', 'snowflake', '.', 'ConfigurationError', '(', '""', 'Unable to determine Snowflake Cortex Search Service, is SNOWFLAKE_CORTEX_SEARCH_SERVICE set?', '""', ')', 'parts', '=', 'index_name', '.', 'split', '(', '""', '.', '""', ')', 'if', 'len', '(', 'parts', ')', '!=', '3', ':', 'raise', 'snowflake', '.', 'ConfigurationError', '(', 'f', '""', 'Invalid SNOWFLAKE_CORTEX_SEARCH_SERVICE, expected format:<database>.<schema>.<service>, got ', '{', 'index_name', '}', '""', ')', 'return', '(', 'parts', '[', '0', ']', ',', 'parts', '[', '1', ']', ',', 'parts', '[', '2', ']', ')']",Validate and parse configuration for Snowflake Cortex Search Service details.,"['Validate', 'and', 'parse', 'configuration', 'for', 'Snowflake', 'Cortex', 'Search', 'Service', 'details', '.']"
86,"def _jsonl_to_csv(self, jsonl_file, csv_file):
        
        with open(jsonl_file, 'r', encoding='utf-8') as infile:
            data = [json.loads(line) for line in infile]
        
        if not data:
            print(""Empty JSONL file."")
            return
        
        with open(csv_file, 'w', newline='', encoding='utf-8') as outfile:
            writer = csv.DictWriter(outfile, fieldnames=data[0].keys())
            writer.writeheader()
            writer.writerows(data)
        
        print(f""Converted {jsonl_file} to {csv_file}"")","['def', '_jsonl_to_csv', '(', 'self', ',', 'jsonl_file', ',', 'csv_file', ')', ':', 'with', 'open', '(', 'jsonl_file', ',', ""'"", 'r', ""'"", ',', 'encoding', '=', ""'"", 'utf-8', ""'"", ')', 'as', 'infile', ':', 'data', '=', '[', 'json', '.', 'loads', '(', 'line', ')', 'for', 'line', 'in', 'infile', ']', 'if', 'not', 'data', ':', 'print', '(', '""', 'Empty JSONL file.', '""', ')', 'return', 'with', 'open', '(', 'csv_file', ',', ""'"", 'w', ""'"", ',', 'newline', '=', ""'"", ""'"", ',', 'encoding', '=', ""'"", 'utf-8', ""'"", ')', 'as', 'outfile', ':', 'writer', '=', 'csv', '.', 'DictWriter', '(', 'outfile', ',', 'fieldnames', '=', 'data', '[', '0', ']', '.', 'keys', '(', ')', ')', 'writer', '.', 'writeheader', '(', ')', 'writer', '.', 'writerows', '(', 'data', ')', 'print', '(', 'f', '""', 'Converted ', '{', 'jsonl_file', '}', ' to ', '{', 'csv_file', '}', '""', ')']",Convert JSONL data to CSV format and save to file,"['Convert', 'JSONL', 'data', 'to', 'CSV', 'format', 'and', 'save', 'to', 'file']"
87,"def recursive_mask_values(obj, parent_key=None):
            
            if isinstance(obj, dict):
                return {k: recursive_mask_values(v, k) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [recursive_mask_values(item, parent_key) for item in obj]
            elif isinstance(obj, str):
                # List of keys that should NOT be masked
                excluded_keys = {
                    'start_time', 'end_time', 'name', 'id', 
                    'hash_id', 'parent_id', 'source_hash_id',
                    'cost', 'type', 'feedback', 'error', 'ctx','telemetry.sdk.version',
                    'telemetry.sdk.language','service.name'
                }
                # Apply masking only if the key is NOT in the excluded list
                if parent_key and parent_key.lower() not in excluded_keys:
                    return masking_func(obj)
                return obj
            else:
                return obj","['def', 'recursive_mask_values', '(', 'obj', ',', 'parent_key', '=', 'None', ')', ':', 'if', 'isinstance', '(', 'obj', ',', 'dict', ')', ':', 'return', '{', 'k', ':', 'recursive_mask_values', '(', 'v', ',', 'k', ')', 'for', 'k', ',', 'v', 'in', 'obj', '.', 'items', '(', ')', '}', 'elif', 'isinstance', '(', 'obj', ',', 'list', ')', ':', 'return', '[', 'recursive_mask_values', '(', 'item', ',', 'parent_key', ')', 'for', 'item', 'in', 'obj', ']', 'elif', 'isinstance', '(', 'obj', ',', 'str', ')', ':', '# List of keys that should NOT be masked', 'excluded_keys', '=', '{', ""'"", 'start_time', ""'"", ',', ""'"", 'end_time', ""'"", ',', ""'"", 'name', ""'"", ',', ""'"", 'id', ""'"", ',', ""'"", 'hash_id', ""'"", ',', ""'"", 'parent_id', ""'"", ',', ""'"", 'source_hash_id', ""'"", ',', ""'"", 'cost', ""'"", ',', ""'"", 'type', ""'"", ',', ""'"", 'feedback', ""'"", ',', ""'"", 'error', ""'"", ',', ""'"", 'ctx', ""'"", ',', ""'"", 'telemetry.sdk.version', ""'"", ',', ""'"", 'telemetry.sdk.language', ""'"", ',', ""'"", 'service.name', ""'"", '}', '# Apply masking only if the key is NOT in the excluded list', 'if', 'parent_key', 'and', 'parent_key', '.', 'lower', '(', ')', 'not', 'in', 'excluded_keys', ':', 'return', 'masking_func', '(', 'obj', ')', 'return', 'obj', 'else', ':', 'return', 'obj']","Recursively mask string values in nested structures, excluding specified keys.","['Recursively', 'mask', 'string', 'values', 'in', 'nested', 'structures', ',', 'excluding', 'specified', 'keys', '.']"
88,"def deprecated(replacement: str = """"):
    
    import functools
    import warnings

    def decorator(func):
        qualified_name = _get_qualified_name(func)
        @functools.wraps(func)
        def wrapped(*args, **kwargs):
            msg = f""Warning: API '{qualified_name}' is deprecated.""
            if replacement:
                msg += f"" Please use '{replacement}' instead.""
            warnings.warn(msg, category=DeprecationWarning, stacklevel=2)
            return func(*args, **kwargs)
        return wrapped
    return decorator","['def', 'deprecated', '(', 'replacement', ':', 'str', '=', '""', '""', ')', ':', 'import', 'functools', 'import', 'warnings', 'def', 'decorator', '(', 'func', ')', ':', 'qualified_name', '=', '_get_qualified_name', '(', 'func', ')', '@functools', '.', 'wraps', '(', 'func', ')', 'def', 'wrapped', '(', '*', 'args', ',', '*', '*', 'kwargs', ')', ':', 'msg', '=', 'f', '""', 'Warning: API ', ""'"", '{', 'qualified_name', '}', ""'"", ' is deprecated.', '""', 'if', 'replacement', ':', 'msg', '+', '=', 'f', '""', ' Please use ', ""'"", '{', 'replacement', '}', ""'"", ' instead.', '""', 'warnings', '.', 'warn', '(', 'msg', ',', 'category', '=', 'DeprecationWarning', ',', 'stacklevel', '=', '2', ')', 'return', 'func', '(', '*', 'args', ',', '*', '*', 'kwargs', ')', 'return', 'wrapped', 'return', 'decorator']","Decorator issues deprecation warnings for outdated functions, suggesting replacements.","['Decorator', 'issues', 'deprecation', 'warnings', 'for', 'outdated', 'functions', ',', 'suggesting', 'replacements', '.']"
90,"def _format_timestamp(self, timestamp: str) -> str:
        
        if len(timestamp) < 14:
            return timestamp

        try:
            year = timestamp[0:4]
            month = timestamp[4:6]
            day = timestamp[6:8]
            hour = timestamp[8:10]
            minute = timestamp[10:12]
            second = timestamp[12:14]
            return f""{year}-{month}-{day} {hour}:{minute}:{second}""
        except Exception:
            return timestamp","['def', '_format_timestamp', '(', 'self', ',', 'timestamp', ':', 'str', ')', '-', '>', 'str', ':', 'if', 'len', '(', 'timestamp', ')', '<', '14', ':', 'return', 'timestamp', 'try', ':', 'year', '=', 'timestamp', '[', '0', ':', '4', ']', 'month', '=', 'timestamp', '[', '4', ':', '6', ']', 'day', '=', 'timestamp', '[', '6', ':', '8', ']', 'hour', '=', 'timestamp', '[', '8', ':', '10', ']', 'minute', '=', 'timestamp', '[', '10', ':', '12', ']', 'second', '=', 'timestamp', '[', '12', ':', '14', ']', 'return', 'f', '""', '{', 'year', '}', '-', '{', 'month', '}', '-', '{', 'day', '}', '{', 'hour', '}', ':', '{', 'minute', '}', ':', '{', 'second', '}', '""', 'except', 'Exception', ':', 'return', 'timestamp']",Convert a string timestamp into a formatted datetime string if valid,"['Convert', 'a', 'string', 'timestamp', 'into', 'a', 'formatted', 'datetime', 'string', 'if', 'valid']"
91,"def get_skill_states(skill_category: str) -> Set[str]:
    
    if skill_category in _skill_states_cache:
        return _skill_states_cache[skill_category]

    try:
        # Import the skill category module
        skill_module = importlib.import_module(f""skills.{skill_category}"")

        # Look for the SkillStates TypedDict class
        if hasattr(skill_module, ""SkillStates""):
            skill_states_class = getattr(skill_module, ""SkillStates"")
            # Get the annotations which contain the state names
            if hasattr(skill_states_class, ""__annotations__""):
                states = set(skill_states_class.__annotations__.keys())
                _skill_states_cache[skill_category] = states
                return states

        logger.warning(f""Could not find SkillStates for {skill_category}"")
        return set()

    except ImportError as e:
        logger.warning(f""Could not import skill category {skill_category}: {e}"")
        return set()","['def', 'get_skill_states', '(', 'skill_category', ':', 'str', ')', '-', '>', 'Set', '[', 'str', ']', ':', 'if', 'skill_category', 'in', '_skill_states_cache', ':', 'return', '_skill_states_cache', '[', 'skill_category', ']', 'try', ':', '# Import the skill category module', 'skill_module', '=', 'importlib', '.', 'import_module', '(', 'f', '""', 'skills.', '{', 'skill_category', '}', '""', ')', '# Look for the SkillStates TypedDict class', 'if', 'hasattr', '(', 'skill_module', ',', '""', 'SkillStates', '""', ')', ':', 'skill_states_class', '=', 'getattr', '(', 'skill_module', ',', '""', 'SkillStates', '""', ')', '# Get the annotations which contain the state names', 'if', 'hasattr', '(', 'skill_states_class', ',', '""', '__annotations__', '""', ')', ':', 'states', '=', 'set', '(', 'skill_states_class', '.', '__annotations__', '.', 'keys', '(', ')', ')', '_skill_states_cache', '[', 'skill_category', ']', '=', 'states', 'return', 'states', 'logger', '.', 'warning', '(', 'f', '""', 'Could not find SkillStates for ', '{', 'skill_category', '}', '""', ')', 'return', 'set', '(', ')', 'except', 'ImportError', 'as', 'e', ':', 'logger', '.', 'warning', '(', 'f', '""', 'Could not import skill category ', '{', 'skill_category', '}', ': ', '{', 'e', '}', '""', ')', 'return', 'set', '(', ')']",Retrieve and cache skill states from a module based on a given category.,"['Retrieve', 'and', 'cache', 'skill', 'states', 'from', 'a', 'module', 'based', 'on', 'a', 'given', 'category', '.']"
92,"def _progress_callback(self, message: str, progress: int, metadata: dict) -> None:
        
        logger.info(f""Progress: {progress}% - {message}"")
        if hasattr(self, ""progress_callback""):
            self.progress_callback(message, progress, metadata)","['def', '_progress_callback', '(', 'self', ',', 'message', ':', 'str', ',', 'progress', ':', 'int', ',', 'metadata', ':', 'dict', ')', '-', '>', 'None', ':', 'logger', '.', 'info', '(', 'f', '""', 'Progress: ', '{', 'progress', '}', '% - ', '{', 'message', '}', '""', ')', 'if', 'hasattr', '(', 'self', ',', '""', 'progress_callback', '""', ')', ':', 'self', '.', 'progress_callback', '(', 'message', ',', 'progress', ',', 'metadata', ')']",Logs progress updates and invokes a custom callback if available.,"['Logs', 'progress', 'updates', 'and', 'invokes', 'a', 'custom', 'callback', 'if', 'available', '.']"
93,"def custom_get_evaluation_llm(custom_config=None):
            
            if custom_config is None:
                custom_config = evaluation_config

            print(f""Getting evaluation LLM with config: {custom_config}"")
            return get_llm(**custom_config)","['def', 'custom_get_evaluation_llm', '(', 'custom_config', '=', 'None', ')', ':', 'if', 'custom_config', 'is', 'None', ':', 'custom_config', '=', 'evaluation_config', 'print', '(', 'f', '""', 'Getting evaluation LLM with config: ', '{', 'custom_config', '}', '""', ')', 'return', 'get_llm', '(', '*', '*', 'custom_config', ')']",Retrieve and configure a language model for evaluation using specified or default settings.,"['Retrieve', 'and', 'configure', 'a', 'language', 'model', 'for', 'evaluation', 'using', 'specified', 'or', 'default', 'settings', '.']"
94,"def to_dict(self):
        
        result = {
            ""id"": self.id,
            ""name"": self.name,
            ""type"": self.type,
            ""path"": self.path,
            ""created_at"": self.created_at.isoformat() if self.created_at else None,
            ""meta_data"": self.meta_data,
        }
        if self.document_id:
            result[""document_id""] = self.document_id
        return result","['def', 'to_dict', '(', 'self', ')', ':', 'result', '=', '{', '""', 'id', '""', ':', 'self', '.', 'id', ',', '""', 'name', '""', ':', 'self', '.', 'name', ',', '""', 'type', '""', ':', 'self', '.', 'type', ',', '""', 'path', '""', ':', 'self', '.', 'path', ',', '""', 'created_at', '""', ':', 'self', '.', 'created_at', '.', 'isoformat', '(', ')', 'if', 'self', '.', 'created_at', 'else', 'None', ',', '""', 'meta_data', '""', ':', 'self', '.', 'meta_data', ',', '}', 'if', 'self', '.', 'document_id', ':', 'result', '[', '""', 'document_id', '""', ']', '=', 'self', '.', 'document_id', 'return', 'result']",Convert object attributes to a dictionary representation with optional fields.,"['Convert', 'object', 'attributes', 'to', 'a', 'dictionary', 'representation', 'with', 'optional', 'fields', '.']"
95,"def mcp_tool(
    name: str | None = None,
    description: str | None = None,
    tags: set[str] | None = None,
) -> Callable[[Callable[..., Any]], Callable[..., Any]]:
    

    def decorator(func: Callable[..., Any]) -> Callable[..., Any]:
        call_args = {
            ""name"": name or func.__name__,
            ""description"": description,
            ""tags"": tags,
        }
        call_args = {k: v for k, v in call_args.items() if v is not None}
        setattr(func, _MCP_REGISTRATION_TOOL_ATTR, call_args)
        return func

    return decorator","['def', 'mcp_tool', '(', 'name', ':', 'str', '|', 'None', '=', 'None', ',', 'description', ':', 'str', '|', 'None', '=', 'None', ',', 'tags', ':', 'set', '[', 'str', ']', '|', 'None', '=', 'None', ',', ')', '-', '>', 'Callable', '[', '[', 'Callable', '[', '.', '.', '.', ',', 'Any', ']', ']', ',', 'Callable', '[', '.', '.', '.', ',', 'Any', ']', ']', ':', 'def', 'decorator', '(', 'func', ':', 'Callable', '[', '.', '.', '.', ',', 'Any', ']', ')', '-', '>', 'Callable', '[', '.', '.', '.', ',', 'Any', ']', ':', 'call_args', '=', '{', '""', 'name', '""', ':', 'name', 'or', 'func', '.', '__name__', ',', '""', 'description', '""', ':', 'description', ',', '""', 'tags', '""', ':', 'tags', ',', '}', 'call_args', '=', '{', 'k', ':', 'v', 'for', 'k', ',', 'v', 'in', 'call_args', '.', 'items', '(', ')', 'if', 'v', 'is', 'not', 'None', '}', 'setattr', '(', 'func', ',', '_MCP_REGISTRATION_TOOL_ATTR', ',', 'call_args', ')', 'return', 'func', 'return', 'decorator']",Decorator function to register metadata attributes to another function.,"['Decorator', 'function', 'to', 'register', 'metadata', 'attributes', 'to', 'another', 'function', '.']"
96,"def trace_id_to_uuid(trace_id: str) -> str:
    
    trace_id = str(trace.format_trace_id(trace_id))
    if len(trace_id) != 32:
        raise ValueError(""Trace ID must be a 32-character hexadecimal string"")
    return f""{trace_id[:8]}-{trace_id[8:12]}-{trace_id[12:16]}-{trace_id[16:20]}-{trace_id[20:]}""","['def', 'trace_id_to_uuid', '(', 'trace_id', ':', 'str', ')', '-', '>', 'str', ':', 'trace_id', '=', 'str', '(', 'trace', '.', 'format_trace_id', '(', 'trace_id', ')', ')', 'if', 'len', '(', 'trace_id', ')', '!=', '32', ':', 'raise', 'ValueError', '(', '""', 'Trace ID must be a 32-character hexadecimal string', '""', ')', 'return', 'f', '""', '{', 'trace_id', '[', ':', '8', ']', '}', '-', '{', 'trace_id', '[', '8', ':', '12', ']', '}', '-', '{', 'trace_id', '[', '12', ':', '16', ']', '}', '-', '{', 'trace_id', '[', '16', ':', '20', ']', '}', '-', '{', 'trace_id', '[', '20', ':', ']', '}', '""']",Convert a 32-character trace ID into a UUID format.,"['Convert', 'a', '32-character', 'trace', 'ID', 'into', 'a', 'UUID', 'format', '.']"
97,"def _get_endpoint_config(self):
        
        endpoint_config = CONFIG.retrieval_endpoints.get(self.endpoint_name)
        
        if not endpoint_config:
            error_msg = f""No configuration found for endpoint {self.endpoint_name}""
            logger.error(error_msg)
            raise ValueError(error_msg)
        
        # Verify this is an Azure AI Search endpoint
        if endpoint_config.db_type != ""azure_ai_search"":
            error_msg = f""Endpoint {self.endpoint_name} is not an Azure AI Search endpoint (type: {endpoint_config.db_type})""
            logger.error(error_msg)
            raise ValueError(error_msg)
            
        return endpoint_config","['def', '_get_endpoint_config', '(', 'self', ')', ':', 'endpoint_config', '=', 'CONFIG', '.', 'retrieval_endpoints', '.', 'get', '(', 'self', '.', 'endpoint_name', ')', 'if', 'not', 'endpoint_config', ':', 'error_msg', '=', 'f', '""', 'No configuration found for endpoint ', '{', 'self', '.', 'endpoint_name', '}', '""', 'logger', '.', 'error', '(', 'error_msg', ')', 'raise', 'ValueError', '(', 'error_msg', ')', '# Verify this is an Azure AI Search endpoint', 'if', 'endpoint_config', '.', 'db_type', '!=', '""', 'azure_ai_search', '""', ':', 'error_msg', '=', 'f', '""', 'Endpoint ', '{', 'self', '.', 'endpoint_name', '}', ' is not an Azure AI Search endpoint (type: ', '{', 'endpoint_config', '.', 'db_type', '}', ')', '""', 'logger', '.', 'error', '(', 'error_msg', ')', 'raise', 'ValueError', '(', 'error_msg', ')', 'return', 'endpoint_config']",Validate and retrieve Azure AI Search endpoint configuration,"['Validate', 'and', 'retrieve', 'Azure', 'AI', 'Search', 'endpoint', 'configuration']"
98,"def save_results(results, output_dir):
    
    output_path = Path(output_dir) / ""rich_autoscan_results.json""

    # Convert results to serializable format
    serializable_results = []
    for result in results:
        if result is None:
            continue
        serializable_results.append(result)

    with open(output_path, ""w"") as f:
        json.dump(serializable_results, f, indent=2, default=lambda o: o.value if isinstance(o, Enum) else o)

    print(f""Results saved to {output_path}"")","['def', 'save_results', '(', 'results', ',', 'output_dir', ')', ':', 'output_path', '=', 'Path', '(', 'output_dir', ')', '/', '""', 'rich_autoscan_results.json', '""', '# Convert results to serializable format', 'serializable_results', '=', '[', ']', 'for', 'result', 'in', 'results', ':', 'if', 'result', 'is', 'None', ':', 'continue', 'serializable_results', '.', 'append', '(', 'result', ')', 'with', 'open', '(', 'output_path', ',', '""', 'w', '""', ')', 'as', 'f', ':', 'json', '.', 'dump', '(', 'serializable_results', ',', 'f', ',', 'indent', '=', '2', ',', 'default', '=', 'lambda', 'o', ':', 'o', '.', 'value', 'if', 'isinstance', '(', 'o', ',', 'Enum', ')', 'else', 'o', ')', 'print', '(', 'f', '""', 'Results saved to ', '{', 'output_path', '}', '""', ')']",Serialize and save processed results to a JSON file in the specified directory.,"['Serialize', 'and', 'save', 'processed', 'results', 'to', 'a', 'JSON', 'file', 'in', 'the', 'specified', 'directory', '.']"
103,"def __exit__(self, exc_type, exc_value, exc_tb):
        
        if os.environ.get('HY3DGEN_DEBUG', '0') == '1':
            self.end.record()
            torch.cuda.synchronize()
            self.time = self.start.elapsed_time(self.end)
            if self.name is not None:
                logger.info(f'{self.name} takes {self.time} ms')","['def', '__exit__', '(', 'self', ',', 'exc_type', ',', 'exc_value', ',', 'exc_tb', ')', ':', 'if', 'os', '.', 'environ', '.', 'get', '(', ""'"", 'HY3DGEN_DEBUG', ""'"", ',', ""'"", '0', ""'"", ')', '==', ""'"", '1', ""'"", ':', 'self', '.', 'end', '.', 'record', '(', ')', 'torch', '.', 'cuda', '.', 'synchronize', '(', ')', 'self', '.', 'time', '=', 'self', '.', 'start', '.', 'elapsed_time', '(', 'self', '.', 'end', ')', 'if', 'self', '.', 'name', 'is', 'not', 'None', ':', 'logger', '.', 'info', '(', 'f', ""'"", '{', 'self', '.', 'name', '}', ' takes ', '{', 'self', '.', 'time', '}', ' ms', ""'"", ')']",Log execution time if debugging is enabled in environment settings.,"['Log', 'execution', 'time', 'if', 'debugging', 'is', 'enabled', 'in', 'environment', 'settings', '.']"
104,"def create_turn_dir(self) -> None:
        
        if not self.run_dir:
            logger.warning(""Cannot create turn directory: run_dir not set"")
            return

        # Increment turn counter
        self.turn_count += 1

        # Create turn directory with padded number
        turn_name = f""turn_{self.turn_count:03d}""
        self.current_turn_dir = os.path.join(self.run_dir, turn_name)
        os.makedirs(self.current_turn_dir, exist_ok=True)
        logger.info(f""Created turn directory: {self.current_turn_dir}"")","['def', 'create_turn_dir', '(', 'self', ')', '-', '>', 'None', ':', 'if', 'not', 'self', '.', 'run_dir', ':', 'logger', '.', 'warning', '(', '""', 'Cannot create turn directory: run_dir not set', '""', ')', 'return', '# Increment turn counter', 'self', '.', 'turn_count', '+', '=', '1', '# Create turn directory with padded number', 'turn_name', '=', 'f', '""', 'turn_', '{', 'self', '.', 'turn_count', ':', '03d', '}', '""', 'self', '.', 'current_turn_dir', '=', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'run_dir', ',', 'turn_name', ')', 'os', '.', 'makedirs', '(', 'self', '.', 'current_turn_dir', ',', 'exist_ok', '=', 'True', ')', 'logger', '.', 'info', '(', 'f', '""', 'Created turn directory: ', '{', 'self', '.', 'current_turn_dir', '}', '""', ')']","Create a new directory for each turn in a sequence, logging the process.","['Create', 'a', 'new', 'directory', 'for', 'each', 'turn', 'in', 'a', 'sequence', ',', 'logging', 'the', 'process', '.']"
105,"def copy_and_update(self, *args, **kwargs):
        
        new_info = asdict(self)

        for si in args:
            assert isinstance(si, StreamInfo)
            new_info.update({k: v for k, v in asdict(si).items() if v is not None})

        if len(kwargs) > 0:
            new_info.update(kwargs)

        return StreamInfo(**new_info)","['def', 'copy_and_update', '(', 'self', ',', '*', 'args', ',', '*', '*', 'kwargs', ')', ':', 'new_info', '=', 'asdict', '(', 'self', ')', 'for', 'si', 'in', 'args', ':', 'assert', 'isinstance', '(', 'si', ',', 'StreamInfo', ')', 'new_info', '.', 'update', '(', '{', 'k', ':', 'v', 'for', 'k', ',', 'v', 'in', 'asdict', '(', 'si', ')', '.', 'items', '(', ')', 'if', 'v', 'is', 'not', 'None', '}', ')', 'if', 'len', '(', 'kwargs', ')', '>', '0', ':', 'new_info', '.', 'update', '(', 'kwargs', ')', 'return', 'StreamInfo', '(', '*', '*', 'new_info', ')']",Create a new StreamInfo by merging attributes from instances and keyword arguments,"['Create', 'a', 'new', 'StreamInfo', 'by', 'merging', 'attributes', 'from', 'instances', 'and', 'keyword', 'arguments']"
106,"def write_output(
    inference_result: model.InferenceResult,
    output_dir: os.PathLike[str] | str,
    terms_of_use: str | None = None,
    name: str | None = None,
) -> None:
  
  processed_result = post_process_inference_result(inference_result)

  prefix = f'{name}_' if name is not None else ''

  with open(os.path.join(output_dir, f'{prefix}model.cif'), 'wb') as f:
    f.write(processed_result.cif)

  with open(
      os.path.join(output_dir, f'{prefix}summary_confidences.json'), 'wb'
  ) as f:
    f.write(processed_result.structure_confidence_summary_json)

  with open(os.path.join(output_dir, f'{prefix}confidences.json'), 'wb') as f:
    f.write(processed_result.structure_full_data_json)

  if terms_of_use is not None:
    with open(os.path.join(output_dir, 'TERMS_OF_USE.md'), 'wt') as f:
      f.write(terms_of_use)","['def', 'write_output', '(', 'inference_result', ':', 'model', '.', 'InferenceResult', ',', 'output_dir', ':', 'os', '.', 'PathLike', '[', 'str', ']', '|', 'str', ',', 'terms_of_use', ':', 'str', '|', 'None', '=', 'None', ',', 'name', ':', 'str', '|', 'None', '=', 'None', ',', ')', '-', '>', 'None', ':', 'processed_result', '=', 'post_process_inference_result', '(', 'inference_result', ')', 'prefix', '=', 'f', ""'"", '{', 'name', '}', '_', ""'"", 'if', 'name', 'is', 'not', 'None', 'else', ""'"", ""'"", 'with', 'open', '(', 'os', '.', 'path', '.', 'join', '(', 'output_dir', ',', 'f', ""'"", '{', 'prefix', '}', 'model.cif', ""'"", ')', ',', ""'"", 'wb', ""'"", ')', 'as', 'f', ':', 'f', '.', 'write', '(', 'processed_result', '.', 'cif', ')', 'with', 'open', '(', 'os', '.', 'path', '.', 'join', '(', 'output_dir', ',', 'f', ""'"", '{', 'prefix', '}', 'summary_confidences.json', ""'"", ')', ',', ""'"", 'wb', ""'"", ')', 'as', 'f', ':', 'f', '.', 'write', '(', 'processed_result', '.', 'structure_confidence_summary_json', ')', 'with', 'open', '(', 'os', '.', 'path', '.', 'join', '(', 'output_dir', ',', 'f', ""'"", '{', 'prefix', '}', 'confidences.json', ""'"", ')', ',', ""'"", 'wb', ""'"", ')', 'as', 'f', ':', 'f', '.', 'write', '(', 'processed_result', '.', 'structure_full_data_json', ')', 'if', 'terms_of_use', 'is', 'not', 'None', ':', 'with', 'open', '(', 'os', '.', 'path', '.', 'join', '(', 'output_dir', ',', ""'"", 'TERMS_OF_USE.md', ""'"", ')', ',', ""'"", 'wt', ""'"", ')', 'as', 'f', ':', 'f', '.', 'write', '(', 'terms_of_use', ')']",Save processed model results and optional terms of use to specified directory.,"['Save', 'processed', 'model', 'results', 'and', 'optional', 'terms', 'of', 'use', 'to', 'specified', 'directory', '.']"
107,"def xpath_attr_functional_pseudo_element(
            xpath: OriginalXPathExpr, function: FunctionalPseudoElement
    ) -> XPathExpr:
        
        if function.argument_types() not in ([""STRING""], [""IDENT""]):
            raise ExpressionError(
                f""Expected a single string or ident for ::attr(), got {function.arguments!r}""
            )
        return XPathExpr.from_xpath(xpath, attribute=function.arguments[0].value)","['def', 'xpath_attr_functional_pseudo_element', '(', 'xpath', ':', 'OriginalXPathExpr', ',', 'function', ':', 'FunctionalPseudoElement', ')', '-', '>', 'XPathExpr', ':', 'if', 'function', '.', 'argument_types', '(', ')', 'not', 'in', '(', '[', '""', 'STRING', '""', ']', ',', '[', '""', 'IDENT', '""', ']', ')', ':', 'raise', 'ExpressionError', '(', 'f', '""', 'Expected a single string or ident for ::attr(), got ', '{', 'function', '.', 'arguments', '!r}', '""', ')', 'return', 'XPathExpr', '.', 'from_xpath', '(', 'xpath', ',', 'attribute', '=', 'function', '.', 'arguments', '[', '0', ']', '.', 'value', ')']",Convert functional pseudo-element to XPath expression with attribute handling.,"['Convert', 'functional', 'pseudo-element', 'to', 'XPath', 'expression', 'with', 'attribute', 'handling', '.']"
109,"def initialize_cache(self, num_gpu_blocks: int, num_cpu_blocks: int) -> None:
        

        # NOTE: We log here to avoid multiple logs when number of workers is
        # greater than one. We could log in the engine, but not all executors
        # have GPUs.
        logger.info(""# GPU blocks: %d, # CPU blocks: %d"", num_gpu_blocks, num_cpu_blocks)

        self.cache_config.num_gpu_blocks = num_gpu_blocks
        self.cache_config.num_cpu_blocks = num_cpu_blocks

        if torch.distributed.get_rank() == 0:
            print(
                f""before init cache memory allocated: {torch.cuda.memory_allocated() / 1e9}GB, reserved: {torch.cuda.memory_reserved() / 1e9}GB""
            )
        self.worker.initialize_cache(num_gpu_blocks=num_gpu_blocks, num_cpu_blocks=num_cpu_blocks)
        if torch.distributed.get_rank() == 0:
            print(
                f""after init cache memory allocated: {torch.cuda.memory_allocated() / 1e9}GB, reserved: {torch.cuda.memory_reserved() / 1e9}GB""
            )","['def', 'initialize_cache', '(', 'self', ',', 'num_gpu_blocks', ':', 'int', ',', 'num_cpu_blocks', ':', 'int', ')', '-', '>', 'None', ':', '# NOTE: We log here to avoid multiple logs when number of workers is', '# greater than one. We could log in the engine, but not all executors', '# have GPUs.', 'logger', '.', 'info', '(', '""', '# GPU blocks: ', '%d', ', # CPU blocks: ', '%d', '""', ',', 'num_gpu_blocks', ',', 'num_cpu_blocks', ')', 'self', '.', 'cache_config', '.', 'num_gpu_blocks', '=', 'num_gpu_blocks', 'self', '.', 'cache_config', '.', 'num_cpu_blocks', '=', 'num_cpu_blocks', 'if', 'torch', '.', 'distributed', '.', 'get_rank', '(', ')', '==', '0', ':', 'print', '(', 'f', '""', 'before init cache memory allocated: ', '{', 'torch', '.', 'cuda', '.', 'memory_allocated', '(', ')', '/', '1e9', '}', 'GB, reserved: ', '{', 'torch', '.', 'cuda', '.', 'memory_reserved', '(', ')', '/', '1e9', '}', 'GB', '""', ')', 'self', '.', 'worker', '.', 'initialize_cache', '(', 'num_gpu_blocks', '=', 'num_gpu_blocks', ',', 'num_cpu_blocks', '=', 'num_cpu_blocks', ')', 'if', 'torch', '.', 'distributed', '.', 'get_rank', '(', ')', '==', '0', ':', 'print', '(', 'f', '""', 'after init cache memory allocated: ', '{', 'torch', '.', 'cuda', '.', 'memory_allocated', '(', ')', '/', '1e9', '}', 'GB, reserved: ', '{', 'torch', '.', 'cuda', '.', 'memory_reserved', '(', ')', '/', '1e9', '}', 'GB', '""', ')']","Initialize cache with specified GPU and CPU blocks, logging memory usage and configuration.","['Initialize', 'cache', 'with', 'specified', 'GPU', 'and', 'CPU', 'blocks', ',', 'logging', 'memory', 'usage', 'and', 'configuration', '.']"
112,"def extract_urls_from_jsonl(file_path):
    
    urls = set()
    url_to_data = {}
    with open(file_path, ""r"", encoding=""utf-8"") as f:
        for line in f:
            try:
                data = json.loads(line.strip())
                if ""url"" in data and data[""url""]:
                    url = data[""url""]
                    urls.add(url)
                    # Store minimal context for each URL
                    url_to_data[url] = {""id"": data.get(""id"", """"), ""type"": data.get(""type"", """"), ""page"": data.get(""page"", """")}
            except json.JSONDecodeError:
                print(f""Warning: Could not parse JSON from line in {file_path}"")
                continue
    return urls, url_to_data","['def', 'extract_urls_from_jsonl', '(', 'file_path', ')', ':', 'urls', '=', 'set', '(', ')', 'url_to_data', '=', '{', '}', 'with', 'open', '(', 'file_path', ',', '""', 'r', '""', ',', 'encoding', '=', '""', 'utf-8', '""', ')', 'as', 'f', ':', 'for', 'line', 'in', 'f', ':', 'try', ':', 'data', '=', 'json', '.', 'loads', '(', 'line', '.', 'strip', '(', ')', ')', 'if', '""', 'url', '""', 'in', 'data', 'and', 'data', '[', '""', 'url', '""', ']', ':', 'url', '=', 'data', '[', '""', 'url', '""', ']', 'urls', '.', 'add', '(', 'url', ')', '# Store minimal context for each URL', 'url_to_data', '[', 'url', ']', '=', '{', '""', 'id', '""', ':', 'data', '.', 'get', '(', '""', 'id', '""', ',', '""', '""', ')', ',', '""', 'type', '""', ':', 'data', '.', 'get', '(', '""', 'type', '""', ',', '""', '""', ')', ',', '""', 'page', '""', ':', 'data', '.', 'get', '(', '""', 'page', '""', ',', '""', '""', ')', '}', 'except', 'json', '.', 'JSONDecodeError', ':', 'print', '(', 'f', '""', 'Warning: Could not parse JSON from line in ', '{', 'file_path', '}', '""', ')', 'continue', 'return', 'urls', ',', 'url_to_data']",Extract unique URLs and associated metadata from a JSONL file,"['Extract', 'unique', 'URLs', 'and', 'associated', 'metadata', 'from', 'a', 'JSONL', 'file']"
113,"def get_status_html(is_available: bool) -> str:
    
    color = ""green"" if is_available else ""red""
    status = ""Available"" if is_available else ""Unavailable""
    return f","['def', 'get_status_html', '(', 'is_available', ':', 'bool', ')', '-', '>', 'str', ':', 'color', '=', '""', 'green', '""', 'if', 'is_available', 'else', '""', 'red', '""', 'status', '=', '""', 'Available', '""', 'if', 'is_available', 'else', '""', 'Unavailable', '""', 'return', 'f']",Generate HTML status message with color based on availability,"['Generate', 'HTML', 'status', 'message', 'with', 'color', 'based', 'on', 'availability']"
114,"def get_port():
    
    if 'PORT' in os.environ:
        port = int(os.environ['PORT'])
        print(f""Using PORT from environment variable: {port}"")
        return port
    elif 'WEBSITE_SITE_NAME' in os.environ:
        # Running in Azure App Service
        print(""Running in Azure App Service, using default port 8000"")
        return 8000  # Azure will redirect requests to this port
    else:
        # Use configured port
        print(f""Using configured port {CONFIG.port}"")
        return CONFIG.port","['def', 'get_port', '(', ')', ':', 'if', ""'"", 'PORT', ""'"", 'in', 'os', '.', 'environ', ':', 'port', '=', 'int', '(', 'os', '.', 'environ', '[', ""'"", 'PORT', ""'"", ']', ')', 'print', '(', 'f', '""', 'Using PORT from environment variable: ', '{', 'port', '}', '""', ')', 'return', 'port', 'elif', ""'"", 'WEBSITE_SITE_NAME', ""'"", 'in', 'os', '.', 'environ', ':', '# Running in Azure App Service', 'print', '(', '""', 'Running in Azure App Service, using default port 8000', '""', ')', 'return', '8000', '# Azure will redirect requests to this port', 'else', ':', '# Use configured port', 'print', '(', 'f', '""', 'Using configured port ', '{', 'CONFIG', '.', 'port', '}', '""', ')', 'return', 'CONFIG', '.', 'port']",Determine the appropriate server port based on environment variables or configuration.,"['Determine', 'the', 'appropriate', 'server', 'port', 'based', 'on', 'environment', 'variables', 'or', 'configuration', '.']"
115,"def parse_rules_file(file_path):
    
    pdf_rules = defaultdict(list)

    with open(file_path, ""r"") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            try:
                rule = json.loads(line)
                if ""pdf"" in rule:
                    pdf_rules[rule[""pdf""]].append(rule)
            except json.JSONDecodeError:
                print(f""Warning: Could not parse line as JSON: {line}"")

    return pdf_rules","['def', 'parse_rules_file', '(', 'file_path', ')', ':', 'pdf_rules', '=', 'defaultdict', '(', 'list', ')', 'with', 'open', '(', 'file_path', ',', '""', 'r', '""', ')', 'as', 'f', ':', 'for', 'line', 'in', 'f', ':', 'line', '=', 'line', '.', 'strip', '(', ')', 'if', 'not', 'line', ':', 'continue', 'try', ':', 'rule', '=', 'json', '.', 'loads', '(', 'line', ')', 'if', '""', 'pdf', '""', 'in', 'rule', ':', 'pdf_rules', '[', 'rule', '[', '""', 'pdf', '""', ']', ']', '.', 'append', '(', 'rule', ')', 'except', 'json', '.', 'JSONDecodeError', ':', 'print', '(', 'f', '""', 'Warning: Could not parse line as JSON: ', '{', 'line', '}', '""', ')', 'return', 'pdf_rules']",Parse a file to extract and organize PDF-related rules into a dictionary.,"['Parse', 'a', 'file', 'to', 'extract', 'and', 'organize', 'PDF-related', 'rules', 'into', 'a', 'dictionary', '.']"
119,"def recreate_collection(collection_name, vector_size):
    
    if client.collection_exists(collection_name):
        print(f""Dropping existing collection '{collection_name}'"")
        client.delete_collection(collection_name)

    print(f""Creating collection '{collection_name}' with vector size {vector_size}"")
    client.create_collection(
        collection_name=collection_name,
        vectors_config=VectorParams(size=vector_size, distance=Distance.COSINE),
    )","['def', 'recreate_collection', '(', 'collection_name', ',', 'vector_size', ')', ':', 'if', 'client', '.', 'collection_exists', '(', 'collection_name', ')', ':', 'print', '(', 'f', '""', 'Dropping existing collection ', ""'"", '{', 'collection_name', '}', ""'"", '""', ')', 'client', '.', 'delete_collection', '(', 'collection_name', ')', 'print', '(', 'f', '""', 'Creating collection ', ""'"", '{', 'collection_name', '}', ""'"", ' with vector size ', '{', 'vector_size', '}', '""', ')', 'client', '.', 'create_collection', '(', 'collection_name', '=', 'collection_name', ',', 'vectors_config', '=', 'VectorParams', '(', 'size', '=', 'vector_size', ',', 'distance', '=', 'Distance', '.', 'COSINE', ')', ',', ')']",Recreate a vector collection by deleting and then creating it anew.,"['Recreate', 'a', 'vector', 'collection', 'by', 'deleting', 'and', 'then', 'creating', 'it', 'anew', '.']"
120,"def to_dict(self):
        
        return {
            ""retrieved_task"": self.retrieved_task,
            ""sub_question"": [item.to_dict() for item in self.sub_retrieved_set],
            ""graph_data"": (
                [str(spo) for spo in self.graph_data.get_all_spo()]
                if self.graph_data
                else []
            ),
            ""chunk_datas"": [item.to_dict() for item in self.chunk_datas],
            ""summary"": self.summary,
        }","['def', 'to_dict', '(', 'self', ')', ':', 'return', '{', '""', 'retrieved_task', '""', ':', 'self', '.', 'retrieved_task', ',', '""', 'sub_question', '""', ':', '[', 'item', '.', 'to_dict', '(', ')', 'for', 'item', 'in', 'self', '.', 'sub_retrieved_set', ']', ',', '""', 'graph_data', '""', ':', '(', '[', 'str', '(', 'spo', ')', 'for', 'spo', 'in', 'self', '.', 'graph_data', '.', 'get_all_spo', '(', ')', ']', 'if', 'self', '.', 'graph_data', 'else', '[', ']', ')', ',', '""', 'chunk_datas', '""', ':', '[', 'item', '.', 'to_dict', '(', ')', 'for', 'item', 'in', 'self', '.', 'chunk_datas', ']', ',', '""', 'summary', '""', ':', 'self', '.', 'summary', ',', '}']",Convert object attributes to a structured dictionary format.,"['Convert', 'object', 'attributes', 'to', 'a', 'structured', 'dictionary', 'format', '.']"
122,"def _fetch_max_batch_size(self, model_name, model_version: str = """") -> int:
        
        if model_name in self._max_batch_sizes:
            return self._max_batch_sizes[model_name]

        with self._lock:
            # Double check, just in case another thread set the value while we were waiting
            if model_name in self._max_batch_sizes:
                return self._max_batch_sizes[model_name]

            if not self._grpc_endpoint:
                self._max_batch_sizes[model_name] = 1
                return 1

            try:
                client = self.client if self.client else grpcclient.InferenceServerClient(url=self._grpc_endpoint)
                model_config = client.get_model_config(model_name=model_name, model_version=model_version)
                self._max_batch_sizes[model_name] = model_config.config.max_batch_size
                logger.debug(f""Max batch size for model '{model_name}': {self._max_batch_sizes[model_name]}"")
            except Exception as e:
                self._max_batch_sizes[model_name] = 1
                logger.warning(f""Failed to retrieve max batch size: {e}, defaulting to 1"")

            return self._max_batch_sizes[model_name]","['def', '_fetch_max_batch_size', '(', 'self', ',', 'model_name', ',', 'model_version', ':', 'str', '=', '""', '""', ')', '-', '>', 'int', ':', 'if', 'model_name', 'in', 'self', '.', '_max_batch_sizes', ':', 'return', 'self', '.', '_max_batch_sizes', '[', 'model_name', ']', 'with', 'self', '.', '_lock', ':', '# Double check, just in case another thread set the value while we were waiting', 'if', 'model_name', 'in', 'self', '.', '_max_batch_sizes', ':', 'return', 'self', '.', '_max_batch_sizes', '[', 'model_name', ']', 'if', 'not', 'self', '.', '_grpc_endpoint', ':', 'self', '.', '_max_batch_sizes', '[', 'model_name', ']', '=', '1', 'return', '1', 'try', ':', 'client', '=', 'self', '.', 'client', 'if', 'self', '.', 'client', 'else', 'grpcclient', '.', 'InferenceServerClient', '(', 'url', '=', 'self', '.', '_grpc_endpoint', ')', 'model_config', '=', 'client', '.', 'get_model_config', '(', 'model_name', '=', 'model_name', ',', 'model_version', '=', 'model_version', ')', 'self', '.', '_max_batch_sizes', '[', 'model_name', ']', '=', 'model_config', '.', 'config', '.', 'max_batch_size', 'logger', '.', 'debug', '(', 'f', '""', 'Max batch size for model ', ""'"", '{', 'model_name', '}', ""'"", ': ', '{', 'self', '.', '_max_batch_sizes', '[', 'model_name', ']', '}', '""', ')', 'except', 'Exception', 'as', 'e', ':', 'self', '.', '_max_batch_sizes', '[', 'model_name', ']', '=', '1', 'logger', '.', 'warning', '(', 'f', '""', 'Failed to retrieve max batch size: ', '{', 'e', '}', ', defaulting to 1', '""', ')', 'return', 'self', '.', '_max_batch_sizes', '[', 'model_name', ']']",Retrieve and cache the maximum batch size for a given model using gRPC client,"['Retrieve', 'and', 'cache', 'the', 'maximum', 'batch', 'size', 'for', 'a', 'given', 'model', 'using', 'gRPC', 'client']"
124,"def from_dict(cls, data: dict) -> ""L1GenerationResult"":
        
        return cls(
            bio=data.get(""bio""),
            clusters=data.get(""clusters"", {""clusterList"": []}),
            chunk_topics=data.get(""chunk_topics"", {}),
            generate_time=datetime.fromisoformat(data[""generate_time""])
            if ""generate_time"" in data
            else datetime.now(),
        )","['def', 'from_dict', '(', 'cls', ',', 'data', ':', 'dict', ')', '-', '>', '""', 'L1GenerationResult', '""', ':', 'return', 'cls', '(', 'bio', '=', 'data', '.', 'get', '(', '""', 'bio', '""', ')', ',', 'clusters', '=', 'data', '.', 'get', '(', '""', 'clusters', '""', ',', '{', '""', 'clusterList', '""', ':', '[', ']', '}', ')', ',', 'chunk_topics', '=', 'data', '.', 'get', '(', '""', 'chunk_topics', '""', ',', '{', '}', ')', ',', 'generate_time', '=', 'datetime', '.', 'fromisoformat', '(', 'data', '[', '""', 'generate_time', '""', ']', ')', 'if', '""', 'generate_time', '""', 'in', 'data', 'else', 'datetime', '.', 'now', '(', ')', ',', ')']",Convert dictionary data into an L1GenerationResult object instance.,"['Convert', 'dictionary', 'data', 'into', 'an', 'L1GenerationResult', 'object', 'instance', '.']"
125,"def extract_json_from_string(s: str) -> Optional[Any]:
    
    # Regex to find JSON objects (greedy, matches first { to last })
    match = re.search(r""\{.*\}"", s, re.DOTALL)
    if match:
        json_str = match.group(0)
        try:
            return json.loads(json_str)
        except json.JSONDecodeError:
            return None
    return None","['def', 'extract_json_from_string', '(', 's', ':', 'str', ')', '-', '>', 'Optional', '[', 'Any', ']', ':', '# Regex to find JSON objects (greedy, matches first { to last })', 'match', '=', 're', '.', 'search', '(', 'r', '""', '\\', '{', '.*', '\\', '}', '""', ',', 's', ',', 're', '.', 'DOTALL', ')', 'if', 'match', ':', 'json_str', '=', 'match', '.', 'group', '(', '0', ')', 'try', ':', 'return', 'json', '.', 'loads', '(', 'json_str', ')', 'except', 'json', '.', 'JSONDecodeError', ':', 'return', 'None', 'return', 'None']",Extracts and parses JSON object from a given string using regex.,"['Extracts', 'and', 'parses', 'JSON', 'object', 'from', 'a', 'given', 'string', 'using', 'regex', '.']"
126,"def create_gpt_vector_store(client: ""OpenAI"", name: str, fild_ids: list[str]) -> Any:
    
    try:
        vector_store = client.vector_stores.create(name=name)
    except Exception as e:
        raise AttributeError(f""Failed to create vector store, please install the latest OpenAI python package: {e}"")

    # poll the status of the file batch for completion.
    batch = client.vector_stores.file_batches.create_and_poll(vector_store_id=vector_store.id, file_ids=fild_ids)

    if batch.status == ""in_progress"":
        time.sleep(1)
        logging.debug(f""file batch status: {batch.file_counts}"")
        batch = client.vector_stores.file_batches.poll(vector_store_id=vector_store.id, batch_id=batch.id)

    if batch.status == ""completed"":
        return vector_store

    raise ValueError(f""Failed to upload files to vector store {vector_store.id}:{batch.status}"")","['def', 'create_gpt_vector_store', '(', 'client', ':', '""', 'OpenAI', '""', ',', 'name', ':', 'str', ',', 'fild_ids', ':', 'list', '[', 'str', ']', ')', '-', '>', 'Any', ':', 'try', ':', 'vector_store', '=', 'client', '.', 'vector_stores', '.', 'create', '(', 'name', '=', 'name', ')', 'except', 'Exception', 'as', 'e', ':', 'raise', 'AttributeError', '(', 'f', '""', 'Failed to create vector store, please install the latest OpenAI python package: ', '{', 'e', '}', '""', ')', '# poll the status of the file batch for completion.', 'batch', '=', 'client', '.', 'vector_stores', '.', 'file_batches', '.', 'create_and_poll', '(', 'vector_store_id', '=', 'vector_store', '.', 'id', ',', 'file_ids', '=', 'fild_ids', ')', 'if', 'batch', '.', 'status', '==', '""', 'in_progress', '""', ':', 'time', '.', 'sleep', '(', '1', ')', 'logging', '.', 'debug', '(', 'f', '""', 'file batch status: ', '{', 'batch', '.', 'file_counts', '}', '""', ')', 'batch', '=', 'client', '.', 'vector_stores', '.', 'file_batches', '.', 'poll', '(', 'vector_store_id', '=', 'vector_store', '.', 'id', ',', 'batch_id', '=', 'batch', '.', 'id', ')', 'if', 'batch', '.', 'status', '==', '""', 'completed', '""', ':', 'return', 'vector_store', 'raise', 'ValueError', '(', 'f', '""', 'Failed to upload files to vector store ', '{', 'vector_store', '.', 'id', '}', ':', '{', 'batch', '.', 'status', '}', '""', ')']",Create and monitor a vector store for file processing using OpenAI client,"['Create', 'and', 'monitor', 'a', 'vector', 'store', 'for', 'file', 'processing', 'using', 'OpenAI', 'client']"
127,"def get_config_for_model(model_string: str) -> Dict[str, Any]:
    
    if model_string in MODEL_CONFIG_MAP:
        return MODEL_CONFIG_MAP[model_string]
    # If model not found, use default configuration based on provider
    provider, _ = parse_model_string(model_string)
    return {
        ""provider"": provider,
        ""default_params"": {""temperature"": 0.3},
    }","['def', 'get_config_for_model', '(', 'model_string', ':', 'str', ')', '-', '>', 'Dict', '[', 'str', ',', 'Any', ']', ':', 'if', 'model_string', 'in', 'MODEL_CONFIG_MAP', ':', 'return', 'MODEL_CONFIG_MAP', '[', 'model_string', ']', '# If model not found, use default configuration based on provider', 'provider', ',', '_', '=', 'parse_model_string', '(', 'model_string', ')', 'return', '{', '""', 'provider', '""', ':', 'provider', ',', '""', 'default_params', '""', ':', '{', '""', 'temperature', '""', ':', '0.3', '}', ',', '}']",Retrieve model configuration or default settings based on model identifier.,"['Retrieve', 'model', 'configuration', 'or', 'default', 'settings', 'based', 'on', 'model', 'identifier', '.']"
128,"def _validate_resolution_format(resolution: str):
    
    pattern = r""^\d+x\d+$""  # Matches a pattern of digits, ""x"", and digits
    matched_resolution = re.match(pattern, resolution)
    if matched_resolution is None:
        raise ValueError(f""Invalid resolution format: {resolution}"")","['def', '_validate_resolution_format', '(', 'resolution', ':', 'str', ')', ':', 'pattern', '=', 'r', '""', '^', '\\', 'd+x', '\\', 'd+$', '""', '# Matches a pattern of digits, ""x"", and digits', 'matched_resolution', '=', 're', '.', 'match', '(', 'pattern', ',', 'resolution', ')', 'if', 'matched_resolution', 'is', 'None', ':', 'raise', 'ValueError', '(', 'f', '""', 'Invalid resolution format: ', '{', 'resolution', '}', '""', ')']","Validate if a resolution string matches the ""widthxheight"" format.","['Validate', 'if', 'a', 'resolution', 'string', 'matches', 'the', '``', 'widthxheight', ""''"", 'format', '.']"
129,"def create_item(
        name: str, value: int, state: dict[str, Any] | None = None
    ) -> dict[str, Any]:
        
        if state:
            # state was read
            pass
        return {""name"": name, ""value"": value}","['def', 'create_item', '(', 'name', ':', 'str', ',', 'value', ':', 'int', ',', 'state', ':', 'dict', '[', 'str', ',', 'Any', ']', '|', 'None', '=', 'None', ')', '-', '>', 'dict', '[', 'str', ',', 'Any', ']', ':', 'if', 'state', ':', '# state was read', 'pass', 'return', '{', '""', 'name', '""', ':', 'name', ',', '""', 'value', '""', ':', 'value', '}']",Create a dictionary representing an item with optional state handling.,"['Create', 'a', 'dictionary', 'representing', 'an', 'item', 'with', 'optional', 'state', 'handling', '.']"
130,"def create_named_schedule_sampler(name, diffusion):
    
    if name == ""uniform"":
        return UniformSampler(diffusion)
    elif name == ""loss-second-moment"":
        return LossSecondMomentResampler(diffusion)
    else:
        raise NotImplementedError(f""unknown schedule sampler: {name}"")","['def', 'create_named_schedule_sampler', '(', 'name', ',', 'diffusion', ')', ':', 'if', 'name', '==', '""', 'uniform', '""', ':', 'return', 'UniformSampler', '(', 'diffusion', ')', 'elif', 'name', '==', '""', 'loss-second-moment', '""', ':', 'return', 'LossSecondMomentResampler', '(', 'diffusion', ')', 'else', ':', 'raise', 'NotImplementedError', '(', 'f', '""', 'unknown schedule sampler: ', '{', 'name', '}', '""', ')']",Selects and returns a schedule sampler based on the given name.,"['Selects', 'and', 'returns', 'a', 'schedule', 'sampler', 'based', 'on', 'the', 'given', 'name', '.']"
131,"def validate_window_size(configured: dict[str, Any], actual: dict[str, Any]) -> None:
	
	# Allow for small differences due to browser chrome, scrollbars, etc.
	width_diff = abs(configured['width'] - actual['width'])
	height_diff = abs(configured['height'] - actual['height'])

	# Tolerance of 5% or 20px, whichever is greater
	width_tolerance = max(configured['width'] * 0.05, 20)
	height_tolerance = max(configured['height'] * 0.05, 20)

	if width_diff > width_tolerance or height_diff > height_tolerance:
		print(f'âš ï¸  WARNING: Significant difference between expected and actual page size! Â±{width_diff}x{height_diff}px')
		raise Exception('Window size validation failed')
	else:
		print('âœ… Window size validation passed: actual size matches configured size within tolerance')","['def', 'validate_window_size', '(', 'configured', ':', 'dict', '[', 'str', ',', 'Any', ']', ',', 'actual', ':', 'dict', '[', 'str', ',', 'Any', ']', ')', '-', '>', 'None', ':', '# Allow for small differences due to browser chrome, scrollbars, etc.', 'width_diff', '=', 'abs', '(', 'configured', '[', ""'"", 'width', ""'"", ']', '-', 'actual', '[', ""'"", 'width', ""'"", ']', ')', 'height_diff', '=', 'abs', '(', 'configured', '[', ""'"", 'height', ""'"", ']', '-', 'actual', '[', ""'"", 'height', ""'"", ']', ')', '# Tolerance of 5% or 20px, whichever is greater', 'width_tolerance', '=', 'max', '(', 'configured', '[', ""'"", 'width', ""'"", ']', '*', '0.05', ',', '20', ')', 'height_tolerance', '=', 'max', '(', 'configured', '[', ""'"", 'height', ""'"", ']', '*', '0.05', ',', '20', ')', 'if', 'width_diff', '>', 'width_tolerance', 'or', 'height_diff', '>', 'height_tolerance', ':', 'print', '(', 'f', ""'"", 'âš ï¸  WARNING: Significant difference between expected and actual page size! Â±', '{', 'width_diff', '}', 'x', '{', 'height_diff', '}', 'px', ""'"", ')', 'raise', 'Exception', '(', ""'"", 'Window size validation failed', ""'"", ')', 'else', ':', 'print', '(', ""'"", 'âœ… Window size validation passed: actual size matches configured size within tolerance', ""'"", ')']",Validate if actual window size is within tolerance of configured dimensions.,"['Validate', 'if', 'actual', 'window', 'size', 'is', 'within', 'tolerance', 'of', 'configured', 'dimensions', '.']"
132,"def cleanup_memory(self):
        
        # Clear CUDA cache
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

            # Log memory usage if in verbose mode
            allocated = torch.cuda.memory_allocated() / (1024 ** 3)
            reserved = torch.cuda.memory_reserved() / (1024 ** 3)
            logger.info(f""GPU Memory: {allocated:.2f}GB allocated, {reserved:.2f}GB reserved"")

        # Collect Python garbage
        import gc
        gc.collect()","['def', 'cleanup_memory', '(', 'self', ')', ':', '# Clear CUDA cache', 'if', 'torch', '.', 'cuda', '.', 'is_available', '(', ')', ':', 'torch', '.', 'cuda', '.', 'empty_cache', '(', ')', '# Log memory usage if in verbose mode', 'allocated', '=', 'torch', '.', 'cuda', '.', 'memory_allocated', '(', ')', '/', '(', '1024', '*', '*', '3', ')', 'reserved', '=', 'torch', '.', 'cuda', '.', 'memory_reserved', '(', ')', '/', '(', '1024', '*', '*', '3', ')', 'logger', '.', 'info', '(', 'f', '""', 'GPU Memory: ', '{', 'allocated', ':', '.2f', '}', 'GB allocated, ', '{', 'reserved', ':', '.2f', '}', 'GB reserved', '""', ')', '# Collect Python garbage', 'import', 'gc', 'gc', '.', 'collect', '(', ')']",Release GPU resources and collect garbage to optimize memory usage.,"['Release', 'GPU', 'resources', 'and', 'collect', 'garbage', 'to', 'optimize', 'memory', 'usage', '.']"
133,"def copy_to_shm(src:str):
    
    shm_model_root = '/dev/shm/verl-cache/'
    src_abs = os.path.abspath(os.path.normpath(src))
    dest = os.path.join(shm_model_root, hashlib.md5(src_abs.encode('utf-8')).hexdigest())
    os.makedirs(dest, exist_ok=True)
    dest = os.path.join(dest, os.path.basename(src_abs))
    if os.path.exists(dest) and verify_copy(src, dest):
        # inform user and depends on him
        print(f""[WARNING]: The memory model path {dest} already exists. If it is not you want, please clear it and restart the task."")
    else:
        if os.path.isdir(src):
            shutil.copytree(src, dest, symlinks=False, dirs_exist_ok=True)
        else:
            shutil.copy2(src, dest)
    return dest","['def', 'copy_to_shm', '(', 'src', ':', 'str', ')', ':', 'shm_model_root', '=', ""'"", '/dev/shm/verl-cache/', ""'"", 'src_abs', '=', 'os', '.', 'path', '.', 'abspath', '(', 'os', '.', 'path', '.', 'normpath', '(', 'src', ')', ')', 'dest', '=', 'os', '.', 'path', '.', 'join', '(', 'shm_model_root', ',', 'hashlib', '.', 'md5', '(', 'src_abs', '.', 'encode', '(', ""'"", 'utf-8', ""'"", ')', ')', '.', 'hexdigest', '(', ')', ')', 'os', '.', 'makedirs', '(', 'dest', ',', 'exist_ok', '=', 'True', ')', 'dest', '=', 'os', '.', 'path', '.', 'join', '(', 'dest', ',', 'os', '.', 'path', '.', 'basename', '(', 'src_abs', ')', ')', 'if', 'os', '.', 'path', '.', 'exists', '(', 'dest', ')', 'and', 'verify_copy', '(', 'src', ',', 'dest', ')', ':', '# inform user and depends on him', 'print', '(', 'f', '""', '[WARNING]: The memory model path ', '{', 'dest', '}', ' already exists. If it is not you want, please clear it and restart the task.', '""', ')', 'else', ':', 'if', 'os', '.', 'path', '.', 'isdir', '(', 'src', ')', ':', 'shutil', '.', 'copytree', '(', 'src', ',', 'dest', ',', 'symlinks', '=', 'False', ',', 'dirs_exist_ok', '=', 'True', ')', 'else', ':', 'shutil', '.', 'copy2', '(', 'src', ',', 'dest', ')', 'return', 'dest']","Copy a source file or directory to a shared memory location, ensuring uniqueness.","['Copy', 'a', 'source', 'file', 'or', 'directory', 'to', 'a', 'shared', 'memory', 'location', ',', 'ensuring', 'uniqueness', '.']"
135,"def from_pretrained(self, repo_id: str) -> ""Vocos"":
        
        config_path = hf_hub_download(repo_id=repo_id, filename=""config.yaml"")
        model_path = hf_hub_download(repo_id=repo_id, filename=""pytorch_model.bin"")
        model = self.from_hparams(config_path)
        state_dict = torch.load(model_path, map_location=""cpu"")
        if isinstance(model.feature_extractor, EncodecFeatures):
            encodec_parameters = {
                ""feature_extractor.encodec."" + key: value
                for key, value in model.feature_extractor.encodec.state_dict().items()
            }
            state_dict.update(encodec_parameters)
        model.load_state_dict(state_dict)
        model.eval()
        return model","['def', 'from_pretrained', '(', 'self', ',', 'repo_id', ':', 'str', ')', '-', '>', '""', 'Vocos', '""', ':', 'config_path', '=', 'hf_hub_download', '(', 'repo_id', '=', 'repo_id', ',', 'filename', '=', '""', 'config.yaml', '""', ')', 'model_path', '=', 'hf_hub_download', '(', 'repo_id', '=', 'repo_id', ',', 'filename', '=', '""', 'pytorch_model.bin', '""', ')', 'model', '=', 'self', '.', 'from_hparams', '(', 'config_path', ')', 'state_dict', '=', 'torch', '.', 'load', '(', 'model_path', ',', 'map_location', '=', '""', 'cpu', '""', ')', 'if', 'isinstance', '(', 'model', '.', 'feature_extractor', ',', 'EncodecFeatures', ')', ':', 'encodec_parameters', '=', '{', '""', 'feature_extractor.encodec.', '""', '+', 'key', ':', 'value', 'for', 'key', ',', 'value', 'in', 'model', '.', 'feature_extractor', '.', 'encodec', '.', 'state_dict', '(', ')', '.', 'items', '(', ')', '}', 'state_dict', '.', 'update', '(', 'encodec_parameters', ')', 'model', '.', 'load_state_dict', '(', 'state_dict', ')', 'model', '.', 'eval', '(', ')', 'return', 'model']",Load and initialize a pre-trained Vocos model from a specified repository.,"['Load', 'and', 'initialize', 'a', 'pre-trained', 'Vocos', 'model', 'from', 'a', 'specified', 'repository', '.']"
136,"def add_legal_comment(cif: str) -> str:
  
  # fmt: off
  # pylint: disable=line-too-long
  comment = (
      '# By using this file you agree to the legally binding terms of use found at\n'
      f'# {_LICENSE_URL}.\n'
      '# To request access to the AlphaFold 3 model parameters, follow the process set\n'
      '# received directly from Google. Use is subject to terms of use available at\n'
  )
  # pylint: enable=line-too-long
  # fmt: on
  return f'{comment}\n{cif}'","['def', 'add_legal_comment', '(', 'cif', ':', 'str', ')', '-', '>', 'str', ':', '# fmt: off', '# pylint: disable=line-too-long', 'comment', '=', '(', ""'"", '# By using this file you agree to the legally binding terms of use found at', '\\n', ""'"", 'f', ""'"", '# ', '{', '_LICENSE_URL', '}', '.', '\\n', ""'"", ""'"", '# To request access to the AlphaFold 3 model parameters, follow the process set', '\\n', ""'"", ""'"", '# received directly from Google. Use is subject to terms of use available at', '\\n', ""'"", ')', '# pylint: enable=line-too-long', '# fmt: on', 'return', 'f', ""'"", '{', 'comment', '}', '\\n', '{', 'cif', '}', ""'""]",Appends legal disclaimer to a given text content.,"['Appends', 'legal', 'disclaimer', 'to', 'a', 'given', 'text', 'content', '.']"
137,"def _parse_datetime_field(self, dt_obj: Optional[Dict]) -> Optional[datetime]:
        
        if not dt_obj or not dt_obj.get(""dateTime""):
            return None
        try:
            dt_str = dt_obj[""dateTime""]
            if ""T"" in dt_str:
                # Handle timezone info
                if dt_str.endswith(""Z""):
                    dt_str = dt_str.replace(""Z"", ""+00:00"")
                elif ""+"" not in dt_str and ""-"" not in dt_str[-6:]:
                    # If no timezone info, assume UTC
                    dt_str += ""+00:00""
                return datetime.fromisoformat(dt_str)
        except (ValueError, TypeError) as e:
            logger.warning(f""Error parsing datetime: {str(e)}"")
        return None","['def', '_parse_datetime_field', '(', 'self', ',', 'dt_obj', ':', 'Optional', '[', 'Dict', ']', ')', '-', '>', 'Optional', '[', 'datetime', ']', ':', 'if', 'not', 'dt_obj', 'or', 'not', 'dt_obj', '.', 'get', '(', '""', 'dateTime', '""', ')', ':', 'return', 'None', 'try', ':', 'dt_str', '=', 'dt_obj', '[', '""', 'dateTime', '""', ']', 'if', '""', 'T', '""', 'in', 'dt_str', ':', '# Handle timezone info', 'if', 'dt_str', '.', 'endswith', '(', '""', 'Z', '""', ')', ':', 'dt_str', '=', 'dt_str', '.', 'replace', '(', '""', 'Z', '""', ',', '""', '+00:00', '""', ')', 'elif', '""', '+', '""', 'not', 'in', 'dt_str', 'and', '""', '-', '""', 'not', 'in', 'dt_str', '[', '-', '6', ':', ']', ':', '# If no timezone info, assume UTC', 'dt_str', '+', '=', '""', '+00:00', '""', 'return', 'datetime', '.', 'fromisoformat', '(', 'dt_str', ')', 'except', '(', 'ValueError', ',', 'TypeError', ')', 'as', 'e', ':', 'logger', '.', 'warning', '(', 'f', '""', 'Error parsing datetime: ', '{', 'str', '(', 'e', ')', '}', '""', ')', 'return', 'None']",Parse and convert a datetime string with timezone handling from a dictionary.,"['Parse', 'and', 'convert', 'a', 'datetime', 'string', 'with', 'timezone', 'handling', 'from', 'a', 'dictionary', '.']"
138,"def _get_auth_headers(self) -> Dict[str, str]:
        
        headers = {
            ""Content-Type"": ""application/json"",
            ""Accept"": ""application/json""
        }
        
        if ':' in self.credentials:
            # Basic authentication (username:password)
            encoded_credentials = base64.b64encode(self.credentials.encode()).decode()
            headers[""Authorization""] = f""Basic {encoded_credentials}""
        else:
            # API key authentication
            headers[""Authorization""] = f""Bearer {self.credentials}""
        
        return headers","['def', '_get_auth_headers', '(', 'self', ')', '-', '>', 'Dict', '[', 'str', ',', 'str', ']', ':', 'headers', '=', '{', '""', 'Content-Type', '""', ':', '""', 'application/json', '""', ',', '""', 'Accept', '""', ':', '""', 'application/json', '""', '}', 'if', ""'"", ':', ""'"", 'in', 'self', '.', 'credentials', ':', '# Basic authentication (username:password)', 'encoded_credentials', '=', 'base64', '.', 'b64encode', '(', 'self', '.', 'credentials', '.', 'encode', '(', ')', ')', '.', 'decode', '(', ')', 'headers', '[', '""', 'Authorization', '""', ']', '=', 'f', '""', 'Basic ', '{', 'encoded_credentials', '}', '""', 'else', ':', '# API key authentication', 'headers', '[', '""', 'Authorization', '""', ']', '=', 'f', '""', 'Bearer ', '{', 'self', '.', 'credentials', '}', '""', 'return', 'headers']",Generate HTTP headers for authentication using credentials.,"['Generate', 'HTTP', 'headers', 'for', 'authentication', 'using', 'credentials', '.']"
143,"def close_idle_connections(cls) -> None:
        
        now = datetime.now()
        idle_threshold = now - timedelta(minutes=cls._idle_timeout)

        idle_connections = [
            conn_id
            for conn_id, info in cls._connections.items()
            if info.last_used < idle_threshold
        ]

        for conn_id in idle_connections:
            logger.info(f'Closing idle DocumentDB connection {conn_id}')
            cls._connections[conn_id].client.close()
            del cls._connections[conn_id]","['def', 'close_idle_connections', '(', 'cls', ')', '-', '>', 'None', ':', 'now', '=', 'datetime', '.', 'now', '(', ')', 'idle_threshold', '=', 'now', '-', 'timedelta', '(', 'minutes', '=', 'cls', '.', '_idle_timeout', ')', 'idle_connections', '=', '[', 'conn_id', 'for', 'conn_id', ',', 'info', 'in', 'cls', '.', '_connections', '.', 'items', '(', ')', 'if', 'info', '.', 'last_used', '<', 'idle_threshold', ']', 'for', 'conn_id', 'in', 'idle_connections', ':', 'logger', '.', 'info', '(', 'f', ""'"", 'Closing idle DocumentDB connection ', '{', 'conn_id', '}', ""'"", ')', 'cls', '.', '_connections', '[', 'conn_id', ']', '.', 'client', '.', 'close', '(', ')', 'del', 'cls', '.', '_connections', '[', 'conn_id', ']']",Close and remove idle database connections exceeding timeout threshold.,"['Close', 'and', 'remove', 'idle', 'database', 'connections', 'exceeding', 'timeout', 'threshold', '.']"
144,"def build_agent_tree(parent_tree, agent_obj):
            
            parent_tree.add(create_tools_section(agent_obj.tools))

            if agent_obj.managed_agents:
                agents_branch = parent_tree.add(""ðŸ¤– [italic #1E90FF]Managed agents:"")
                for name, managed_agent in agent_obj.managed_agents.items():
                    agent_tree = agents_branch.add(get_agent_headline(managed_agent, name))
                    if managed_agent.__class__.__name__ == ""CodeAgent"":
                        agent_tree.add(
                            f""âœ… [italic #1E90FF]Authorized imports:[/italic #1E90FF] {managed_agent.additional_authorized_imports}""
                        )
                    agent_tree.add(f""ðŸ“ [italic #1E90FF]Description:[/italic #1E90FF] {managed_agent.description}"")
                    build_agent_tree(agent_tree, managed_agent)","['def', 'build_agent_tree', '(', 'parent_tree', ',', 'agent_obj', ')', ':', 'parent_tree', '.', 'add', '(', 'create_tools_section', '(', 'agent_obj', '.', 'tools', ')', ')', 'if', 'agent_obj', '.', 'managed_agents', ':', 'agents_branch', '=', 'parent_tree', '.', 'add', '(', '""', 'ðŸ¤– [italic #1E90FF]Managed agents:', '""', ')', 'for', 'name', ',', 'managed_agent', 'in', 'agent_obj', '.', 'managed_agents', '.', 'items', '(', ')', ':', 'agent_tree', '=', 'agents_branch', '.', 'add', '(', 'get_agent_headline', '(', 'managed_agent', ',', 'name', ')', ')', 'if', 'managed_agent', '.', '__class__', '.', '__name__', '==', '""', 'CodeAgent', '""', ':', 'agent_tree', '.', 'add', '(', 'f', '""', 'âœ… [italic #1E90FF]Authorized imports:[/italic #1E90FF] ', '{', 'managed_agent', '.', 'additional_authorized_imports', '}', '""', ')', 'agent_tree', '.', 'add', '(', 'f', '""', 'ðŸ“ [italic #1E90FF]Description:[/italic #1E90FF] ', '{', 'managed_agent', '.', 'description', '}', '""', ')', 'build_agent_tree', '(', 'agent_tree', ',', 'managed_agent', ')']",Constructs a hierarchical tree structure for agents and their managed sub-agents.,"['Constructs', 'a', 'hierarchical', 'tree', 'structure', 'for', 'agents', 'and', 'their', 'managed', 'sub-agents', '.']"
146,"def _read_file_safe(file_path: str) -> Optional[str]:
    
    if not os.path.exists(file_path):
        return None
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            return f.read()
    except Exception as e:
        logger.error(f""Error reading file {file_path}: {e}"")
        return None","['def', '_read_file_safe', '(', 'file_path', ':', 'str', ')', '-', '>', 'Optional', '[', 'str', ']', ':', 'if', 'not', 'os', '.', 'path', '.', 'exists', '(', 'file_path', ')', ':', 'return', 'None', 'try', ':', 'with', 'open', '(', 'file_path', ',', ""'"", 'r', ""'"", ',', 'encoding', '=', ""'"", 'utf-8', ""'"", ')', 'as', 'f', ':', 'return', 'f', '.', 'read', '(', ')', 'except', 'Exception', 'as', 'e', ':', 'logger', '.', 'error', '(', 'f', '""', 'Error reading file ', '{', 'file_path', '}', ': ', '{', 'e', '}', '""', ')', 'return', 'None']","Safely read file content, handling errors and non-existent paths.","['Safely', 'read', 'file', 'content', ',', 'handling', 'errors', 'and', 'non-existent', 'paths', '.']"
147,"def __init__(
        self,
        exit_stack: contextlib.AsyncExitStack | None = None,
        component_name_hook: _ComponentNameHook | None = None,
    ) -> None:
        

        self._tools = {}
        self._resources = {}
        self._prompts = {}

        self._sessions = {}
        self._tool_to_session = {}
        if exit_stack is None:
            self._exit_stack = contextlib.AsyncExitStack()
            self._owns_exit_stack = True
        else:
            self._exit_stack = exit_stack
            self._owns_exit_stack = False
        self._session_exit_stacks = {}
        self._component_name_hook = component_name_hook","['def', '__init__', '(', 'self', ',', 'exit_stack', ':', 'contextlib', '.', 'AsyncExitStack', '|', 'None', '=', 'None', ',', 'component_name_hook', ':', '_ComponentNameHook', '|', 'None', '=', 'None', ',', ')', '-', '>', 'None', ':', 'self', '.', '_tools', '=', '{', '}', 'self', '.', '_resources', '=', '{', '}', 'self', '.', '_prompts', '=', '{', '}', 'self', '.', '_sessions', '=', '{', '}', 'self', '.', '_tool_to_session', '=', '{', '}', 'if', 'exit_stack', 'is', 'None', ':', 'self', '.', '_exit_stack', '=', 'contextlib', '.', 'AsyncExitStack', '(', ')', 'self', '.', '_owns_exit_stack', '=', 'True', 'else', ':', 'self', '.', '_exit_stack', '=', 'exit_stack', 'self', '.', '_owns_exit_stack', '=', 'False', 'self', '.', '_session_exit_stacks', '=', '{', '}', 'self', '.', '_component_name_hook', '=', 'component_name_hook']",Initialize resource management with optional exit stack and component hook,"['Initialize', 'resource', 'management', 'with', 'optional', 'exit', 'stack', 'and', 'component', 'hook']"
148,"def _fix_chrome_permissions(self, user_data_dir):
        
        try:
            if sys.platform == 'darwin':  # macOS
                import subprocess
                import pwd
                
                # Get current user
                current_user = pwd.getpwuid(os.getuid()).pw_name
                
                # Fix permissions for Chrome directory
                chrome_dir = os.path.expanduser('~/Library/Application Support/Google/Chrome')
                if os.path.exists(chrome_dir):
                    subprocess.run(['chmod', '-R', 'u+rwX', chrome_dir])
                    subprocess.run(['chown', '-R', f'{current_user}:staff', chrome_dir])
                    print(f""{Fore.GREEN}{EMOJI['SUCCESS']} {self.translator.get('oauth.chrome_permissions_fixed') if self.translator else 'Fixed Chrome user data directory permissions'}{Style.RESET_ALL}"")
        except Exception as e:
            print(f""{Fore.YELLOW}{EMOJI['WARNING']} {self.translator.get('oauth.chrome_permissions_fix_failed', error=str(e)) if self.translator else f'Failed to fix Chrome permissions: {str(e)}'}{Style.RESET_ALL}"")","['def', '_fix_chrome_permissions', '(', 'self', ',', 'user_data_dir', ')', ':', 'try', ':', 'if', 'sys', '.', 'platform', '==', ""'"", 'darwin', ""'"", ':', '# macOS', 'import', 'subprocess', 'import', 'pwd', '# Get current user', 'current_user', '=', 'pwd', '.', 'getpwuid', '(', 'os', '.', 'getuid', '(', ')', ')', '.', 'pw_name', '# Fix permissions for Chrome directory', 'chrome_dir', '=', 'os', '.', 'path', '.', 'expanduser', '(', ""'"", '~/Library/Application Support/Google/Chrome', ""'"", ')', 'if', 'os', '.', 'path', '.', 'exists', '(', 'chrome_dir', ')', ':', 'subprocess', '.', 'run', '(', '[', ""'"", 'chmod', ""'"", ',', ""'"", '-R', ""'"", ',', ""'"", 'u+rwX', ""'"", ',', 'chrome_dir', ']', ')', 'subprocess', '.', 'run', '(', '[', ""'"", 'chown', ""'"", ',', ""'"", '-R', ""'"", ',', 'f', ""'"", '{', 'current_user', '}', ':staff', ""'"", ',', 'chrome_dir', ']', ')', 'print', '(', 'f', '""', '{', 'Fore', '.', 'GREEN', '}', '{', 'EMOJI', '[', ""'"", 'SUCCESS', ""'"", ']', '}', '{', 'self', '.', 'translator', '.', 'get', '(', ""'"", 'oauth.chrome_permissions_fixed', ""'"", ')', 'if', 'self', '.', 'translator', 'else', ""'"", 'Fixed Chrome user data directory permissions', ""'"", '}', '{', 'Style', '.', 'RESET_ALL', '}', '""', ')', 'except', 'Exception', 'as', 'e', ':', 'print', '(', 'f', '""', '{', 'Fore', '.', 'YELLOW', '}', '{', 'EMOJI', '[', ""'"", 'WARNING', ""'"", ']', '}', '{', 'self', '.', 'translator', '.', 'get', '(', ""'"", 'oauth.chrome_permissions_fix_failed', ""'"", ',', 'error', '=', 'str', '(', 'e', ')', ')', 'if', 'self', '.', 'translator', 'else', 'f', ""'"", 'Failed to fix Chrome permissions: ', '{', 'str', '(', 'e', ')', '}', ""'"", '}', '{', 'Style', '.', 'RESET_ALL', '}', '""', ')']","Adjusts macOS Chrome directory permissions for the current user, handling exceptions.","['Adjusts', 'macOS', 'Chrome', 'directory', 'permissions', 'for', 'the', 'current', 'user', ',', 'handling', 'exceptions', '.']"
149,"def init_progressive(self, noise_scale=0.01):
        
        self._copy_non_transformer_params()

        # copy pretrained layers
        for i in range(self.pretrained_layers):
            for key in self.pretrained_state:
                if f""blocks.{i}."" in key:
                    new_key = key.replace(f""blocks.{i}."", f""blocks.{i}."")
                    self.target_state[new_key] = self.pretrained_state[key]

        # progressive init new layers
        for i in range(self.pretrained_layers, self.target_layers):
            prev_layer = i - 1
            for key in self.target_state:
                if f""blocks.{i}."" in key:
                    prev_key = key.replace(f""blocks.{i}."", f""blocks.{prev_layer}."")
                    # add random noise
                    noise = torch.randn_like(self.target_state[prev_key]) * noise_scale
                    self.target_state[key] = self.target_state[prev_key] + noise

        self.target_model.load_state_dict(self.target_state)
        return self.target_model","['def', 'init_progressive', '(', 'self', ',', 'noise_scale', '=', '0.01', ')', ':', 'self', '.', '_copy_non_transformer_params', '(', ')', '# copy pretrained layers', 'for', 'i', 'in', 'range', '(', 'self', '.', 'pretrained_layers', ')', ':', 'for', 'key', 'in', 'self', '.', 'pretrained_state', ':', 'if', 'f', '""', 'blocks.', '{', 'i', '}', '.', '""', 'in', 'key', ':', 'new_key', '=', 'key', '.', 'replace', '(', 'f', '""', 'blocks.', '{', 'i', '}', '.', '""', ',', 'f', '""', 'blocks.', '{', 'i', '}', '.', '""', ')', 'self', '.', 'target_state', '[', 'new_key', ']', '=', 'self', '.', 'pretrained_state', '[', 'key', ']', '# progressive init new layers', 'for', 'i', 'in', 'range', '(', 'self', '.', 'pretrained_layers', ',', 'self', '.', 'target_layers', ')', ':', 'prev_layer', '=', 'i', '-', '1', 'for', 'key', 'in', 'self', '.', 'target_state', ':', 'if', 'f', '""', 'blocks.', '{', 'i', '}', '.', '""', 'in', 'key', ':', 'prev_key', '=', 'key', '.', 'replace', '(', 'f', '""', 'blocks.', '{', 'i', '}', '.', '""', ',', 'f', '""', 'blocks.', '{', 'prev_layer', '}', '.', '""', ')', '# add random noise', 'noise', '=', 'torch', '.', 'randn_like', '(', 'self', '.', 'target_state', '[', 'prev_key', ']', ')', '*', 'noise_scale', 'self', '.', 'target_state', '[', 'key', ']', '=', 'self', '.', 'target_state', '[', 'prev_key', ']', '+', 'noise', 'self', '.', 'target_model', '.', 'load_state_dict', '(', 'self', '.', 'target_state', ')', 'return', 'self', '.', 'target_model']",Progressively initialize model layers with noise based on pretrained state,"['Progressively', 'initialize', 'model', 'layers', 'with', 'noise', 'based', 'on', 'pretrained', 'state']"
152,"def check_and_install_dependencies():
    
    required_packages = [
        ""uvicorn"",
        ""tiktoken"",
        ""fastapi"",
        # Add other required packages here
    ]

    for package in required_packages:
        if not pm.is_installed(package):
            print(f""Installing {package}..."")
            pm.install(package)
            print(f""{package} installed successfully"")","['def', 'check_and_install_dependencies', '(', ')', ':', 'required_packages', '=', '[', '""', 'uvicorn', '""', ',', '""', 'tiktoken', '""', ',', '""', 'fastapi', '""', ',', '# Add other required packages here', ']', 'for', 'package', 'in', 'required_packages', ':', 'if', 'not', 'pm', '.', 'is_installed', '(', 'package', ')', ':', 'print', '(', 'f', '""', 'Installing ', '{', 'package', '}', '...', '""', ')', 'pm', '.', 'install', '(', 'package', ')', 'print', '(', 'f', '""', '{', 'package', '}', ' installed successfully', '""', ')']","Ensure required packages are installed, installing if necessary.","['Ensure', 'required', 'packages', 'are', 'installed', ',', 'installing', 'if', 'necessary', '.']"
153,"def _analyze_contrarian_sentiment(news):
    

    max_score = 1
    score = 0
    details: list[str] = []

    if not news:
        details.append(""No recent news"")
        return {""score"": score, ""max_score"": max_score, ""details"": ""; "".join(details)}

    # Count negative sentiment articles
    sentiment_negative_count = sum(
        1 for n in news if n.sentiment and n.sentiment.lower() in [""negative"", ""bearish""]
    )
    
    if sentiment_negative_count >= 5:
        score += 1  # The more hated, the better (assuming fundamentals hold up)
        details.append(f""{sentiment_negative_count} negative headlines (contrarian opportunity)"")
    else:
        details.append(""Limited negative press"")

    return {""score"": score, ""max_score"": max_score, ""details"": ""; "".join(details)}","['def', '_analyze_contrarian_sentiment', '(', 'news', ')', ':', 'max_score', '=', '1', 'score', '=', '0', 'details', ':', 'list', '[', 'str', ']', '=', '[', ']', 'if', 'not', 'news', ':', 'details', '.', 'append', '(', '""', 'No recent news', '""', ')', 'return', '{', '""', 'score', '""', ':', 'score', ',', '""', 'max_score', '""', ':', 'max_score', ',', '""', 'details', '""', ':', '""', '; ', '""', '.', 'join', '(', 'details', ')', '}', '# Count negative sentiment articles', 'sentiment_negative_count', '=', 'sum', '(', '1', 'for', 'n', 'in', 'news', 'if', 'n', '.', 'sentiment', 'and', 'n', '.', 'sentiment', '.', 'lower', '(', ')', 'in', '[', '""', 'negative', '""', ',', '""', 'bearish', '""', ']', ')', 'if', 'sentiment_negative_count', '>', '=', '5', ':', 'score', '+', '=', '1', '# The more hated, the better (assuming fundamentals hold up)', 'details', '.', 'append', '(', 'f', '""', '{', 'sentiment_negative_count', '}', ' negative headlines (contrarian opportunity)', '""', ')', 'else', ':', 'details', '.', 'append', '(', '""', 'Limited negative press', '""', ')', 'return', '{', '""', 'score', '""', ':', 'score', ',', '""', 'max_score', '""', ':', 'max_score', ',', '""', 'details', '""', ':', '""', '; ', '""', '.', 'join', '(', 'details', ')', '}']",Evaluate contrarian sentiment in news articles to identify potential investment opportunities.,"['Evaluate', 'contrarian', 'sentiment', 'in', 'news', 'articles', 'to', 'identify', 'potential', 'investment', 'opportunities', '.']"
154,"def opt_repetitions(up_to_n, prefix_with_sep=False):
        

        content = (
            f""{separator_rule} {item_rule}""
            if prefix_with_sep and separator_rule
            else item_rule
        )
        if up_to_n == 0:
            return """"
        elif up_to_n == 1:
            return f""({content})?""
        elif separator_rule and not prefix_with_sep:
            return f""({content} {opt_repetitions(up_to_n - 1, prefix_with_sep=True)})?""
        else:
            return (f""({content} "" * up_to_n).rstrip() + ("")?"" * up_to_n)","['def', 'opt_repetitions', '(', 'up_to_n', ',', 'prefix_with_sep', '=', 'False', ')', ':', 'content', '=', '(', 'f', '""', '{', 'separator_rule', '}', '{', 'item_rule', '}', '""', 'if', 'prefix_with_sep', 'and', 'separator_rule', 'else', 'item_rule', ')', 'if', 'up_to_n', '==', '0', ':', 'return', '""', '""', 'elif', 'up_to_n', '==', '1', ':', 'return', 'f', '""', '(', '{', 'content', '}', ')?', '""', 'elif', 'separator_rule', 'and', 'not', 'prefix_with_sep', ':', 'return', 'f', '""', '(', '{', 'content', '}', '{', 'opt_repetitions', '(', 'up_to_n', '-', '1', ',', 'prefix_with_sep', '=', 'True', ')', '}', ')?', '""', 'else', ':', 'return', '(', 'f', '""', '(', '{', 'content', '}', '""', '*', 'up_to_n', ')', '.', 'rstrip', '(', ')', '+', '(', '""', ')?', '""', '*', 'up_to_n', ')']",Generate optional repetition patterns with customizable separators.,"['Generate', 'optional', 'repetition', 'patterns', 'with', 'customizable', 'separators', '.']"
155,"def comment_magic_commands(script_content: str) -> str:
    
    lines = script_content.splitlines()
    commented_lines = []
    for line in lines:
        # Check for magic commands, shell commands, or direct execution commands
        if re.match(r'^\s*(!|%|pip|apt-get|curl|conda)', line.strip()):
            commented_lines.append(f""# {line}"")  # Comment the line
        else:
            commented_lines.append(line)  # Keep the line unchanged
    return ""\n"".join(commented_lines)","['def', 'comment_magic_commands', '(', 'script_content', ':', 'str', ')', '-', '>', 'str', ':', 'lines', '=', 'script_content', '.', 'splitlines', '(', ')', 'commented_lines', '=', '[', ']', 'for', 'line', 'in', 'lines', ':', '# Check for magic commands, shell commands, or direct execution commands', 'if', 're', '.', 'match', '(', 'r', ""'"", '^', '\\', 's*(!|', '%', '|pip|apt-get|curl|conda)', ""'"", ',', 'line', '.', 'strip', '(', ')', ')', ':', 'commented_lines', '.', 'append', '(', 'f', '""', '# ', '{', 'line', '}', '""', ')', '# Comment the line', 'else', ':', 'commented_lines', '.', 'append', '(', 'line', ')', '# Keep the line unchanged', 'return', '""', '\\n', '""', '.', 'join', '(', 'commented_lines', ')']",Transform script lines by commenting out shell or magic commands.,"['Transform', 'script', 'lines', 'by', 'commenting', 'out', 'shell', 'or', 'magic', 'commands', '.']"
156,"def to_native_types(obj: Any, resolve: bool = True, throw_on_missing: bool = True, enum_to_str: bool = True) -> Any:
    

    # convert dataclass to structured config
    if hasattr(obj, ""to_dict""):
        # huggingface objects have a to_dict method, we prefer that
        obj = obj.to_dict()
    elif is_dataclass(obj):
        # we go through structured config instead and hope for the best
        obj = om.to_container(obj)

    if isinstance(obj, DictConfig) or isinstance(obj, ListConfig):
        obj = om.to_container(obj, resolve=resolve, throw_on_missing=throw_on_missing, enum_to_str=enum_to_str)

    if isinstance(obj, dict):
        return {k: to_native_types(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [to_native_types(v) for v in obj]
    else:
        return obj","['def', 'to_native_types', '(', 'obj', ':', 'Any', ',', 'resolve', ':', 'bool', '=', 'True', ',', 'throw_on_missing', ':', 'bool', '=', 'True', ',', 'enum_to_str', ':', 'bool', '=', 'True', ')', '-', '>', 'Any', ':', '# convert dataclass to structured config', 'if', 'hasattr', '(', 'obj', ',', '""', 'to_dict', '""', ')', ':', '# huggingface objects have a to_dict method, we prefer that', 'obj', '=', 'obj', '.', 'to_dict', '(', ')', 'elif', 'is_dataclass', '(', 'obj', ')', ':', '# we go through structured config instead and hope for the best', 'obj', '=', 'om', '.', 'to_container', '(', 'obj', ')', 'if', 'isinstance', '(', 'obj', ',', 'DictConfig', ')', 'or', 'isinstance', '(', 'obj', ',', 'ListConfig', ')', ':', 'obj', '=', 'om', '.', 'to_container', '(', 'obj', ',', 'resolve', '=', 'resolve', ',', 'throw_on_missing', '=', 'throw_on_missing', ',', 'enum_to_str', '=', 'enum_to_str', ')', 'if', 'isinstance', '(', 'obj', ',', 'dict', ')', ':', 'return', '{', 'k', ':', 'to_native_types', '(', 'v', ')', 'for', 'k', ',', 'v', 'in', 'obj', '.', 'items', '(', ')', '}', 'elif', 'isinstance', '(', 'obj', ',', 'list', ')', ':', 'return', '[', 'to_native_types', '(', 'v', ')', 'for', 'v', 'in', 'obj', ']', 'else', ':', 'return', 'obj']","Convert complex objects to native Python types, handling dataclasses and configurations.","['Convert', 'complex', 'objects', 'to', 'native', 'Python', 'types', ',', 'handling', 'dataclasses', 'and', 'configurations', '.']"
158,"def detect_mutating_keywords(sql: str) -> list[str]:
    
    matched = []

    if DDL_REGEX.search(sql):
        matched.append('DDL')

    if PERMISSION_REGEX.search(sql):
        matched.append('PERMISSION')

    if SYSTEM_REGEX.search(sql):
        matched.append('SYSTEM')

    # Match individual keywords from MUTATING_KEYWORDS
    keyword_matches = MUTATING_PATTERN.findall(sql)
    if keyword_matches:
        # Deduplicate and normalize casing
        matched.extend(sorted({k.upper() for k in keyword_matches}))

    return matched","['def', 'detect_mutating_keywords', '(', 'sql', ':', 'str', ')', '-', '>', 'list', '[', 'str', ']', ':', 'matched', '=', '[', ']', 'if', 'DDL_REGEX', '.', 'search', '(', 'sql', ')', ':', 'matched', '.', 'append', '(', ""'"", 'DDL', ""'"", ')', 'if', 'PERMISSION_REGEX', '.', 'search', '(', 'sql', ')', ':', 'matched', '.', 'append', '(', ""'"", 'PERMISSION', ""'"", ')', 'if', 'SYSTEM_REGEX', '.', 'search', '(', 'sql', ')', ':', 'matched', '.', 'append', '(', ""'"", 'SYSTEM', ""'"", ')', '# Match individual keywords from MUTATING_KEYWORDS', 'keyword_matches', '=', 'MUTATING_PATTERN', '.', 'findall', '(', 'sql', ')', 'if', 'keyword_matches', ':', '# Deduplicate and normalize casing', 'matched', '.', 'extend', '(', 'sorted', '(', '{', 'k', '.', 'upper', '(', ')', 'for', 'k', 'in', 'keyword_matches', '}', ')', ')', 'return', 'matched']",Detects and categorizes SQL mutating keywords into predefined groups.,"['Detects', 'and', 'categorizes', 'SQL', 'mutating', 'keywords', 'into', 'predefined', 'groups', '.']"
159,"def _get_trace_filename(self) -> str:
        
        path_pattern = self.path_settings.path_pattern
        unique_id_type = self.path_settings.unique_id

        if unique_id_type == ""session_id"":
            unique_id = self.session_id
        elif unique_id_type == ""timestamp"":
            now = datetime.now()
            time_format = self.path_settings.timestamp_format
            unique_id = now.strftime(time_format)
        else:
            raise ValueError(
                f""Invalid unique_id type: {unique_id_type}. Expected 'session_id' or 'timestamp'.""
            )

        return path_pattern.replace(""{unique_id}"", unique_id)","['def', '_get_trace_filename', '(', 'self', ')', '-', '>', 'str', ':', 'path_pattern', '=', 'self', '.', 'path_settings', '.', 'path_pattern', 'unique_id_type', '=', 'self', '.', 'path_settings', '.', 'unique_id', 'if', 'unique_id_type', '==', '""', 'session_id', '""', ':', 'unique_id', '=', 'self', '.', 'session_id', 'elif', 'unique_id_type', '==', '""', 'timestamp', '""', ':', 'now', '=', 'datetime', '.', 'now', '(', ')', 'time_format', '=', 'self', '.', 'path_settings', '.', 'timestamp_format', 'unique_id', '=', 'now', '.', 'strftime', '(', 'time_format', ')', 'else', ':', 'raise', 'ValueError', '(', 'f', '""', 'Invalid unique_id type: ', '{', 'unique_id_type', '}', '. Expected ', ""'"", 'session_id', ""'"", ' or ', ""'"", 'timestamp', ""'"", '.', '""', ')', 'return', 'path_pattern', '.', 'replace', '(', '""', '{unique_id}', '""', ',', 'unique_id', ')']",Generate a trace filename using a pattern with a session ID or timestamp,"['Generate', 'a', 'trace', 'filename', 'using', 'a', 'pattern', 'with', 'a', 'session', 'ID', 'or', 'timestamp']"
164,"def print_curl(self, method: str, path: str, data: Optional[Dict[str, Any]] = None) -> None:
        
        curl_cmd = f
        
        if data:
            curl_cmd += f"" \\\n  -H 'Content-Type: application/json' \\\n  -d '{json.dumps(data)}'""
        
        print(""\nEquivalent curl command:"")
        print(curl_cmd)
        print()","['def', 'print_curl', '(', 'self', ',', 'method', ':', 'str', ',', 'path', ':', 'str', ',', 'data', ':', 'Optional', '[', 'Dict', '[', 'str', ',', 'Any', ']', ']', '=', 'None', ')', '-', '>', 'None', ':', 'curl_cmd', '=', 'f', 'if', 'data', ':', 'curl_cmd', '+', '=', 'f', '""', '\\\\', '\\n', '  -H ', ""'"", 'Content-Type: application/json', ""'"", '\\\\', '\\n', '  -d ', ""'"", '{', 'json', '.', 'dumps', '(', 'data', ')', '}', ""'"", '""', 'print', '(', '""', '\\n', 'Equivalent curl command:', '""', ')', 'print', '(', 'curl_cmd', ')', 'print', '(', ')']",Generate and display a curl command for HTTP requests with optional JSON data.,"['Generate', 'and', 'display', 'a', 'curl', 'command', 'for', 'HTTP', 'requests', 'with', 'optional', 'JSON', 'data', '.']"
165,"def _log_available_capabilities(self):
        
        capabilities = []
        if self.mistral_client:
            capabilities.append(""Mistral OCR"")
        if self.exiftool_available:
            capabilities.append(""Exiftool metadata extraction"")
        if self.openai_client:
            capabilities.append(""OpenAI image description"")

        if capabilities:
            logger.info(f""Image converter initialized with: {', '.join(capabilities)}"")
        else:
            logger.warning(""Image converter initialized without any processing capabilities"")","['def', '_log_available_capabilities', '(', 'self', ')', ':', 'capabilities', '=', '[', ']', 'if', 'self', '.', 'mistral_client', ':', 'capabilities', '.', 'append', '(', '""', 'Mistral OCR', '""', ')', 'if', 'self', '.', 'exiftool_available', ':', 'capabilities', '.', 'append', '(', '""', 'Exiftool metadata extraction', '""', ')', 'if', 'self', '.', 'openai_client', ':', 'capabilities', '.', 'append', '(', '""', 'OpenAI image description', '""', ')', 'if', 'capabilities', ':', 'logger', '.', 'info', '(', 'f', '""', 'Image converter initialized with: ', '{', ""'"", ', ', ""'"", '.', 'join', '(', 'capabilities', ')', '}', '""', ')', 'else', ':', 'logger', '.', 'warning', '(', '""', 'Image converter initialized without any processing capabilities', '""', ')']",Log initialized image processing capabilities based on available tools,"['Log', 'initialized', 'image', 'processing', 'capabilities', 'based', 'on', 'available', 'tools']"
168,"def _run(self, **kwargs) -> str:
        
        if not self.server_manager.active_server:
            return (
                ""No MCP server is currently active. ""
                ""Use connect_to_mcp_server to connect to a server.""
            )
        return f""Currently active MCP server: {self.server_manager.active_server}""","['def', '_run', '(', 'self', ',', '*', '*', 'kwargs', ')', '-', '>', 'str', ':', 'if', 'not', 'self', '.', 'server_manager', '.', 'active_server', ':', 'return', '(', '""', 'No MCP server is currently active. ', '""', '""', 'Use connect_to_mcp_server to connect to a server.', '""', ')', 'return', 'f', '""', 'Currently active MCP server: ', '{', 'self', '.', 'server_manager', '.', 'active_server', '}', '""']",Check if an MCP server is active and return its status message.,"['Check', 'if', 'an', 'MCP', 'server', 'is', 'active', 'and', 'return', 'its', 'status', 'message', '.']"
169,"def load_images_from_directory(image_dir, valid_extensions=('.jpg', '.jpeg', '.png', '.webp')):
    
    images_dict = {}

    if not os.path.exists(image_dir):
        raise ValueError(f""Directory {image_dir} does not exist"")

    for filename in os.listdir(image_dir):
        if filename.lower().endswith(valid_extensions):
            image_path = os.path.join(image_dir, filename)
            try:
                image = Image.open(image_path).convert(""RGB"")
                images_dict[image_path] = image
            except Exception as e:
                print(f""Failed to load image {filename}: {str(e)}"")

    if not images_dict:
        raise ValueError(f""No valid image files found in {image_dir}"")

    return images_dict","['def', 'load_images_from_directory', '(', 'image_dir', ',', 'valid_extensions', '=', '(', ""'"", '.jpg', ""'"", ',', ""'"", '.jpeg', ""'"", ',', ""'"", '.png', ""'"", ',', ""'"", '.webp', ""'"", ')', ')', ':', 'images_dict', '=', '{', '}', 'if', 'not', 'os', '.', 'path', '.', 'exists', '(', 'image_dir', ')', ':', 'raise', 'ValueError', '(', 'f', '""', 'Directory ', '{', 'image_dir', '}', ' does not exist', '""', ')', 'for', 'filename', 'in', 'os', '.', 'listdir', '(', 'image_dir', ')', ':', 'if', 'filename', '.', 'lower', '(', ')', '.', 'endswith', '(', 'valid_extensions', ')', ':', 'image_path', '=', 'os', '.', 'path', '.', 'join', '(', 'image_dir', ',', 'filename', ')', 'try', ':', 'image', '=', 'Image', '.', 'open', '(', 'image_path', ')', '.', 'convert', '(', '""', 'RGB', '""', ')', 'images_dict', '[', 'image_path', ']', '=', 'image', 'except', 'Exception', 'as', 'e', ':', 'print', '(', 'f', '""', 'Failed to load image ', '{', 'filename', '}', ': ', '{', 'str', '(', 'e', ')', '}', '""', ')', 'if', 'not', 'images_dict', ':', 'raise', 'ValueError', '(', 'f', '""', 'No valid image files found in ', '{', 'image_dir', '}', '""', ')', 'return', 'images_dict']",Load and return valid images from a specified directory as a dictionary.,"['Load', 'and', 'return', 'valid', 'images', 'from', 'a', 'specified', 'directory', 'as', 'a', 'dictionary', '.']"
171,"def _update_component_states(self, states: dict[str, Any]):
        
        with self._lock:
            logger.info(""[STATE] Updating states"")
            for component_id, new_value in states.items():
                old_value = self._component_states.get(component_id)

                cleaned_new_value = clean_nan_values(new_value)
                cleaned_old_value = clean_nan_values(old_value)

                if cleaned_old_value != cleaned_new_value:
                    self._component_states[component_id] = cleaned_new_value
                    logger.info(f""[STATE] State changed for {component_id=}"")
                    if logger.isEnabledFor(logging.DEBUG):
                        logger.debug(f""[STATE]  - {cleaned_old_value=}\n  - {cleaned_new_value=}"")","['def', '_update_component_states', '(', 'self', ',', 'states', ':', 'dict', '[', 'str', ',', 'Any', ']', ')', ':', 'with', 'self', '.', '_lock', ':', 'logger', '.', 'info', '(', '""', '[STATE] Updating states', '""', ')', 'for', 'component_id', ',', 'new_value', 'in', 'states', '.', 'items', '(', ')', ':', 'old_value', '=', 'self', '.', '_component_states', '.', 'get', '(', 'component_id', ')', 'cleaned_new_value', '=', 'clean_nan_values', '(', 'new_value', ')', 'cleaned_old_value', '=', 'clean_nan_values', '(', 'old_value', ')', 'if', 'cleaned_old_value', '!=', 'cleaned_new_value', ':', 'self', '.', '_component_states', '[', 'component_id', ']', '=', 'cleaned_new_value', 'logger', '.', 'info', '(', 'f', '""', '[STATE] State changed for ', '{', 'component_id', '=}', '""', ')', 'if', 'logger', '.', 'isEnabledFor', '(', 'logging', '.', 'DEBUG', ')', ':', 'logger', '.', 'debug', '(', 'f', '""', '[STATE]  - ', '{', 'cleaned_old_value', '=}', '\\n', '  - ', '{', 'cleaned_new_value', '=}', '""', ')']","Safely update component states, logging changes and handling NaN values.","['Safely', 'update', 'component', 'states', ',', 'logging', 'changes', 'and', 'handling', 'NaN', 'values', '.']"
173,"def safe_save_model_for_hf_trainer(trainer: transformers.Trainer, output_dir: str):
    

    if trainer.deepspeed:
        torch.cuda.synchronize()
        trainer.save_model(output_dir)
        return

    state_dict = trainer.model.state_dict()
    if trainer.args.should_save:
        cpu_state_dict = {key: value.cpu() for key, value in state_dict.items()}
        del state_dict
        trainer._save(output_dir, state_dict=cpu_state_dict)","['def', 'safe_save_model_for_hf_trainer', '(', 'trainer', ':', 'transformers', '.', 'Trainer', ',', 'output_dir', ':', 'str', ')', ':', 'if', 'trainer', '.', 'deepspeed', ':', 'torch', '.', 'cuda', '.', 'synchronize', '(', ')', 'trainer', '.', 'save_model', '(', 'output_dir', ')', 'return', 'state_dict', '=', 'trainer', '.', 'model', '.', 'state_dict', '(', ')', 'if', 'trainer', '.', 'args', '.', 'should_save', ':', 'cpu_state_dict', '=', '{', 'key', ':', 'value', '.', 'cpu', '(', ')', 'for', 'key', ',', 'value', 'in', 'state_dict', '.', 'items', '(', ')', '}', 'del', 'state_dict', 'trainer', '.', '_save', '(', 'output_dir', ',', 'state_dict', '=', 'cpu_state_dict', ')']","Safely save a model using a trainer, handling deepspeed and CPU state.","['Safely', 'save', 'a', 'model', 'using', 'a', 'trainer', ',', 'handling', 'deepspeed', 'and', 'CPU', 'state', '.']"
174,"def load_model(self) -> None:
        
        # TODO: add mps (apple metal) support, currently cant benchmark mps device accurately for energy
        if self.config.device == ""cuda"" or self.config.device == ""mps"":
            nexa_model = NexaTextInference(model_path=self.config.model, device=""gpu"", **self.config.model_kwargs)
        elif self.config.device == ""cpu"":
            nexa_model = NexaTextInference(model_path=self.config.model, device=""cpu"", **self.config.model_kwargs)
        else:
            raise ValueError(f""Invalid device: {self.config.device}"")
        
        self.pretrained_model = nexa_model.model","['def', 'load_model', '(', 'self', ')', '-', '>', 'None', ':', '# TODO: add mps (apple metal) support, currently cant benchmark mps device accurately for energy', 'if', 'self', '.', 'config', '.', 'device', '==', '""', 'cuda', '""', 'or', 'self', '.', 'config', '.', 'device', '==', '""', 'mps', '""', ':', 'nexa_model', '=', 'NexaTextInference', '(', 'model_path', '=', 'self', '.', 'config', '.', 'model', ',', 'device', '=', '""', 'gpu', '""', ',', '*', '*', 'self', '.', 'config', '.', 'model_kwargs', ')', 'elif', 'self', '.', 'config', '.', 'device', '==', '""', 'cpu', '""', ':', 'nexa_model', '=', 'NexaTextInference', '(', 'model_path', '=', 'self', '.', 'config', '.', 'model', ',', 'device', '=', '""', 'cpu', '""', ',', '*', '*', 'self', '.', 'config', '.', 'model_kwargs', ')', 'else', ':', 'raise', 'ValueError', '(', 'f', '""', 'Invalid device: ', '{', 'self', '.', 'config', '.', 'device', '}', '""', ')', 'self', '.', 'pretrained_model', '=', 'nexa_model', '.', 'model']",Load a text inference model based on specified device configuration,"['Load', 'a', 'text', 'inference', 'model', 'based', 'on', 'specified', 'device', 'configuration']"
175,"def switch_applications(self, app_code):
        
        if self.platform == ""darwin"":
            return f""import pyautogui; import time; pyautogui.hotkey('command', 'space', interval=0.5); pyautogui.typewrite({repr(app_code)}); pyautogui.press('enter'); time.sleep(1.0)""
        elif self.platform == ""linux"":
            return UBUNTU_APP_SETUP.replace(""APP_NAME"", app_code)
        elif self.platform == ""windows"":
            return f""import pyautogui; import time; pyautogui.hotkey('win', 'd', interval=0.5); pyautogui.typewrite({repr(app_code)}); pyautogui.press('enter'); time.sleep(1.0)""","['def', 'switch_applications', '(', 'self', ',', 'app_code', ')', ':', 'if', 'self', '.', 'platform', '==', '""', 'darwin', '""', ':', 'return', 'f', '""', 'import pyautogui; import time; pyautogui.hotkey(', ""'"", 'command', ""'"", ', ', ""'"", 'space', ""'"", ', interval=0.5); pyautogui.typewrite(', '{', 'repr', '(', 'app_code', ')', '}', '); pyautogui.press(', ""'"", 'enter', ""'"", '); time.sleep(1.0)', '""', 'elif', 'self', '.', 'platform', '==', '""', 'linux', '""', ':', 'return', 'UBUNTU_APP_SETUP', '.', 'replace', '(', '""', 'APP_NAME', '""', ',', 'app_code', ')', 'elif', 'self', '.', 'platform', '==', '""', 'windows', '""', ':', 'return', 'f', '""', 'import pyautogui; import time; pyautogui.hotkey(', ""'"", 'win', ""'"", ', ', ""'"", 'd', ""'"", ', interval=0.5); pyautogui.typewrite(', '{', 'repr', '(', 'app_code', ')', '}', '); pyautogui.press(', ""'"", 'enter', ""'"", '); time.sleep(1.0)', '""']",Generate platform-specific scripts to switch applications using automation tools.,"['Generate', 'platform-specific', 'scripts', 'to', 'switch', 'applications', 'using', 'automation', 'tools', '.']"
176,"def _merge_data(self, existing: list[dict] | None, new_data: list[dict], key_field: str) -> list[dict]:
        
        if not existing:
            return new_data

        # Create a set of existing keys for O(1) lookup
        existing_keys = {item[key_field] for item in existing}

        # Only add items that don't exist yet
        merged = existing.copy()
        merged.extend([item for item in new_data if item[key_field] not in existing_keys])
        return merged","['def', '_merge_data', '(', 'self', ',', 'existing', ':', 'list', '[', 'dict', ']', '|', 'None', ',', 'new_data', ':', 'list', '[', 'dict', ']', ',', 'key_field', ':', 'str', ')', '-', '>', 'list', '[', 'dict', ']', ':', 'if', 'not', 'existing', ':', 'return', 'new_data', '# Create a set of existing keys for O(1) lookup', 'existing_keys', '=', '{', 'item', '[', 'key_field', ']', 'for', 'item', 'in', 'existing', '}', ""# Only add items that don't exist yet"", 'merged', '=', 'existing', '.', 'copy', '(', ')', 'merged', '.', 'extend', '(', '[', 'item', 'for', 'item', 'in', 'new_data', 'if', 'item', '[', 'key_field', ']', 'not', 'in', 'existing_keys', ']', ')', 'return', 'merged']",Merge new data into existing list by unique key field,"['Merge', 'new', 'data', 'into', 'existing', 'list', 'by', 'unique', 'key', 'field']"
177,"def check_hub_revision_exists(training_args: SFTConfig | GRPOConfig):
    
    if repo_exists(training_args.hub_model_id):
        if training_args.push_to_hub_revision is True:
            # First check if the revision exists
            revisions = [rev.name for rev in list_repo_refs(training_args.hub_model_id).branches]
            # If the revision exists, we next check it has a README file
            if training_args.hub_model_revision in revisions:
                repo_files = list_repo_files(
                    repo_id=training_args.hub_model_id,
                    revision=training_args.hub_model_revision,
                )
                if ""README.md"" in repo_files and training_args.overwrite_hub_revision is False:
                    raise ValueError(
                        f""Revision {training_args.hub_model_revision} already exists. ""
                        ""Use --overwrite_hub_revision to overwrite it.""
                    )","['def', 'check_hub_revision_exists', '(', 'training_args', ':', 'SFTConfig', '|', 'GRPOConfig', ')', ':', 'if', 'repo_exists', '(', 'training_args', '.', 'hub_model_id', ')', ':', 'if', 'training_args', '.', 'push_to_hub_revision', 'is', 'True', ':', '# First check if the revision exists', 'revisions', '=', '[', 'rev', '.', 'name', 'for', 'rev', 'in', 'list_repo_refs', '(', 'training_args', '.', 'hub_model_id', ')', '.', 'branches', ']', '# If the revision exists, we next check it has a README file', 'if', 'training_args', '.', 'hub_model_revision', 'in', 'revisions', ':', 'repo_files', '=', 'list_repo_files', '(', 'repo_id', '=', 'training_args', '.', 'hub_model_id', ',', 'revision', '=', 'training_args', '.', 'hub_model_revision', ',', ')', 'if', '""', 'README.md', '""', 'in', 'repo_files', 'and', 'training_args', '.', 'overwrite_hub_revision', 'is', 'False', ':', 'raise', 'ValueError', '(', 'f', '""', 'Revision ', '{', 'training_args', '.', 'hub_model_revision', '}', ' already exists. ', '""', '""', 'Use --overwrite_hub_revision to overwrite it.', '""', ')']",Check if a specific model revision exists on the hub and validate its README presence.,"['Check', 'if', 'a', 'specific', 'model', 'revision', 'exists', 'on', 'the', 'hub', 'and', 'validate', 'its', 'README', 'presence', '.']"
178,"def __update_session_state(self, session: Session, event: Event) -> None:
    
    if not event.actions or not event.actions.state_delta:
      return
    for key, value in event.actions.state_delta.items():
      if key.startswith(State.TEMP_PREFIX):
        continue
      session.state.update({key: value})","['def', '__update_session_state', '(', 'self', ',', 'session', ':', 'Session', ',', 'event', ':', 'Event', ')', '-', '>', 'None', ':', 'if', 'not', 'event', '.', 'actions', 'or', 'not', 'event', '.', 'actions', '.', 'state_delta', ':', 'return', 'for', 'key', ',', 'value', 'in', 'event', '.', 'actions', '.', 'state_delta', '.', 'items', '(', ')', ':', 'if', 'key', '.', 'startswith', '(', 'State', '.', 'TEMP_PREFIX', ')', ':', 'continue', 'session', '.', 'state', '.', 'update', '(', '{', 'key', ':', 'value', '}', ')']","Update session state with event-driven changes, ignoring temporary keys.","['Update', 'session', 'state', 'with', 'event-driven', 'changes', ',', 'ignoring', 'temporary', 'keys', '.']"
179,"def restart(self) -> None:
        
        self._container.restart()
        if self._container.status != ""running"":
            raise ValueError(f""Failed to restart container. Logs: {self._container.logs()}"")","['def', 'restart', '(', 'self', ')', '-', '>', 'None', ':', 'self', '.', '_container', '.', 'restart', '(', ')', 'if', 'self', '.', '_container', '.', 'status', '!=', '""', 'running', '""', ':', 'raise', 'ValueError', '(', 'f', '""', 'Failed to restart container. Logs: ', '{', 'self', '.', '_container', '.', 'logs', '(', ')', '}', '""', ')']","Restart a container and verify its running status, raising an error if unsuccessful.","['Restart', 'a', 'container', 'and', 'verify', 'its', 'running', 'status', ',', 'raising', 'an', 'error', 'if', 'unsuccessful', '.']"
180,"def save_fid_stats(paths, batch_size, device, dims, num_workers=1):
    
    if not os.path.exists(paths[0]):
        raise RuntimeError(""Invalid path: %s"" % paths[0])

    if os.path.exists(paths[1]):
        raise RuntimeError(""Existing output file: %s"" % paths[1])

    block_idx = InceptionV3.BLOCK_INDEX_BY_DIM[dims]

    model = InceptionV3([block_idx]).to(device)

    print(f""Saving statistics for {paths[0]}"")

    m1, s1 = compute_statistics_of_path(paths[0], model, batch_size, dims, device, num_workers)

    np.savez_compressed(paths[1], mu=m1, sigma=s1)","['def', 'save_fid_stats', '(', 'paths', ',', 'batch_size', ',', 'device', ',', 'dims', ',', 'num_workers', '=', '1', ')', ':', 'if', 'not', 'os', '.', 'path', '.', 'exists', '(', 'paths', '[', '0', ']', ')', ':', 'raise', 'RuntimeError', '(', '""', 'Invalid path: ', '%s', '""', '%', 'paths', '[', '0', ']', ')', 'if', 'os', '.', 'path', '.', 'exists', '(', 'paths', '[', '1', ']', ')', ':', 'raise', 'RuntimeError', '(', '""', 'Existing output file: ', '%s', '""', '%', 'paths', '[', '1', ']', ')', 'block_idx', '=', 'InceptionV3', '.', 'BLOCK_INDEX_BY_DIM', '[', 'dims', ']', 'model', '=', 'InceptionV3', '(', '[', 'block_idx', ']', ')', '.', 'to', '(', 'device', ')', 'print', '(', 'f', '""', 'Saving statistics for ', '{', 'paths', '[', '0', ']', '}', '""', ')', 'm1', ',', 's1', '=', 'compute_statistics_of_path', '(', 'paths', '[', '0', ']', ',', 'model', ',', 'batch_size', ',', 'dims', ',', 'device', ',', 'num_workers', ')', 'np', '.', 'savez_compressed', '(', 'paths', '[', '1', ']', ',', 'mu', '=', 'm1', ',', 'sigma', '=', 's1', ')']",Compute and save statistical data from a specified path using a neural model.,"['Compute', 'and', 'save', 'statistical', 'data', 'from', 'a', 'specified', 'path', 'using', 'a', 'neural', 'model', '.']"
182,"def _update_macos_platform_uuid(self, new_ids):
        
        try:
            uuid_file = ""/var/root/Library/Preferences/SystemConfiguration/com.apple.platform.uuid.plist""
            if os.path.exists(uuid_file):
                # Use sudo to execute plutil command
                cmd = f'sudo plutil -replace ""UUID"" -string ""{new_ids[""telemetry.macMachineId""]}"" ""{uuid_file}""'
                result = os.system(cmd)
                if result == 0:
                    print(f""{Fore.GREEN}{EMOJI['SUCCESS']} {self.translator.get('reset.macos_platform_uuid_updated')}{Style.RESET_ALL}"")
                else:
                    raise Exception(f""{Fore.RED}{EMOJI['ERROR']} {self.translator.get('reset.failed_to_execute_plutil_command')}{Style.RESET_ALL}"")
        except Exception as e:
            print(f""{Fore.RED}{EMOJI['ERROR']} {self.translator.get('reset.update_macos_platform_uuid_failed', error=str(e))}{Style.RESET_ALL}"")
            raise","['def', '_update_macos_platform_uuid', '(', 'self', ',', 'new_ids', ')', ':', 'try', ':', 'uuid_file', '=', '""', '/var/root/Library/Preferences/SystemConfiguration/com.apple.platform.uuid.plist', '""', 'if', 'os', '.', 'path', '.', 'exists', '(', 'uuid_file', ')', ':', '# Use sudo to execute plutil command', 'cmd', '=', 'f', ""'"", 'sudo plutil -replace ', '""', 'UUID', '""', ' -string ', '""', '{', 'new_ids', '[', '""', 'telemetry.macMachineId', '""', ']', '}', '""', '""', '{', 'uuid_file', '}', '""', ""'"", 'result', '=', 'os', '.', 'system', '(', 'cmd', ')', 'if', 'result', '==', '0', ':', 'print', '(', 'f', '""', '{', 'Fore', '.', 'GREEN', '}', '{', 'EMOJI', '[', ""'"", 'SUCCESS', ""'"", ']', '}', '{', 'self', '.', 'translator', '.', 'get', '(', ""'"", 'reset.macos_platform_uuid_updated', ""'"", ')', '}', '{', 'Style', '.', 'RESET_ALL', '}', '""', ')', 'else', ':', 'raise', 'Exception', '(', 'f', '""', '{', 'Fore', '.', 'RED', '}', '{', 'EMOJI', '[', ""'"", 'ERROR', ""'"", ']', '}', '{', 'self', '.', 'translator', '.', 'get', '(', ""'"", 'reset.failed_to_execute_plutil_command', ""'"", ')', '}', '{', 'Style', '.', 'RESET_ALL', '}', '""', ')', 'except', 'Exception', 'as', 'e', ':', 'print', '(', 'f', '""', '{', 'Fore', '.', 'RED', '}', '{', 'EMOJI', '[', ""'"", 'ERROR', ""'"", ']', '}', '{', 'self', '.', 'translator', '.', 'get', '(', ""'"", 'reset.update_macos_platform_uuid_failed', ""'"", ',', 'error', '=', 'str', '(', 'e', ')', ')', '}', '{', 'Style', '.', 'RESET_ALL', '}', '""', ')', 'raise']",Update macOS platform UUID using a new telemetry ID with error handling.,"['Update', 'macOS', 'platform', 'UUID', 'using', 'a', 'new', 'telemetry', 'ID', 'with', 'error', 'handling', '.']"
184,"def process_audio_file(audio_path, text, polyphone):
    
    if not Path(audio_path).exists():
        print(f""audio {audio_path} not found, skipping"")
        return None
    try:
        audio_duration = get_audio_duration(audio_path)
        if audio_duration <= 0:
            raise ValueError(f""Duration {audio_duration} is non-positive."")
        return (audio_path, text, audio_duration)
    except Exception as e:
        print(f""Warning: Failed to process {audio_path} due to error: {e}. Skipping corrupt file."")
        return None","['def', 'process_audio_file', '(', 'audio_path', ',', 'text', ',', 'polyphone', ')', ':', 'if', 'not', 'Path', '(', 'audio_path', ')', '.', 'exists', '(', ')', ':', 'print', '(', 'f', '""', 'audio ', '{', 'audio_path', '}', ' not found, skipping', '""', ')', 'return', 'None', 'try', ':', 'audio_duration', '=', 'get_audio_duration', '(', 'audio_path', ')', 'if', 'audio_duration', '<', '=', '0', ':', 'raise', 'ValueError', '(', 'f', '""', 'Duration ', '{', 'audio_duration', '}', ' is non-positive.', '""', ')', 'return', '(', 'audio_path', ',', 'text', ',', 'audio_duration', ')', 'except', 'Exception', 'as', 'e', ':', 'print', '(', 'f', '""', 'Warning: Failed to process ', '{', 'audio_path', '}', ' due to error: ', '{', 'e', '}', '. Skipping corrupt file.', '""', ')', 'return', 'None']","Validate and extract audio file duration, handling errors gracefully.","['Validate', 'and', 'extract', 'audio', 'file', 'duration', ',', 'handling', 'errors', 'gracefully', '.']"
185,"def create_dataset_if_not_exists():
    
    # Construct a BigQuery client object.
    dataset_id = f""{client.project}.{DATASET_ID}""
    dataset = bigquery.Dataset(dataset_id)
    dataset.location = ""US""
    client.delete_dataset(
        dataset_id, delete_contents=True, not_found_ok=True
    )  # Make an API request.
    dataset = client.create_dataset(dataset)  # Make an API request.
    print(""Created dataset {}.{}"".format(client.project, dataset.dataset_id))
    return dataset","['def', 'create_dataset_if_not_exists', '(', ')', ':', '# Construct a BigQuery client object.', 'dataset_id', '=', 'f', '""', '{', 'client', '.', 'project', '}', '.', '{', 'DATASET_ID', '}', '""', 'dataset', '=', 'bigquery', '.', 'Dataset', '(', 'dataset_id', ')', 'dataset', '.', 'location', '=', '""', 'US', '""', 'client', '.', 'delete_dataset', '(', 'dataset_id', ',', 'delete_contents', '=', 'True', ',', 'not_found_ok', '=', 'True', ')', '# Make an API request.', 'dataset', '=', 'client', '.', 'create_dataset', '(', 'dataset', ')', '# Make an API request.', 'print', '(', '""', 'Created dataset ', '{}', '.', '{}', '""', '.', 'format', '(', 'client', '.', 'project', ',', 'dataset', '.', 'dataset_id', ')', ')', 'return', 'dataset']",Create or replace a BigQuery dataset in the US location.,"['Create', 'or', 'replace', 'a', 'BigQuery', 'dataset', 'in', 'the', 'US', 'location', '.']"
188,"def _search_by_url_sync(self, url: str, collection_name: str) -> Optional[List[str]]:
        
        client = self._get_milvus_client()
        
        logger.debug(f""Querying collection: {collection_name} for URL: {url}"")
        res = client.query(
            collection_name=collection_name,
            filter=f""url == '{url}'"",
            limit=1,
            output_fields=[""url"", ""text"", ""name"", ""site""],
        )
        
        if len(res) == 0:
            logger.warning(f""No item found for URL: {url}"")
            return None
        
        item = res[0]
        txt = json.dumps(item[""text""])
        logger.info(f""Successfully retrieved item for URL: {url}"")
        return [item[""url""], txt, item[""name""], item[""site""]]","['def', '_search_by_url_sync', '(', 'self', ',', 'url', ':', 'str', ',', 'collection_name', ':', 'str', ')', '-', '>', 'Optional', '[', 'List', '[', 'str', ']', ']', ':', 'client', '=', 'self', '.', '_get_milvus_client', '(', ')', 'logger', '.', 'debug', '(', 'f', '""', 'Querying collection: ', '{', 'collection_name', '}', ' for URL: ', '{', 'url', '}', '""', ')', 'res', '=', 'client', '.', 'query', '(', 'collection_name', '=', 'collection_name', ',', 'filter', '=', 'f', '""', 'url == ', ""'"", '{', 'url', '}', ""'"", '""', ',', 'limit', '=', '1', ',', 'output_fields', '=', '[', '""', 'url', '""', ',', '""', 'text', '""', ',', '""', 'name', '""', ',', '""', 'site', '""', ']', ',', ')', 'if', 'len', '(', 'res', ')', '==', '0', ':', 'logger', '.', 'warning', '(', 'f', '""', 'No item found for URL: ', '{', 'url', '}', '""', ')', 'return', 'None', 'item', '=', 'res', '[', '0', ']', 'txt', '=', 'json', '.', 'dumps', '(', 'item', '[', '""', 'text', '""', ']', ')', 'logger', '.', 'info', '(', 'f', '""', 'Successfully retrieved item for URL: ', '{', 'url', '}', '""', ')', 'return', '[', 'item', '[', '""', 'url', '""', ']', ',', 'txt', ',', 'item', '[', '""', 'name', '""', ']', ',', 'item', '[', '""', 'site', '""', ']', ']']",Retrieve and log data from a collection by matching a given URL.,"['Retrieve', 'and', 'log', 'data', 'from', 'a', 'collection', 'by', 'matching', 'a', 'given', 'URL', '.']"
189,"def get_all_pages(s3_client, document_files):
    
    file_contents = {}

    # First, collect all file paths and their document info
    for file_path in tqdm(document_files, desc=""Loading document files""):
        lines = load_document_file(s3_client, file_path)
        if not lines:
            logger.warning(f""Empty or invalid file: {file_path}"")
            continue

        # Parse each line for document info
        documents = []
        for i, line in enumerate(lines):
            doc_info = get_document_info_from_line(line, file_path, i)
            # Always add an entry for each line, even if None, to preserve line alignment
            documents.append(doc_info)

        # Store all documents for this file
        file_contents[file_path] = documents
        logger.info(f""Loaded {len(documents)} documents from {file_path}"")

    logger.info(f""Loaded documents from {len(file_contents)} files"")
    return file_contents","['def', 'get_all_pages', '(', 's3_client', ',', 'document_files', ')', ':', 'file_contents', '=', '{', '}', '# First, collect all file paths and their document info', 'for', 'file_path', 'in', 'tqdm', '(', 'document_files', ',', 'desc', '=', '""', 'Loading document files', '""', ')', ':', 'lines', '=', 'load_document_file', '(', 's3_client', ',', 'file_path', ')', 'if', 'not', 'lines', ':', 'logger', '.', 'warning', '(', 'f', '""', 'Empty or invalid file: ', '{', 'file_path', '}', '""', ')', 'continue', '# Parse each line for document info', 'documents', '=', '[', ']', 'for', 'i', ',', 'line', 'in', 'enumerate', '(', 'lines', ')', ':', 'doc_info', '=', 'get_document_info_from_line', '(', 'line', ',', 'file_path', ',', 'i', ')', '# Always add an entry for each line, even if None, to preserve line alignment', 'documents', '.', 'append', '(', 'doc_info', ')', '# Store all documents for this file', 'file_contents', '[', 'file_path', ']', '=', 'documents', 'logger', '.', 'info', '(', 'f', '""', 'Loaded ', '{', 'len', '(', 'documents', ')', '}', ' documents from ', '{', 'file_path', '}', '""', ')', 'logger', '.', 'info', '(', 'f', '""', 'Loaded documents from ', '{', 'len', '(', 'file_contents', ')', '}', ' files', '""', ')', 'return', 'file_contents']","Load and parse document files from S3, storing structured information for each line.","['Load', 'and', 'parse', 'document', 'files', 'from', 'S3', ',', 'storing', 'structured', 'information', 'for', 'each', 'line', '.']"
191,"def _store_conversation_turn(
        self, prompt: str, response_content: Dict[str, Any], call_type: str
    ):
        
        if self.request_id not in _conversation_history:
            _conversation_history[self.request_id] = []

        # Add user message
        _conversation_history[self.request_id].append(
            {""role"": ""user"", ""content"": prompt}
        )

        # Add AI response based on call type
        ai_content = self._format_ai_response(response_content, call_type)
        if ai_content:
            _conversation_history[self.request_id].append(
                {""role"": ""assistant"", ""content"": ai_content}
            )

        # Update project metadata
        if self.request_id in _project_metadata:
            _project_metadata[self.request_id][""last_activity""] = time.time()","['def', '_store_conversation_turn', '(', 'self', ',', 'prompt', ':', 'str', ',', 'response_content', ':', 'Dict', '[', 'str', ',', 'Any', ']', ',', 'call_type', ':', 'str', ')', ':', 'if', 'self', '.', 'request_id', 'not', 'in', '_conversation_history', ':', '_conversation_history', '[', 'self', '.', 'request_id', ']', '=', '[', ']', '# Add user message', '_conversation_history', '[', 'self', '.', 'request_id', ']', '.', 'append', '(', '{', '""', 'role', '""', ':', '""', 'user', '""', ',', '""', 'content', '""', ':', 'prompt', '}', ')', '# Add AI response based on call type', 'ai_content', '=', 'self', '.', '_format_ai_response', '(', 'response_content', ',', 'call_type', ')', 'if', 'ai_content', ':', '_conversation_history', '[', 'self', '.', 'request_id', ']', '.', 'append', '(', '{', '""', 'role', '""', ':', '""', 'assistant', '""', ',', '""', 'content', '""', ':', 'ai_content', '}', ')', '# Update project metadata', 'if', 'self', '.', 'request_id', 'in', '_project_metadata', ':', '_project_metadata', '[', 'self', '.', 'request_id', ']', '[', '""', 'last_activity', '""', ']', '=', 'time', '.', 'time', '(', ')']",Store and update conversation history and metadata for user interactions.,"['Store', 'and', 'update', 'conversation', 'history', 'and', 'metadata', 'for', 'user', 'interactions', '.']"
194,"def _create_branch_ctx_for_sub_agent(
    agent: BaseAgent,
    sub_agent: BaseAgent,
    invocation_context: InvocationContext,
) -> InvocationContext:
  
  invocation_context = invocation_context.model_copy()
  branch_suffix = f""{agent.name}.{sub_agent.name}""
  invocation_context.branch = (
      f""{invocation_context.branch}.{branch_suffix}""
      if invocation_context.branch
      else branch_suffix
  )
  return invocation_context","['def', '_create_branch_ctx_for_sub_agent', '(', 'agent', ':', 'BaseAgent', ',', 'sub_agent', ':', 'BaseAgent', ',', 'invocation_context', ':', 'InvocationContext', ',', ')', '-', '>', 'InvocationContext', ':', 'invocation_context', '=', 'invocation_context', '.', 'model_copy', '(', ')', 'branch_suffix', '=', 'f', '""', '{', 'agent', '.', 'name', '}', '.', '{', 'sub_agent', '.', 'name', '}', '""', 'invocation_context', '.', 'branch', '=', '(', 'f', '""', '{', 'invocation_context', '.', 'branch', '}', '.', '{', 'branch_suffix', '}', '""', 'if', 'invocation_context', '.', 'branch', 'else', 'branch_suffix', ')', 'return', 'invocation_context']",Create a new invocation context with updated branch information for a sub-agent.,"['Create', 'a', 'new', 'invocation', 'context', 'with', 'updated', 'branch', 'information', 'for', 'a', 'sub-agent', '.']"
195,"def build_list_tables_context(keyspace_name: str, tables: List[TableInfo]) -> str:
    
    context = {
        'cassandra_knowledge': build_cassandra_knowledge(),
        'amazon_keyspaces_knowledge': build_amazon_keyspaces_knowledge(),
    }

    # Add table-specific guidance
    tables_guidance = {
        'data_modeling': 'In Cassandra, tables are containers for related data, similar to tablesin relational databases. '
        'However, Cassandra tables  are optimized for specific access patterns based on their primary key '
        'design. The primary key determines how data is distributed physically in the database, and the '
        'attributes that can be specified for efficient query execution. Primary keys consist of a '
        'partition key (which determines data distribution) and optional cluster columns which determine '
        'how data is ordered within a partition.',
        'naming_conventions': 'Table names typically use snake_case and should be descriptive of the entity they represent.',
    }
    context['tables_guidance'] = tables_guidance

    return dict_to_markdown(context)","['def', 'build_list_tables_context', '(', 'keyspace_name', ':', 'str', ',', 'tables', ':', 'List', '[', 'TableInfo', ']', ')', '-', '>', 'str', ':', 'context', '=', '{', ""'"", 'cassandra_knowledge', ""'"", ':', 'build_cassandra_knowledge', '(', ')', ',', ""'"", 'amazon_keyspaces_knowledge', ""'"", ':', 'build_amazon_keyspaces_knowledge', '(', ')', ',', '}', '# Add table-specific guidance', 'tables_guidance', '=', '{', ""'"", 'data_modeling', ""'"", ':', ""'"", 'In Cassandra, tables are containers for related data, similar to tablesin relational databases. ', ""'"", ""'"", 'However, Cassandra tables  are optimized for specific access patterns based on their primary key ', ""'"", ""'"", 'design. The primary key determines how data is distributed physically in the database, and the ', ""'"", ""'"", 'attributes that can be specified for efficient query execution. Primary keys consist of a ', ""'"", ""'"", 'partition key (which determines data distribution) and optional cluster columns which determine ', ""'"", ""'"", 'how data is ordered within a partition.', ""'"", ',', ""'"", 'naming_conventions', ""'"", ':', ""'"", 'Table names typically use snake_case and should be descriptive of the entity they represent.', ""'"", ',', '}', 'context', '[', ""'"", 'tables_guidance', ""'"", ']', '=', 'tables_guidance', 'return', 'dict_to_markdown', '(', 'context', ')']",Generate a markdown context for Cassandra and Amazon Keyspaces table guidance.,"['Generate', 'a', 'markdown', 'context', 'for', 'Cassandra', 'and', 'Amazon', 'Keyspaces', 'table', 'guidance', '.']"
196,"def _generate_summary(self) -> Dict[str, Any]:
        
        correct = sum(result[""score""] for result in self._results)
        return {
            ""total"": len(self._results),
            ""correct"": correct,
            ""results"": self._results,
            ""accuracy"": correct / len(self._results) if len(self._results) > 0 else 0,
        }","['def', '_generate_summary', '(', 'self', ')', '-', '>', 'Dict', '[', 'str', ',', 'Any', ']', ':', 'correct', '=', 'sum', '(', 'result', '[', '""', 'score', '""', ']', 'for', 'result', 'in', 'self', '.', '_results', ')', 'return', '{', '""', 'total', '""', ':', 'len', '(', 'self', '.', '_results', ')', ',', '""', 'correct', '""', ':', 'correct', ',', '""', 'results', '""', ':', 'self', '.', '_results', ',', '""', 'accuracy', '""', ':', 'correct', '/', 'len', '(', 'self', '.', '_results', ')', 'if', 'len', '(', 'self', '.', '_results', ')', '>', '0', 'else', '0', ',', '}']",Calculate and return performance metrics from results data.,"['Calculate', 'and', 'return', 'performance', 'metrics', 'from', 'results', 'data', '.']"
198,"def resolve_ref(ref_string, current_doc):
      
      parts = ref_string.split(""/"")
      if parts[0] != ""#"":
        raise ValueError(f""External references not supported: {ref_string}"")

      current = current_doc
      for part in parts[1:]:
        if part in current:
          current = current[part]
        else:
          return None  # Reference not found
      return current","['def', 'resolve_ref', '(', 'ref_string', ',', 'current_doc', ')', ':', 'parts', '=', 'ref_string', '.', 'split', '(', '""', '/', '""', ')', 'if', 'parts', '[', '0', ']', '!=', '""', '#', '""', ':', 'raise', 'ValueError', '(', 'f', '""', 'External references not supported: ', '{', 'ref_string', '}', '""', ')', 'current', '=', 'current_doc', 'for', 'part', 'in', 'parts', '[', '1', ':', ']', ':', 'if', 'part', 'in', 'current', ':', 'current', '=', 'current', '[', 'part', ']', 'else', ':', 'return', 'None', '# Reference not found', 'return', 'current']",Resolve JSON reference paths within a document to retrieve nested values.,"['Resolve', 'JSON', 'reference', 'paths', 'within', 'a', 'document', 'to', 'retrieve', 'nested', 'values', '.']"
199,"def toggle_light(light_name: str, state: bool) -> dict[str, Any]:
    
    if not (bridge := _get_bridge()):
        return {""error"": ""Bridge not connected"", ""success"": False}
    try:
        result = bridge.set_light(light_name, ""on"", state)
        return {
            ""light"": light_name,
            ""set_on_state"": state,
            ""success"": True,
            ""phue2_result"": result,
        }
    except (KeyError, PhueException, Exception) as e:
        return handle_phue_error(light_name, ""toggle_light"", e)","['def', 'toggle_light', '(', 'light_name', ':', 'str', ',', 'state', ':', 'bool', ')', '-', '>', 'dict', '[', 'str', ',', 'Any', ']', ':', 'if', 'not', '(', 'bridge', ':=', '_get_bridge', '(', ')', ')', ':', 'return', '{', '""', 'error', '""', ':', '""', 'Bridge not connected', '""', ',', '""', 'success', '""', ':', 'False', '}', 'try', ':', 'result', '=', 'bridge', '.', 'set_light', '(', 'light_name', ',', '""', 'on', '""', ',', 'state', ')', 'return', '{', '""', 'light', '""', ':', 'light_name', ',', '""', 'set_on_state', '""', ':', 'state', ',', '""', 'success', '""', ':', 'True', ',', '""', 'phue2_result', '""', ':', 'result', ',', '}', 'except', '(', 'KeyError', ',', 'PhueException', ',', 'Exception', ')', 'as', 'e', ':', 'return', 'handle_phue_error', '(', 'light_name', ',', '""', 'toggle_light', '""', ',', 'e', ')']",Toggle a smart light's state and handle potential errors,"['Toggle', 'a', 'smart', 'light', ""'s"", 'state', 'and', 'handle', 'potential', 'errors']"
200,"def list_files(relative_path: str = """") -> List[str]:
    
    path = (CONTEXT_PATH / relative_path).resolve()
    if not str(path).startswith(str(CONTEXT_PATH)):
        return [f""Access denied: {relative_path}""]
    if not path.exists() or not path.is_dir():
        return [f""Not a directory: {relative_path}""]
    return os.listdir(path)","['def', 'list_files', '(', 'relative_path', ':', 'str', '=', '""', '""', ')', '-', '>', 'List', '[', 'str', ']', ':', 'path', '=', '(', 'CONTEXT_PATH', '/', 'relative_path', ')', '.', 'resolve', '(', ')', 'if', 'not', 'str', '(', 'path', ')', '.', 'startswith', '(', 'str', '(', 'CONTEXT_PATH', ')', ')', ':', 'return', '[', 'f', '""', 'Access denied: ', '{', 'relative_path', '}', '""', ']', 'if', 'not', 'path', '.', 'exists', '(', ')', 'or', 'not', 'path', '.', 'is_dir', '(', ')', ':', 'return', '[', 'f', '""', 'Not a directory: ', '{', 'relative_path', '}', '""', ']', 'return', 'os', '.', 'listdir', '(', 'path', ')']","Function lists directory contents, ensuring path security and validity.","['Function', 'lists', 'directory', 'contents', ',', 'ensuring', 'path', 'security', 'and', 'validity', '.']"
201,"def process_query(ner_query):
            
            candidate_entities = self.ner.invoke(ner_query, **kwargs)
            for candidate_entity in candidate_entities:
                query_type = candidate_entity.get_entity_first_type_or_un_std()

                ner_id = f""{candidate_entity.entity_name}_{query_type}""
                if ner_id not in ner_maps:
                    ner_maps[ner_id] = {
                        ""candidate"": candidate_entity,
                        ""query"": ner_query,
                        ""query_type"": query_type,
                    }","['def', 'process_query', '(', 'ner_query', ')', ':', 'candidate_entities', '=', 'self', '.', 'ner', '.', 'invoke', '(', 'ner_query', ',', '*', '*', 'kwargs', ')', 'for', 'candidate_entity', 'in', 'candidate_entities', ':', 'query_type', '=', 'candidate_entity', '.', 'get_entity_first_type_or_un_std', '(', ')', 'ner_id', '=', 'f', '""', '{', 'candidate_entity', '.', 'entity_name', '}', '_', '{', 'query_type', '}', '""', 'if', 'ner_id', 'not', 'in', 'ner_maps', ':', 'ner_maps', '[', 'ner_id', ']', '=', '{', '""', 'candidate', '""', ':', 'candidate_entity', ',', '""', 'query', '""', ':', 'ner_query', ',', '""', 'query_type', '""', ':', 'query_type', ',', '}']",Process named entity recognition queries and map unique entities.,"['Process', 'named', 'entity', 'recognition', 'queries', 'and', 'map', 'unique', 'entities', '.']"
202,"def _create_project_entity(self, project_data):
        
        logger.debug(
            f""Creating project entity for: {project_data.get('key')} - {project_data.get('name')}""
        )
        # Use a composite ID format that includes the entity type for uniqueness
        entity_id = f""project-{project_data['id']}""

        return JiraProjectEntity(
            entity_id=entity_id,  # Modified to use unique ID
            breadcrumbs=[],  # top-level object, no parent
            project_key=project_data[""key""],
            name=project_data.get(""name""),
            description=project_data.get(""description""),
        )","['def', '_create_project_entity', '(', 'self', ',', 'project_data', ')', ':', 'logger', '.', 'debug', '(', 'f', '""', 'Creating project entity for: ', '{', 'project_data', '.', 'get', '(', ""'"", 'key', ""'"", ')', '}', ' - ', '{', 'project_data', '.', 'get', '(', ""'"", 'name', ""'"", ')', '}', '""', ')', '# Use a composite ID format that includes the entity type for uniqueness', 'entity_id', '=', 'f', '""', 'project-', '{', 'project_data', '[', ""'"", 'id', ""'"", ']', '}', '""', 'return', 'JiraProjectEntity', '(', 'entity_id', '=', 'entity_id', ',', '# Modified to use unique ID', 'breadcrumbs', '=', '[', ']', ',', '# top-level object, no parent', 'project_key', '=', 'project_data', '[', '""', 'key', '""', ']', ',', 'name', '=', 'project_data', '.', 'get', '(', '""', 'name', '""', ')', ',', 'description', '=', 'project_data', '.', 'get', '(', '""', 'description', '""', ')', ',', ')']","Create a unique project entity with ID, key, name, and description.","['Create', 'a', 'unique', 'project', 'entity', 'with', 'ID', ',', 'key', ',', 'name', ',', 'and', 'description', '.']"
203,"def _generate_context(self) -> str:
        
        context_parts = []

        for idx, intent in enumerate(self.intents.values(), 1):
            description = (
                f""{idx}. Intent: {intent.name}\nDescription: {intent.description}""
            )

            if intent.examples:
                examples = ""\n"".join(f""- {example}"" for example in intent.examples)
                description += f""\nExamples:\n{examples}""

            if intent.metadata:
                metadata = ""\n"".join(
                    f""- {key}: {value}"" for key, value in intent.metadata.items()
                )
                description += f""\nAdditional Information:\n{metadata}""

            context_parts.append(description)

        return ""\n\n"".join(context_parts)","['def', '_generate_context', '(', 'self', ')', '-', '>', 'str', ':', 'context_parts', '=', '[', ']', 'for', 'idx', ',', 'intent', 'in', 'enumerate', '(', 'self', '.', 'intents', '.', 'values', '(', ')', ',', '1', ')', ':', 'description', '=', '(', 'f', '""', '{', 'idx', '}', '. Intent: ', '{', 'intent', '.', 'name', '}', '\\n', 'Description: ', '{', 'intent', '.', 'description', '}', '""', ')', 'if', 'intent', '.', 'examples', ':', 'examples', '=', '""', '\\n', '""', '.', 'join', '(', 'f', '""', '- ', '{', 'example', '}', '""', 'for', 'example', 'in', 'intent', '.', 'examples', ')', 'description', '+', '=', 'f', '""', '\\n', 'Examples:', '\\n', '{', 'examples', '}', '""', 'if', 'intent', '.', 'metadata', ':', 'metadata', '=', '""', '\\n', '""', '.', 'join', '(', 'f', '""', '- ', '{', 'key', '}', ': ', '{', 'value', '}', '""', 'for', 'key', ',', 'value', 'in', 'intent', '.', 'metadata', '.', 'items', '(', ')', ')', 'description', '+', '=', 'f', '""', '\\n', 'Additional Information:', '\\n', '{', 'metadata', '}', '""', 'context_parts', '.', 'append', '(', 'description', ')', 'return', '""', '\\n', '\\n', '""', '.', 'join', '(', 'context_parts', ')']","Generate a formatted string detailing intents with descriptions, examples, and metadata.","['Generate', 'a', 'formatted', 'string', 'detailing', 'intents', 'with', 'descriptions', ',', 'examples', ',', 'and', 'metadata', '.']"
204,"def from_orm_with_collection_mapping(cls, obj):
        
        # Convert to dict and filter out SQLAlchemy internal attributes
        obj_dict = {k: v for k, v in obj.__dict__.items() if not k.startswith(""_"")}

        # Map the readable_collection_id to collection if needed
        if hasattr(obj, ""readable_collection_id""):
            obj_dict[""collection""] = obj.readable_collection_id

        return cls.model_validate(obj_dict)","['def', 'from_orm_with_collection_mapping', '(', 'cls', ',', 'obj', ')', ':', '# Convert to dict and filter out SQLAlchemy internal attributes', 'obj_dict', '=', '{', 'k', ':', 'v', 'for', 'k', ',', 'v', 'in', 'obj', '.', '__dict__', '.', 'items', '(', ')', 'if', 'not', 'k', '.', 'startswith', '(', '""', '_', '""', ')', '}', '# Map the readable_collection_id to collection if needed', 'if', 'hasattr', '(', 'obj', ',', '""', 'readable_collection_id', '""', ')', ':', 'obj_dict', '[', '""', 'collection', '""', ']', '=', 'obj', '.', 'readable_collection_id', 'return', 'cls', '.', 'model_validate', '(', 'obj_dict', ')']",Convert ORM object to validated model dictionary with collection mapping.,"['Convert', 'ORM', 'object', 'to', 'validated', 'model', 'dictionary', 'with', 'collection', 'mapping', '.']"
205,"def _replace_agentlist_placeholder(cls: Type[""GroupManagerSelectionMessageContextStr""], v: Any) -> Union[str, Any]:  # noqa: N805
        
        if isinstance(v, str):
            if ""{agentlist}"" in v:
                return v.replace(""{agentlist}"", ""<<agent_list>>"")  # Perform the replacement
            else:
                return v  # If no replacement is needed, return the original value
        return """"","['def', '_replace_agentlist_placeholder', '(', 'cls', ':', 'Type', '[', '""', 'GroupManagerSelectionMessageContextStr', '""', ']', ',', 'v', ':', 'Any', ')', '-', '>', 'Union', '[', 'str', ',', 'Any', ']', ':', '# noqa: N805', 'if', 'isinstance', '(', 'v', ',', 'str', ')', ':', 'if', '""', '{agentlist}', '""', 'in', 'v', ':', 'return', 'v', '.', 'replace', '(', '""', '{agentlist}', '""', ',', '""', '<<agent_list>>', '""', ')', '# Perform the replacement', 'else', ':', 'return', 'v', '# If no replacement is needed, return the original value', 'return', '""', '""']",Replace placeholder in string with agent list identifier,"['Replace', 'placeholder', 'in', 'string', 'with', 'agent', 'list', 'identifier']"
206,"def from_pretrained0802(self, config_path, model_path):
        
        model = self.from_hparams0802(config_path)
        state_dict_raw = torch.load(model_path, map_location=""cpu"")['state_dict']
        state_dict = dict()
        for k, v in state_dict_raw.items():
            if k.startswith('backbone.') or k.startswith('head.') or k.startswith('feature_extractor.'):
                state_dict[k] = v
        # if isinstance(model.feature_extractor, EncodecFeatures):
        #     encodec_parameters = {
        #         ""feature_extractor.encodec."" + key: value
        #         for key, value in model.feature_extractor.encodec.state_dict().items()
        #     }
        #     state_dict.update(encodec_parameters)
        model.load_state_dict(state_dict)
        model.eval()
        return model","['def', 'from_pretrained0802', '(', 'self', ',', 'config_path', ',', 'model_path', ')', ':', 'model', '=', 'self', '.', 'from_hparams0802', '(', 'config_path', ')', 'state_dict_raw', '=', 'torch', '.', 'load', '(', 'model_path', ',', 'map_location', '=', '""', 'cpu', '""', ')', '[', ""'"", 'state_dict', ""'"", ']', 'state_dict', '=', 'dict', '(', ')', 'for', 'k', ',', 'v', 'in', 'state_dict_raw', '.', 'items', '(', ')', ':', 'if', 'k', '.', 'startswith', '(', ""'"", 'backbone.', ""'"", ')', 'or', 'k', '.', 'startswith', '(', ""'"", 'head.', ""'"", ')', 'or', 'k', '.', 'startswith', '(', ""'"", 'feature_extractor.', ""'"", ')', ':', 'state_dict', '[', 'k', ']', '=', 'v', '# if isinstance(model.feature_extractor, EncodecFeatures):', '#     encodec_parameters = {', '#         ""feature_extractor.encodec."" + key: value', '#         for key, value in model.feature_extractor.encodec.state_dict().items()', '#     }', '#     state_dict.update(encodec_parameters)', 'model', '.', 'load_state_dict', '(', 'state_dict', ')', 'model', '.', 'eval', '(', ')', 'return', 'model']",Load and initialize a model with filtered pretrained weights from specified paths.,"['Load', 'and', 'initialize', 'a', 'model', 'with', 'filtered', 'pretrained', 'weights', 'from', 'specified', 'paths', '.']"
207,"def from_mcp_tool_result(
        cls, result: CallToolResult, tool_use_id: str
    ) -> types.Content:
        
        if result.isError:
            function_response = {""error"": str(result.content)}
        else:
            function_response_parts = mcp_content_to_google_parts(result.content)
            function_response = {""result"": function_response_parts}

        function_response_part = types.Part.from_function_response(
            name=tool_use_id,
            response=function_response,
        )

        function_response_content = types.Content(
            role=""tool"", parts=[function_response_part]
        )

        return function_response_content","['def', 'from_mcp_tool_result', '(', 'cls', ',', 'result', ':', 'CallToolResult', ',', 'tool_use_id', ':', 'str', ')', '-', '>', 'types', '.', 'Content', ':', 'if', 'result', '.', 'isError', ':', 'function_response', '=', '{', '""', 'error', '""', ':', 'str', '(', 'result', '.', 'content', ')', '}', 'else', ':', 'function_response_parts', '=', 'mcp_content_to_google_parts', '(', 'result', '.', 'content', ')', 'function_response', '=', '{', '""', 'result', '""', ':', 'function_response_parts', '}', 'function_response_part', '=', 'types', '.', 'Part', '.', 'from_function_response', '(', 'name', '=', 'tool_use_id', ',', 'response', '=', 'function_response', ',', ')', 'function_response_content', '=', 'types', '.', 'Content', '(', 'role', '=', '""', 'tool', '""', ',', 'parts', '=', '[', 'function_response_part', ']', ')', 'return', 'function_response_content']",Convert tool result into structured content with error handling.,"['Convert', 'tool', 'result', 'into', 'structured', 'content', 'with', 'error', 'handling', '.']"
208,"def load_user_config():
    
    if Path(f'{args.model_dir}/config-user.yaml').exists():
        file_content = open(f'{args.model_dir}/config-user.yaml', 'r').read().strip()

        if file_content:
            user_config = yaml.safe_load(file_content)
        else:
            user_config = {}
    else:
        user_config = {}

    for model_name in user_config:
        user_config[model_name] = transform_legacy_kv_cache_options(user_config[model_name])

    return user_config","['def', 'load_user_config', '(', ')', ':', 'if', 'Path', '(', 'f', ""'"", '{', 'args', '.', 'model_dir', '}', '/config-user.yaml', ""'"", ')', '.', 'exists', '(', ')', ':', 'file_content', '=', 'open', '(', 'f', ""'"", '{', 'args', '.', 'model_dir', '}', '/config-user.yaml', ""'"", ',', ""'"", 'r', ""'"", ')', '.', 'read', '(', ')', '.', 'strip', '(', ')', 'if', 'file_content', ':', 'user_config', '=', 'yaml', '.', 'safe_load', '(', 'file_content', ')', 'else', ':', 'user_config', '=', '{', '}', 'else', ':', 'user_config', '=', '{', '}', 'for', 'model_name', 'in', 'user_config', ':', 'user_config', '[', 'model_name', ']', '=', 'transform_legacy_kv_cache_options', '(', 'user_config', '[', 'model_name', ']', ')', 'return', 'user_config']",Load and transform user configuration from a YAML file if it exists,"['Load', 'and', 'transform', 'user', 'configuration', 'from', 'a', 'YAML', 'file', 'if', 'it', 'exists']"
209,"def shopping_get_sku_latest_review_author(sku: str) -> str:
    
    header = {
        ""Authorization"": f""Bearer {shopping_get_auth_token()}"",
        ""Content-Type"": ""application/json"",
    }
    response = requests.get(
        f""{SHOPPING}/rest/V1/products/{sku}/reviews"", headers=header
    )
    assert response.status_code == 200
    response_obj = response.json()
    if len(response_obj) == 0:
        return """"
    author: str = response_obj[-1][""nickname""]
    return author","['def', 'shopping_get_sku_latest_review_author', '(', 'sku', ':', 'str', ')', '-', '>', 'str', ':', 'header', '=', '{', '""', 'Authorization', '""', ':', 'f', '""', 'Bearer ', '{', 'shopping_get_auth_token', '(', ')', '}', '""', ',', '""', 'Content-Type', '""', ':', '""', 'application/json', '""', ',', '}', 'response', '=', 'requests', '.', 'get', '(', 'f', '""', '{', 'SHOPPING', '}', '/rest/V1/products/', '{', 'sku', '}', '/reviews', '""', ',', 'headers', '=', 'header', ')', 'assert', 'response', '.', 'status_code', '==', '200', 'response_obj', '=', 'response', '.', 'json', '(', ')', 'if', 'len', '(', 'response_obj', ')', '==', '0', ':', 'return', '""', '""', 'author', ':', 'str', '=', 'response_obj', '[', '-', '1', ']', '[', '""', 'nickname', '""', ']', 'return', 'author']",Retrieve the latest review author's name for a product SKU from an API.,"['Retrieve', 'the', 'latest', 'review', 'author', ""'s"", 'name', 'for', 'a', 'product', 'SKU', 'from', 'an', 'API', '.']"
210,"def __init__(self):
        
        self.reader = None
        # Determine best available device
        self.device = ""cpu""
        if torch.cuda.is_available():
            self.device = ""cuda""
        elif (
            hasattr(torch, ""backends"")
            and hasattr(torch.backends, ""mps"")
            and torch.backends.mps.is_available()
        ):
            self.device = ""mps""
        logger.info(f""OCR processor initialized with device: {self.device}"")","['def', '__init__', '(', 'self', ')', ':', 'self', '.', 'reader', '=', 'None', '# Determine best available device', 'self', '.', 'device', '=', '""', 'cpu', '""', 'if', 'torch', '.', 'cuda', '.', 'is_available', '(', ')', ':', 'self', '.', 'device', '=', '""', 'cuda', '""', 'elif', '(', 'hasattr', '(', 'torch', ',', '""', 'backends', '""', ')', 'and', 'hasattr', '(', 'torch', '.', 'backends', ',', '""', 'mps', '""', ')', 'and', 'torch', '.', 'backends', '.', 'mps', '.', 'is_available', '(', ')', ')', ':', 'self', '.', 'device', '=', '""', 'mps', '""', 'logger', '.', 'info', '(', 'f', '""', 'OCR processor initialized with device: ', '{', 'self', '.', 'device', '}', '""', ')']",Initialize OCR processor with optimal device selection for execution.,"['Initialize', 'OCR', 'processor', 'with', 'optimal', 'device', 'selection', 'for', 'execution', '.']"
211,"def pkce_challenge():
    
    import base64
    import hashlib
    import secrets

    # Generate a code verifier
    code_verifier = secrets.token_urlsafe(64)[:128]

    # Create code challenge using S256 method
    code_verifier_bytes = code_verifier.encode(""ascii"")
    sha256 = hashlib.sha256(code_verifier_bytes).digest()
    code_challenge = base64.urlsafe_b64encode(sha256).decode().rstrip(""="")

    return {""code_verifier"": code_verifier, ""code_challenge"": code_challenge}","['def', 'pkce_challenge', '(', ')', ':', 'import', 'base64', 'import', 'hashlib', 'import', 'secrets', '# Generate a code verifier', 'code_verifier', '=', 'secrets', '.', 'token_urlsafe', '(', '64', ')', '[', ':', '128', ']', '# Create code challenge using S256 method', 'code_verifier_bytes', '=', 'code_verifier', '.', 'encode', '(', '""', 'ascii', '""', ')', 'sha256', '=', 'hashlib', '.', 'sha256', '(', 'code_verifier_bytes', ')', '.', 'digest', '(', ')', 'code_challenge', '=', 'base64', '.', 'urlsafe_b64encode', '(', 'sha256', ')', '.', 'decode', '(', ')', '.', 'rstrip', '(', '""', '=', '""', ')', 'return', '{', '""', 'code_verifier', '""', ':', 'code_verifier', ',', '""', 'code_challenge', '""', ':', 'code_challenge', '}']",Generate PKCE code verifier and challenge for secure OAuth2 authentication.,"['Generate', 'PKCE', 'code', 'verifier', 'and', 'challenge', 'for', 'secure', 'OAuth2', 'authentication', '.']"
212,"def add_tool(
        self,
        fn: Callable[..., Any],
        name: str | None = None,
        description: str | None = None,
        annotations: ToolAnnotations | None = None,
    ) -> Tool:
        
        tool = Tool.from_function(
            fn, name=name, description=description, annotations=annotations
        )
        existing = self._tools.get(tool.name)
        if existing:
            if self.warn_on_duplicate_tools:
                logger.warning(f""Tool already exists: {tool.name}"")
            return existing
        self._tools[tool.name] = tool
        return tool","['def', 'add_tool', '(', 'self', ',', 'fn', ':', 'Callable', '[', '.', '.', '.', ',', 'Any', ']', ',', 'name', ':', 'str', '|', 'None', '=', 'None', ',', 'description', ':', 'str', '|', 'None', '=', 'None', ',', 'annotations', ':', 'ToolAnnotations', '|', 'None', '=', 'None', ',', ')', '-', '>', 'Tool', ':', 'tool', '=', 'Tool', '.', 'from_function', '(', 'fn', ',', 'name', '=', 'name', ',', 'description', '=', 'description', ',', 'annotations', '=', 'annotations', ')', 'existing', '=', 'self', '.', '_tools', '.', 'get', '(', 'tool', '.', 'name', ')', 'if', 'existing', ':', 'if', 'self', '.', 'warn_on_duplicate_tools', ':', 'logger', '.', 'warning', '(', 'f', '""', 'Tool already exists: ', '{', 'tool', '.', 'name', '}', '""', ')', 'return', 'existing', 'self', '.', '_tools', '[', 'tool', '.', 'name', ']', '=', 'tool', 'return', 'tool']","Add a new tool to a collection, checking for duplicates and logging warnings.","['Add', 'a', 'new', 'tool', 'to', 'a', 'collection', ',', 'checking', 'for', 'duplicates', 'and', 'logging', 'warnings', '.']"
213,"def cleanup_temp_files(self):
        
        try:
            for part_index in list(self._part_working_dirs.keys()):
                self.cleanup_part_working_dir(part_index)
            if self._is_temp_dir:
                logger.info(f""cleanup temp files: {self.working_dir}"")
                shutil.rmtree(self.working_dir)
        except Exception:
            logger.exception(""Error cleaning up temporary files"")","['def', 'cleanup_temp_files', '(', 'self', ')', ':', 'try', ':', 'for', 'part_index', 'in', 'list', '(', 'self', '.', '_part_working_dirs', '.', 'keys', '(', ')', ')', ':', 'self', '.', 'cleanup_part_working_dir', '(', 'part_index', ')', 'if', 'self', '.', '_is_temp_dir', ':', 'logger', '.', 'info', '(', 'f', '""', 'cleanup temp files: ', '{', 'self', '.', 'working_dir', '}', '""', ')', 'shutil', '.', 'rmtree', '(', 'self', '.', 'working_dir', ')', 'except', 'Exception', ':', 'logger', '.', 'exception', '(', '""', 'Error cleaning up temporary files', '""', ')']",Clean up temporary directories and log any errors encountered,"['Clean', 'up', 'temporary', 'directories', 'and', 'log', 'any', 'errors', 'encountered']"
214,"def print_validation_result(result: dict, rel_path: Path):
    
    print(f""\nValidating: {rel_path}"")
    if ""error"" in result:
        print(f""Error: {result['error']}"")
    else:
        print(f""Duration: {result['duration']}"")
        print(f""Sample Rate: {result['sample_rate']} Hz"")
        print(f""Peak Amplitude: {result['peak_amplitude']}"")
        print(f""RMS Level: {result['rms_level']}"")
        print(f""DC Offset: {result['dc_offset']}"")

        if result[""issues""]:
            print(""\nIssues Found:"")
            for issue in result[""issues""]:
                print(f""- {issue}"")
        else:
            print(""\nNo issues found"")","['def', 'print_validation_result', '(', 'result', ':', 'dict', ',', 'rel_path', ':', 'Path', ')', ':', 'print', '(', 'f', '""', '\\n', 'Validating: ', '{', 'rel_path', '}', '""', ')', 'if', '""', 'error', '""', 'in', 'result', ':', 'print', '(', 'f', '""', 'Error: ', '{', 'result', '[', ""'"", 'error', ""'"", ']', '}', '""', ')', 'else', ':', 'print', '(', 'f', '""', 'Duration: ', '{', 'result', '[', ""'"", 'duration', ""'"", ']', '}', '""', ')', 'print', '(', 'f', '""', 'Sample Rate: ', '{', 'result', '[', ""'"", 'sample_rate', ""'"", ']', '}', ' Hz', '""', ')', 'print', '(', 'f', '""', 'Peak Amplitude: ', '{', 'result', '[', ""'"", 'peak_amplitude', ""'"", ']', '}', '""', ')', 'print', '(', 'f', '""', 'RMS Level: ', '{', 'result', '[', ""'"", 'rms_level', ""'"", ']', '}', '""', ')', 'print', '(', 'f', '""', 'DC Offset: ', '{', 'result', '[', ""'"", 'dc_offset', ""'"", ']', '}', '""', ')', 'if', 'result', '[', '""', 'issues', '""', ']', ':', 'print', '(', '""', '\\n', 'Issues Found:', '""', ')', 'for', 'issue', 'in', 'result', '[', '""', 'issues', '""', ']', ':', 'print', '(', 'f', '""', '- ', '{', 'issue', '}', '""', ')', 'else', ':', 'print', '(', '""', '\\n', 'No issues found', '""', ')']","Display audio validation results, highlighting errors, metrics, and potential issues.","['Display', 'audio', 'validation', 'results', ',', 'highlighting', 'errors', ',', 'metrics', ',', 'and', 'potential', 'issues', '.']"
215,"def get_api_key(self, context: SkillContext) -> Optional[str]:
        
        # Check agent config first
        agent_api_key = context.config.get(""api_key"")
        if agent_api_key:
            logger.debug(f""Using agent-specific Venice API key for skill {self.name}"")
            return agent_api_key

        # Fallback to system config
        system_api_key = self.skill_store.get_system_config(""venice_api_key"")
        if system_api_key:
            logger.debug(f""Using system Venice API key for skill {self.name}"")
            return system_api_key

        logger.warning(
            f""No Venice API key found in agent or system config for skill {self.name}""
        )
        return None","['def', 'get_api_key', '(', 'self', ',', 'context', ':', 'SkillContext', ')', '-', '>', 'Optional', '[', 'str', ']', ':', '# Check agent config first', 'agent_api_key', '=', 'context', '.', 'config', '.', 'get', '(', '""', 'api_key', '""', ')', 'if', 'agent_api_key', ':', 'logger', '.', 'debug', '(', 'f', '""', 'Using agent-specific Venice API key for skill ', '{', 'self', '.', 'name', '}', '""', ')', 'return', 'agent_api_key', '# Fallback to system config', 'system_api_key', '=', 'self', '.', 'skill_store', '.', 'get_system_config', '(', '""', 'venice_api_key', '""', ')', 'if', 'system_api_key', ':', 'logger', '.', 'debug', '(', 'f', '""', 'Using system Venice API key for skill ', '{', 'self', '.', 'name', '}', '""', ')', 'return', 'system_api_key', 'logger', '.', 'warning', '(', 'f', '""', 'No Venice API key found in agent or system config for skill ', '{', 'self', '.', 'name', '}', '""', ')', 'return', 'None']",Retrieve API key from agent or system configuration for a specific skill,"['Retrieve', 'API', 'key', 'from', 'agent', 'or', 'system', 'configuration', 'for', 'a', 'specific', 'skill']"
218,"def process_revision_directives(context, revision, directives):
    
    for directive in directives:
        if isinstance(directive, ops.MigrationScript):
            # Get the current timestamp
            timestamp = time.strftime(""%Y%m%d%H%M%S"")
            # Modify the revision ID to include the timestamp
            directive.rev_id = f""{timestamp}_{directive.rev_id}""","['def', 'process_revision_directives', '(', 'context', ',', 'revision', ',', 'directives', ')', ':', 'for', 'directive', 'in', 'directives', ':', 'if', 'isinstance', '(', 'directive', ',', 'ops', '.', 'MigrationScript', ')', ':', '# Get the current timestamp', 'timestamp', '=', 'time', '.', 'strftime', '(', '""', '%', 'Y', '%', 'm', '%d', '%', 'H', '%', 'M', '%', 'S', '""', ')', '# Modify the revision ID to include the timestamp', 'directive', '.', 'rev_id', '=', 'f', '""', '{', 'timestamp', '}', '_', '{', 'directive', '.', 'rev_id', '}', '""']",Update migration script IDs with timestamps in revision directives.,"['Update', 'migration', 'script', 'IDs', 'with', 'timestamps', 'in', 'revision', 'directives', '.']"
220,"def delete_subscription_by_stripe_id(db_session: Session, stripe_subscription_id: str) -> None:
    
    subscription = get_subscription_by_stripe_id(db_session, stripe_subscription_id)
    if not subscription:
        logger.warning(
            f""Subscription not found for stripe_subscription_id={stripe_subscription_id} during delete attempt.""
        )
        return

    db_session.delete(subscription)
    db_session.flush()
    logger.info(
        ""Deleted subscription"",
        extra={""stripe_subscription_id"": stripe_subscription_id},
    )","['def', 'delete_subscription_by_stripe_id', '(', 'db_session', ':', 'Session', ',', 'stripe_subscription_id', ':', 'str', ')', '-', '>', 'None', ':', 'subscription', '=', 'get_subscription_by_stripe_id', '(', 'db_session', ',', 'stripe_subscription_id', ')', 'if', 'not', 'subscription', ':', 'logger', '.', 'warning', '(', 'f', '""', 'Subscription not found for stripe_subscription_id=', '{', 'stripe_subscription_id', '}', ' during delete attempt.', '""', ')', 'return', 'db_session', '.', 'delete', '(', 'subscription', ')', 'db_session', '.', 'flush', '(', ')', 'logger', '.', 'info', '(', '""', 'Deleted subscription', '""', ',', 'extra', '=', '{', '""', 'stripe_subscription_id', '""', ':', 'stripe_subscription_id', '}', ',', ')']",Remove subscription from database using Stripe ID if it exists,"['Remove', 'subscription', 'from', 'database', 'using', 'Stripe', 'ID', 'if', 'it', 'exists']"
221,"def _safe_save(self, output_dir: str):
        
        state_dict = self.model.state_dict()
        if self.args.should_save:
            cpu_state_dict = {
                key: value.cpu()
                for key, value in state_dict.items()
            }
            del state_dict
            self._save(output_dir, state_dict=cpu_state_dict)","['def', '_safe_save', '(', 'self', ',', 'output_dir', ':', 'str', ')', ':', 'state_dict', '=', 'self', '.', 'model', '.', 'state_dict', '(', ')', 'if', 'self', '.', 'args', '.', 'should_save', ':', 'cpu_state_dict', '=', '{', 'key', ':', 'value', '.', 'cpu', '(', ')', 'for', 'key', ',', 'value', 'in', 'state_dict', '.', 'items', '(', ')', '}', 'del', 'state_dict', 'self', '.', '_save', '(', 'output_dir', ',', 'state_dict', '=', 'cpu_state_dict', ')']",Safely save model state to specified directory if saving is enabled.,"['Safely', 'save', 'model', 'state', 'to', 'specified', 'directory', 'if', 'saving', 'is', 'enabled', '.']"
222,"def __str__(self):
        
        error_message = ""({0})\n"" ""Reason: {1}\n"".format(self.status, self.reason)
        if self.headers:
            error_message += ""HTTP response headers: {0}\n"".format(self.headers)

        if self.body:
            error_message += ""HTTP response body: {0}\n"".format(self.body)

        return error_message","['def', '__str__', '(', 'self', ')', ':', 'error_message', '=', '""', '(', '{0}', ')', '\\n', '""', '""', 'Reason: ', '{1}', '\\n', '""', '.', 'format', '(', 'self', '.', 'status', ',', 'self', '.', 'reason', ')', 'if', 'self', '.', 'headers', ':', 'error_message', '+', '=', '""', 'HTTP response headers: ', '{0}', '\\n', '""', '.', 'format', '(', 'self', '.', 'headers', ')', 'if', 'self', '.', 'body', ':', 'error_message', '+', '=', '""', 'HTTP response body: ', '{0}', '\\n', '""', '.', 'format', '(', 'self', '.', 'body', ')', 'return', 'error_message']",Generate a formatted error message from HTTP response details.,"['Generate', 'a', 'formatted', 'error', 'message', 'from', 'HTTP', 'response', 'details', '.']"
225,"def execute_method(self, method: Union[str, bytes], *args, **kwargs):
        
        if self.vllm_tp_rank == 0 and method != ""execute_model"":
            print(f""[DP={self.vllm_dp_rank},TP={self.vllm_tp_rank}] execute_method: {method if isinstance(method, str) else 'Callable'}"")
        return self.rollout.execute_method(method, *args, **kwargs)","['def', 'execute_method', '(', 'self', ',', 'method', ':', 'Union', '[', 'str', ',', 'bytes', ']', ',', '*', 'args', ',', '*', '*', 'kwargs', ')', ':', 'if', 'self', '.', 'vllm_tp_rank', '==', '0', 'and', 'method', '!=', '""', 'execute_model', '""', ':', 'print', '(', 'f', '""', '[DP=', '{', 'self', '.', 'vllm_dp_rank', '}', ',TP=', '{', 'self', '.', 'vllm_tp_rank', '}', '] execute_method: ', '{', 'method', 'if', 'isinstance', '(', 'method', ',', 'str', ')', 'else', ""'"", 'Callable', ""'"", '}', '""', ')', 'return', 'self', '.', 'rollout', '.', 'execute_method', '(', 'method', ',', '*', 'args', ',', '*', '*', 'kwargs', ')']",Execute a method with logging for specific conditions and delegate execution,"['Execute', 'a', 'method', 'with', 'logging', 'for', 'specific', 'conditions', 'and', 'delegate', 'execution']"
226,"def visualize_scroll(self, direction: str, clicks: int, img_base64: str) -> None:
        
        if (
            not self.agent.save_trajectory
            or not hasattr(self.agent, ""experiment_manager"")
            or not self.agent.experiment_manager
        ):
            return

        try:
            # Use the visualization utility
            img = visualize_scroll(direction, clicks, img_base64)

            # Save the visualization
            self.agent.experiment_manager.save_action_visualization(
                img, ""scroll"", f""{direction}_{clicks}""
            )
        except Exception as e:
            logger.error(f""Error visualizing scroll: {str(e)}"")","['def', 'visualize_scroll', '(', 'self', ',', 'direction', ':', 'str', ',', 'clicks', ':', 'int', ',', 'img_base64', ':', 'str', ')', '-', '>', 'None', ':', 'if', '(', 'not', 'self', '.', 'agent', '.', 'save_trajectory', 'or', 'not', 'hasattr', '(', 'self', '.', 'agent', ',', '""', 'experiment_manager', '""', ')', 'or', 'not', 'self', '.', 'agent', '.', 'experiment_manager', ')', ':', 'return', 'try', ':', '# Use the visualization utility', 'img', '=', 'visualize_scroll', '(', 'direction', ',', 'clicks', ',', 'img_base64', ')', '# Save the visualization', 'self', '.', 'agent', '.', 'experiment_manager', '.', 'save_action_visualization', '(', 'img', ',', '""', 'scroll', '""', ',', 'f', '""', '{', 'direction', '}', '_', '{', 'clicks', '}', '""', ')', 'except', 'Exception', 'as', 'e', ':', 'logger', '.', 'error', '(', 'f', '""', 'Error visualizing scroll: ', '{', 'str', '(', 'e', ')', '}', '""', ')']",Visualize and save scroll action using base64 image data if conditions are met.,"['Visualize', 'and', 'save', 'scroll', 'action', 'using', 'base64', 'image', 'data', 'if', 'conditions', 'are', 'met', '.']"
228,"def format_search_results(self, results: list[tuple[BaseTool, str, float]]) -> str:
        

        # Only show top_k results
        results = results

        formatted_output = ""Search results\n\n""

        for i, (tool, server_name, score) in enumerate(results):
            # Format score as percentage
            if i < 5:
                score_pct = f""{score * 100:.1f}%""
                logger.info(f""{i}: {tool.name} ({score_pct} match)"")
            formatted_output += f""[{i + 1}] Tool: {tool.name} ({score_pct} match)\n""
            formatted_output += f""    Server: {server_name}\n""
            formatted_output += f""    Description: {tool.description}\n\n""

        # Add footer with information about how to use the results
        formatted_output += (
            ""\nTo use a tool, connect to the appropriate server first, then invoke the tool.""
        )

        return formatted_output","['def', 'format_search_results', '(', 'self', ',', 'results', ':', 'list', '[', 'tuple', '[', 'BaseTool', ',', 'str', ',', 'float', ']', ']', ')', '-', '>', 'str', ':', '# Only show top_k results', 'results', '=', 'results', 'formatted_output', '=', '""', 'Search results', '\\n', '\\n', '""', 'for', 'i', ',', '(', 'tool', ',', 'server_name', ',', 'score', ')', 'in', 'enumerate', '(', 'results', ')', ':', '# Format score as percentage', 'if', 'i', '<', '5', ':', 'score_pct', '=', 'f', '""', '{', 'score', '*', '100', ':', '.1f', '}', '%', '""', 'logger', '.', 'info', '(', 'f', '""', '{', 'i', '}', ': ', '{', 'tool', '.', 'name', '}', ' (', '{', 'score_pct', '}', ' match)', '""', ')', 'formatted_output', '+', '=', 'f', '""', '[', '{', 'i', '+', '1', '}', '] Tool: ', '{', 'tool', '.', 'name', '}', ' (', '{', 'score_pct', '}', ' match)', '\\n', '""', 'formatted_output', '+', '=', 'f', '""', '    Server: ', '{', 'server_name', '}', '\\n', '""', 'formatted_output', '+', '=', 'f', '""', '    Description: ', '{', 'tool', '.', 'description', '}', '\\n', '\\n', '""', '# Add footer with information about how to use the results', 'formatted_output', '+', '=', '(', '""', '\\n', 'To use a tool, connect to the appropriate server first, then invoke the tool.', '""', ')', 'return', 'formatted_output']",Format and display top search results with tool details and usage instructions.,"['Format', 'and', 'display', 'top', 'search', 'results', 'with', 'tool', 'details', 'and', 'usage', 'instructions', '.']"
230,"def _is_supported(self, file_path: str) -> bool:
        
        ext = Path(file_path).suffix.lower()
        if ext not in self.SUPPORTED_EXTENSIONS:
            logger.warning(f""Unsupported file extension: {ext} for file: {file_path}"")
            return False
        return True","['def', '_is_supported', '(', 'self', ',', 'file_path', ':', 'str', ')', '-', '>', 'bool', ':', 'ext', '=', 'Path', '(', 'file_path', ')', '.', 'suffix', '.', 'lower', '(', ')', 'if', 'ext', 'not', 'in', 'self', '.', 'SUPPORTED_EXTENSIONS', ':', 'logger', '.', 'warning', '(', 'f', '""', 'Unsupported file extension: ', '{', 'ext', '}', ' for file: ', '{', 'file_path', '}', '""', ')', 'return', 'False', 'return', 'True']","Check if a file's extension is within supported types, logging warnings otherwise.","['Check', 'if', 'a', 'file', ""'s"", 'extension', 'is', 'within', 'supported', 'types', ',', 'logging', 'warnings', 'otherwise', '.']"
231,"def generate_diff_html(a, b):
    
    seq_matcher = SequenceMatcher(None, a, b)
    output_html = """"
    for opcode, a0, a1, b0, b1 in seq_matcher.get_opcodes():
        if opcode == ""equal"":
            output_html += a[a0:a1]
        elif opcode == ""insert"":
            output_html += f""<span class='added'>{b[b0:b1]}</span>""
        elif opcode == ""delete"":
            output_html += f""<span class='removed'>{a[a0:a1]}</span>""
        elif opcode == ""replace"":
            output_html += f""<span class='removed'>{a[a0:a1]}</span><span class='added'>{b[b0:b1]}</span>""
    return output_html","['def', 'generate_diff_html', '(', 'a', ',', 'b', ')', ':', 'seq_matcher', '=', 'SequenceMatcher', '(', 'None', ',', 'a', ',', 'b', ')', 'output_html', '=', '""', '""', 'for', 'opcode', ',', 'a0', ',', 'a1', ',', 'b0', ',', 'b1', 'in', 'seq_matcher', '.', 'get_opcodes', '(', ')', ':', 'if', 'opcode', '==', '""', 'equal', '""', ':', 'output_html', '+', '=', 'a', '[', 'a0', ':', 'a1', ']', 'elif', 'opcode', '==', '""', 'insert', '""', ':', 'output_html', '+', '=', 'f', '""', '<span class=', ""'"", 'added', ""'"", '>', '{', 'b', '[', 'b0', ':', 'b1', ']', '}', '</span>', '""', 'elif', 'opcode', '==', '""', 'delete', '""', ':', 'output_html', '+', '=', 'f', '""', '<span class=', ""'"", 'removed', ""'"", '>', '{', 'a', '[', 'a0', ':', 'a1', ']', '}', '</span>', '""', 'elif', 'opcode', '==', '""', 'replace', '""', ':', 'output_html', '+', '=', 'f', '""', '<span class=', ""'"", 'removed', ""'"", '>', '{', 'a', '[', 'a0', ':', 'a1', ']', '}', '</span><span class=', ""'"", 'added', ""'"", '>', '{', 'b', '[', 'b0', ':', 'b1', ']', '}', '</span>', '""', 'return', 'output_html']",Generate HTML highlighting differences between two text sequences.,"['Generate', 'HTML', 'highlighting', 'differences', 'between', 'two', 'text', 'sequences', '.']"
233,"def _build_docker_image(self):
    
    if not self.docker_path:
      raise ValueError('Docker path is not set.')
    if not os.path.exists(self.docker_path):
      raise FileNotFoundError(f'Invalid Docker path: {self.docker_path}')

    logger.info('Building Docker image...')
    self._client.images.build(
        path=self.docker_path,
        tag=self.image,
        rm=True,
    )
    logger.info('Docker image: %s built.', self.image)","['def', '_build_docker_image', '(', 'self', ')', ':', 'if', 'not', 'self', '.', 'docker_path', ':', 'raise', 'ValueError', '(', ""'"", 'Docker path is not set.', ""'"", ')', 'if', 'not', 'os', '.', 'path', '.', 'exists', '(', 'self', '.', 'docker_path', ')', ':', 'raise', 'FileNotFoundError', '(', 'f', ""'"", 'Invalid Docker path: ', '{', 'self', '.', 'docker_path', '}', ""'"", ')', 'logger', '.', 'info', '(', ""'"", 'Building Docker image...', ""'"", ')', 'self', '.', '_client', '.', 'images', '.', 'build', '(', 'path', '=', 'self', '.', 'docker_path', ',', 'tag', '=', 'self', '.', 'image', ',', 'rm', '=', 'True', ',', ')', 'logger', '.', 'info', '(', ""'"", 'Docker image: ', '%s', ' built.', ""'"", ',', 'self', '.', 'image', ')']","Builds a Docker image from a specified path, logging progress and errors.","['Builds', 'a', 'Docker', 'image', 'from', 'a', 'specified', 'path', ',', 'logging', 'progress', 'and', 'errors', '.']"
234,"def _create_relevant_node(
        self, node: Dict[str, Any], relevance_score: float, reason: str
    ) -> Dict[str, Any]:
        
        relevant_node = {
            ""id"": node[""id""],
            ""name"": node[""name""],
            ""type"": node[""type""],
            ""relevance_score"": relevance_score,
            ""reason"": reason,
            ""children"": [],
        }

        for field in [""file_path"", ""start_line"", ""end_line"", ""relationship""]:
            if field in node:
                relevant_node[field] = node[field]

        return relevant_node","['def', '_create_relevant_node', '(', 'self', ',', 'node', ':', 'Dict', '[', 'str', ',', 'Any', ']', ',', 'relevance_score', ':', 'float', ',', 'reason', ':', 'str', ')', '-', '>', 'Dict', '[', 'str', ',', 'Any', ']', ':', 'relevant_node', '=', '{', '""', 'id', '""', ':', 'node', '[', '""', 'id', '""', ']', ',', '""', 'name', '""', ':', 'node', '[', '""', 'name', '""', ']', ',', '""', 'type', '""', ':', 'node', '[', '""', 'type', '""', ']', ',', '""', 'relevance_score', '""', ':', 'relevance_score', ',', '""', 'reason', '""', ':', 'reason', ',', '""', 'children', '""', ':', '[', ']', ',', '}', 'for', 'field', 'in', '[', '""', 'file_path', '""', ',', '""', 'start_line', '""', ',', '""', 'end_line', '""', ',', '""', 'relationship', '""', ']', ':', 'if', 'field', 'in', 'node', ':', 'relevant_node', '[', 'field', ']', '=', 'node', '[', 'field', ']', 'return', 'relevant_node']",Constructs a node dictionary with relevance metadata and optional fields.,"['Constructs', 'a', 'node', 'dictionary', 'with', 'relevance', 'metadata', 'and', 'optional', 'fields', '.']"
235,"def get_speaker_selection_result(self, groupchat: ""GroupChat"") -> Optional[Union[Agent, str]]:
        
        if self.agent_name is not None:
            # Find the agent by name in the groupchat
            for agent in groupchat.agents:
                if agent.name == self.agent_name:
                    return agent
            raise ValueError(f""Agent '{self.agent_name}' not found in groupchat."")
        elif self.speaker_selection_method is not None:
            return self.speaker_selection_method
        elif self.terminate is not None and self.terminate:
            return None
        else:
            raise ValueError(
                ""Unable to establish speaker selection result. No terminate, agent, or speaker selection method provided.""
            )","['def', 'get_speaker_selection_result', '(', 'self', ',', 'groupchat', ':', '""', 'GroupChat', '""', ')', '-', '>', 'Optional', '[', 'Union', '[', 'Agent', ',', 'str', ']', ']', ':', 'if', 'self', '.', 'agent_name', 'is', 'not', 'None', ':', '# Find the agent by name in the groupchat', 'for', 'agent', 'in', 'groupchat', '.', 'agents', ':', 'if', 'agent', '.', 'name', '==', 'self', '.', 'agent_name', ':', 'return', 'agent', 'raise', 'ValueError', '(', 'f', '""', 'Agent ', ""'"", '{', 'self', '.', 'agent_name', '}', ""'"", ' not found in groupchat.', '""', ')', 'elif', 'self', '.', 'speaker_selection_method', 'is', 'not', 'None', ':', 'return', 'self', '.', 'speaker_selection_method', 'elif', 'self', '.', 'terminate', 'is', 'not', 'None', 'and', 'self', '.', 'terminate', ':', 'return', 'None', 'else', ':', 'raise', 'ValueError', '(', '""', 'Unable to establish speaker selection result. No terminate, agent, or speaker selection method provided.', '""', ')']",Determine speaker selection in group chat based on agent name or method,"['Determine', 'speaker', 'selection', 'in', 'group', 'chat', 'based', 'on', 'agent', 'name', 'or', 'method']"
236,"def file_uri_to_path(file_uri: str) -> Tuple[str | None, str]:
    
    parsed = urlparse(file_uri)
    if parsed.scheme != ""file"":
        raise ValueError(f""Not a file URL: {file_uri}"")

    netloc = parsed.netloc if parsed.netloc else None
    path = os.path.abspath(url2pathname(parsed.path))
    return netloc, path","['def', 'file_uri_to_path', '(', 'file_uri', ':', 'str', ')', '-', '>', 'Tuple', '[', 'str', '|', 'None', ',', 'str', ']', ':', 'parsed', '=', 'urlparse', '(', 'file_uri', ')', 'if', 'parsed', '.', 'scheme', '!=', '""', 'file', '""', ':', 'raise', 'ValueError', '(', 'f', '""', 'Not a file URL: ', '{', 'file_uri', '}', '""', ')', 'netloc', '=', 'parsed', '.', 'netloc', 'if', 'parsed', '.', 'netloc', 'else', 'None', 'path', '=', 'os', '.', 'path', '.', 'abspath', '(', 'url2pathname', '(', 'parsed', '.', 'path', ')', ')', 'return', 'netloc', ',', 'path']",Convert file URI to local file path and network location.,"['Convert', 'file', 'URI', 'to', 'local', 'file', 'path', 'and', 'network', 'location', '.']"
238,"def context(self) -> ""Context"":
        
        # First try instance context
        if self._context is not None:
            return self._context

        try:
            # Fall back to global context if available
            from mcp_agent.core.context import get_current_context

            return get_current_context()
        except Exception as e:
            raise RuntimeError(
                f""No context available for {self.__class__.__name__}. ""
                ""Either initialize MCPApp first or pass context explicitly.""
            ) from e","['def', 'context', '(', 'self', ')', '-', '>', '""', 'Context', '""', ':', '# First try instance context', 'if', 'self', '.', '_context', 'is', 'not', 'None', ':', 'return', 'self', '.', '_context', 'try', ':', '# Fall back to global context if available', 'from', 'mcp_agent', '.', 'core', '.', 'context', 'import', 'get_current_context', 'return', 'get_current_context', '(', ')', 'except', 'Exception', 'as', 'e', ':', 'raise', 'RuntimeError', '(', 'f', '""', 'No context available for ', '{', 'self', '.', '__class__', '.', '__name__', '}', '. ', '""', '""', 'Either initialize MCPApp first or pass context explicitly.', '""', ')', 'from', 'e']","Retrieve instance or global context, raising error if unavailable.","['Retrieve', 'instance', 'or', 'global', 'context', ',', 'raising', 'error', 'if', 'unavailable', '.']"
241,"def update_presigned_url(self, presigned_url, base_url):
        
        #To Do: If Proxy URL has domain name how do we handle such cases
        
        presigned_parts = urlparse(presigned_url)
        base_parts = urlparse(base_url)
        # Check if base_url contains localhost or an IP address
        if re.match(r'^(localhost|\d{1,3}(\.\d{1,3}){3})$', base_parts.hostname):
            new_netloc = base_parts.hostname  # Extract domain from base_url
            if base_parts.port:  # Add port if present in base_url
                new_netloc += f"":{base_parts.port}""
            updated_parts = presigned_parts._replace(netloc=new_netloc)
            return urlunparse(updated_parts)
        return presigned_url","['def', 'update_presigned_url', '(', 'self', ',', 'presigned_url', ',', 'base_url', ')', ':', '#To Do: If Proxy URL has domain name how do we handle such cases', 'presigned_parts', '=', 'urlparse', '(', 'presigned_url', ')', 'base_parts', '=', 'urlparse', '(', 'base_url', ')', '# Check if base_url contains localhost or an IP address', 'if', 're', '.', 'match', '(', 'r', ""'"", '^(localhost|', '\\', 'd', '{', '1,3}(', '\\', '.', '\\', 'd', '{', '1,3})', '{3}', ')$', ""'"", ',', 'base_parts', '.', 'hostname', ')', ':', 'new_netloc', '=', 'base_parts', '.', 'hostname', '# Extract domain from base_url', 'if', 'base_parts', '.', 'port', ':', '# Add port if present in base_url', 'new_netloc', '+', '=', 'f', '""', ':', '{', 'base_parts', '.', 'port', '}', '""', 'updated_parts', '=', 'presigned_parts', '.', '_replace', '(', 'netloc', '=', 'new_netloc', ')', 'return', 'urlunparse', '(', 'updated_parts', ')', 'return', 'presigned_url']",Update presigned URL's domain to match base URL if it's localhost or IP.,"['Update', 'presigned', 'URL', ""'s"", 'domain', 'to', 'match', 'base', 'URL', 'if', 'it', ""'s"", 'localhost', 'or', 'IP', '.']"
242,"def get_audio_files(df):
    
    audios = []
    for index, row in df.iterrows():
        number = row['number']
        line_count = len(eval(row['lines']) if isinstance(row['lines'], str) else row['lines'])
        for line_index in range(line_count):
            temp_file = OUTPUT_FILE_TEMPLATE.format(f""{number}_{line_index}"")
            audios.append(temp_file)
    return audios","['def', 'get_audio_files', '(', 'df', ')', ':', 'audios', '=', '[', ']', 'for', 'index', ',', 'row', 'in', 'df', '.', 'iterrows', '(', ')', ':', 'number', '=', 'row', '[', ""'"", 'number', ""'"", ']', 'line_count', '=', 'len', '(', 'eval', '(', 'row', '[', ""'"", 'lines', ""'"", ']', ')', 'if', 'isinstance', '(', 'row', '[', ""'"", 'lines', ""'"", ']', ',', 'str', ')', 'else', 'row', '[', ""'"", 'lines', ""'"", ']', ')', 'for', 'line_index', 'in', 'range', '(', 'line_count', ')', ':', 'temp_file', '=', 'OUTPUT_FILE_TEMPLATE', '.', 'format', '(', 'f', '""', '{', 'number', '}', '_', '{', 'line_index', '}', '""', ')', 'audios', '.', 'append', '(', 'temp_file', ')', 'return', 'audios']",Generate audio file paths based on data frame content,"['Generate', 'audio', 'file', 'paths', 'based', 'on', 'data', 'frame', 'content']"
243,"def resolve(self, override: ModelSettings | None) -> ModelSettings:
        
        if override is None:
            return self

        changes = {
            field.name: getattr(override, field.name)
            for field in fields(self)
            if getattr(override, field.name) is not None
        }
        return replace(self, **changes)","['def', 'resolve', '(', 'self', ',', 'override', ':', 'ModelSettings', '|', 'None', ')', '-', '>', 'ModelSettings', ':', 'if', 'override', 'is', 'None', ':', 'return', 'self', 'changes', '=', '{', 'field', '.', 'name', ':', 'getattr', '(', 'override', ',', 'field', '.', 'name', ')', 'for', 'field', 'in', 'fields', '(', 'self', ')', 'if', 'getattr', '(', 'override', ',', 'field', '.', 'name', ')', 'is', 'not', 'None', '}', 'return', 'replace', '(', 'self', ',', '*', '*', 'changes', ')']",Merge default settings with overrides to produce updated configuration.,"['Merge', 'default', 'settings', 'with', 'overrides', 'to', 'produce', 'updated', 'configuration', '.']"
245,"def _format_child_blocks(
        self, block_content: dict, block: dict, block_type: str, page_breadcrumbs: List[Breadcrumb]
    ) -> str:
        
        if block_type == ""child_page"":
            title = block_content.get(""title"", ""Untitled Page"")
            return f""é¦ƒæ« **[{title}]** (Child Page)""
        else:  # child_database
            return self._format_child_database_block(block_content, block, page_breadcrumbs)","['def', '_format_child_blocks', '(', 'self', ',', 'block_content', ':', 'dict', ',', 'block', ':', 'dict', ',', 'block_type', ':', 'str', ',', 'page_breadcrumbs', ':', 'List', '[', 'Breadcrumb', ']', ')', '-', '>', 'str', ':', 'if', 'block_type', '==', '""', 'child_page', '""', ':', 'title', '=', 'block_content', '.', 'get', '(', '""', 'title', '""', ',', '""', 'Untitled Page', '""', ')', 'return', 'f', '""', 'é¦ƒæ« **[', '{', 'title', '}', ']** (Child Page)', '""', 'else', ':', '# child_database', 'return', 'self', '.', '_format_child_database_block', '(', 'block_content', ',', 'block', ',', 'page_breadcrumbs', ')']",Formats child blocks as either a page or database based on type.,"['Formats', 'child', 'blocks', 'as', 'either', 'a', 'page', 'or', 'database', 'based', 'on', 'type', '.']"
247,"def get_api_version(cls) -> str:
        
        logger.debug(""Retrieving DeepSeek Azure API version from config"")
        provider_config = CONFIG.llm_endpoints.get(""deepseek_azure"")
        if provider_config and provider_config.api_version:
            logger.debug(f""DeepSeek Azure API version: {provider_config.api_version}"")
            return provider_config.api_version
        logger.warning(""DeepSeek Azure API version not found in config"")
        return None","['def', 'get_api_version', '(', 'cls', ')', '-', '>', 'str', ':', 'logger', '.', 'debug', '(', '""', 'Retrieving DeepSeek Azure API version from config', '""', ')', 'provider_config', '=', 'CONFIG', '.', 'llm_endpoints', '.', 'get', '(', '""', 'deepseek_azure', '""', ')', 'if', 'provider_config', 'and', 'provider_config', '.', 'api_version', ':', 'logger', '.', 'debug', '(', 'f', '""', 'DeepSeek Azure API version: ', '{', 'provider_config', '.', 'api_version', '}', '""', ')', 'return', 'provider_config', '.', 'api_version', 'logger', '.', 'warning', '(', '""', 'DeepSeek Azure API version not found in config', '""', ')', 'return', 'None']",Retrieve and log the DeepSeek Azure API version from configuration settings.,"['Retrieve', 'and', 'log', 'the', 'DeepSeek', 'Azure', 'API', 'version', 'from', 'configuration', 'settings', '.']"
248,"def _update_analyze_status_failed(self, doc_id: int) -> None:
        
        try:
            with self._repository._db.session() as session:
                document = session.get(self._repository.model, doc_id)
                if document:
                    document.analyze_status = ProcessStatus.FAILED
                    session.commit()
                    logger.debug(f""Updated analyze status for document {doc_id} to FAILED"")
                else:
                    logger.warning(f""Document not found with id: {doc_id}"")
        except Exception as e:
            logger.error(f""Error updating document analyze status: {str(e)}"")","['def', '_update_analyze_status_failed', '(', 'self', ',', 'doc_id', ':', 'int', ')', '-', '>', 'None', ':', 'try', ':', 'with', 'self', '.', '_repository', '.', '_db', '.', 'session', '(', ')', 'as', 'session', ':', 'document', '=', 'session', '.', 'get', '(', 'self', '.', '_repository', '.', 'model', ',', 'doc_id', ')', 'if', 'document', ':', 'document', '.', 'analyze_status', '=', 'ProcessStatus', '.', 'FAILED', 'session', '.', 'commit', '(', ')', 'logger', '.', 'debug', '(', 'f', '""', 'Updated analyze status for document ', '{', 'doc_id', '}', ' to FAILED', '""', ')', 'else', ':', 'logger', '.', 'warning', '(', 'f', '""', 'Document not found with id: ', '{', 'doc_id', '}', '""', ')', 'except', 'Exception', 'as', 'e', ':', 'logger', '.', 'error', '(', 'f', '""', 'Error updating document analyze status: ', '{', 'str', '(', 'e', ')', '}', '""', ')']",Update document status to FAILED in database and log the outcome,"['Update', 'document', 'status', 'to', 'FAILED', 'in', 'database', 'and', 'log', 'the', 'outcome']"
249,"def matches(self, uri: str) -> dict[str, Any] | None:
        
        # Convert template to regex pattern
        pattern = self.uri_template.replace(""{"", ""(?P<"").replace(""}"", "">[^/]+)"")
        match = re.match(f""^{pattern}$"", uri)
        if match:
            return match.groupdict()
        return None","['def', 'matches', '(', 'self', ',', 'uri', ':', 'str', ')', '-', '>', 'dict', '[', 'str', ',', 'Any', ']', '|', 'None', ':', '# Convert template to regex pattern', 'pattern', '=', 'self', '.', 'uri_template', '.', 'replace', '(', '""', '{', '""', ',', '""', '(?P<', '""', ')', '.', 'replace', '(', '""', '}', '""', ',', '""', '>[^/]+)', '""', ')', 'match', '=', 're', '.', 'match', '(', 'f', '""', '^', '{', 'pattern', '}', '$', '""', ',', 'uri', ')', 'if', 'match', ':', 'return', 'match', '.', 'groupdict', '(', ')', 'return', 'None']",Extracts parameters from a URI using a template-based regex pattern.,"['Extracts', 'parameters', 'from', 'a', 'URI', 'using', 'a', 'template-based', 'regex', 'pattern', '.']"
250,"def _retry_if_not_cancelled_and_failed(retry_state):
    
    if retry_state.outcome.failed:
        exception = retry_state.outcome.exception()
        # Don't retry on CancelledError
        if isinstance(exception, asyncio.CancelledError):
            logger.debug(""Operation was cancelled, not retrying"")
            return False
        # Retry on network related errors
        if isinstance(
            exception, httpx.HTTPError | ConnectionError | ValueError | TimeoutError
        ):
            logger.warning(f""Network error occurred: {exception}, will retry"")
            return True
    # Don't retry on success
    return False","['def', '_retry_if_not_cancelled_and_failed', '(', 'retry_state', ')', ':', 'if', 'retry_state', '.', 'outcome', '.', 'failed', ':', 'exception', '=', 'retry_state', '.', 'outcome', '.', 'exception', '(', ')', ""# Don't retry on CancelledError"", 'if', 'isinstance', '(', 'exception', ',', 'asyncio', '.', 'CancelledError', ')', ':', 'logger', '.', 'debug', '(', '""', 'Operation was cancelled, not retrying', '""', ')', 'return', 'False', '# Retry on network related errors', 'if', 'isinstance', '(', 'exception', ',', 'httpx', '.', 'HTTPError', '|', 'ConnectionError', '|', 'ValueError', '|', 'TimeoutError', ')', ':', 'logger', '.', 'warning', '(', 'f', '""', 'Network error occurred: ', '{', 'exception', '}', ', will retry', '""', ')', 'return', 'True', ""# Don't retry on success"", 'return', 'False']",Determine retry eligibility based on operation failure and exception type,"['Determine', 'retry', 'eligibility', 'based', 'on', 'operation', 'failure', 'and', 'exception', 'type']"
251,"def validate_config_values(self):
        
        for key, value in self.__dict__.items():
            if isinstance(value, dict):
                raise ValueError(f""Value for '{key}' must not be a dictionary (depth 0 only)"")
        return self","['def', 'validate_config_values', '(', 'self', ')', ':', 'for', 'key', ',', 'value', 'in', 'self', '.', '__dict__', '.', 'items', '(', ')', ':', 'if', 'isinstance', '(', 'value', ',', 'dict', ')', ':', 'raise', 'ValueError', '(', 'f', '""', 'Value for ', ""'"", '{', 'key', '}', ""'"", ' must not be a dictionary (depth 0 only)', '""', ')', 'return', 'self']",Ensure configuration values are not dictionaries at the top level.,"['Ensure', 'configuration', 'values', 'are', 'not', 'dictionaries', 'at', 'the', 'top', 'level', '.']"
255,"def from_runnable_config(
        cls, config: Optional[RunnableConfig] = None
    ) -> ""Configuration"":
        
        configurable = (
            config[""configurable""] if config and ""configurable"" in config else {}
        )
        values: dict[str, Any] = {
            f.name: os.environ.get(f.name.upper(), configurable.get(f.name))
            for f in fields(cls)
            if f.init
        }
        return cls(**{k: v for k, v in values.items() if v})","['def', 'from_runnable_config', '(', 'cls', ',', 'config', ':', 'Optional', '[', 'RunnableConfig', ']', '=', 'None', ')', '-', '>', '""', 'Configuration', '""', ':', 'configurable', '=', '(', 'config', '[', '""', 'configurable', '""', ']', 'if', 'config', 'and', '""', 'configurable', '""', 'in', 'config', 'else', '{', '}', ')', 'values', ':', 'dict', '[', 'str', ',', 'Any', ']', '=', '{', 'f', '.', 'name', ':', 'os', '.', 'environ', '.', 'get', '(', 'f', '.', 'name', '.', 'upper', '(', ')', ',', 'configurable', '.', 'get', '(', 'f', '.', 'name', ')', ')', 'for', 'f', 'in', 'fields', '(', 'cls', ')', 'if', 'f', '.', 'init', '}', 'return', 'cls', '(', '*', '*', '{', 'k', ':', 'v', 'for', 'k', ',', 'v', 'in', 'values', '.', 'items', '(', ')', 'if', 'v', '}', ')']",Create a configuration object from environment variables and optional settings.,"['Create', 'a', 'configuration', 'object', 'from', 'environment', 'variables', 'and', 'optional', 'settings', '.']"
259,"def save_report(report, filename=None):
    
    if filename is None:
        filename = f""test_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt""
    with open(filename, 'w') as file:
        file.write(report)
    print(f""Report saved to {os.path.abspath(filename)}"")","['def', 'save_report', '(', 'report', ',', 'filename', '=', 'None', ')', ':', 'if', 'filename', 'is', 'None', ':', 'filename', '=', 'f', '""', 'test_report_', '{', 'datetime', '.', 'now', '(', ')', '.', 'strftime', '(', ""'"", '%', 'Y', '%', 'm', '%d', '_', '%', 'H', '%', 'M', '%', 'S', ""'"", ')', '}', '.txt', '""', 'with', 'open', '(', 'filename', ',', ""'"", 'w', ""'"", ')', 'as', 'file', ':', 'file', '.', 'write', '(', 'report', ')', 'print', '(', 'f', '""', 'Report saved to ', '{', 'os', '.', 'path', '.', 'abspath', '(', 'filename', ')', '}', '""', ')']",Automatically save a report to a timestamped text file if no filename is provided.,"['Automatically', 'save', 'a', 'report', 'to', 'a', 'timestamped', 'text', 'file', 'if', 'no', 'filename', 'is', 'provided', '.']"
260,"def initialize_cache(self, num_gpu_blocks: int, num_cpu_blocks: int) -> None:
        

        # NOTE: We log here to avoid multiple logs when number of workers is
        # greater than one. We could log in the engine, but not all executors
        # have GPUs.
        logger.info(""# GPU blocks: %d, # CPU blocks: %d"", num_gpu_blocks, num_cpu_blocks)

        self.cache_config.num_gpu_blocks = num_gpu_blocks
        self.cache_config.num_cpu_blocks = num_cpu_blocks

        if torch.distributed.get_rank() == 0:
            print(
                f'before init cache memory allocated: {torch.cuda.memory_allocated() / 1e9}GB, reserved: {torch.cuda.memory_reserved() / 1e9}GB'
            )
        self.worker.initialize_cache(num_gpu_blocks=num_gpu_blocks, num_cpu_blocks=num_cpu_blocks)
        if torch.distributed.get_rank() == 0:
            print(
                f'after init cache memory allocated: {torch.cuda.memory_allocated() / 1e9}GB, reserved: {torch.cuda.memory_reserved() / 1e9}GB'
            )","['def', 'initialize_cache', '(', 'self', ',', 'num_gpu_blocks', ':', 'int', ',', 'num_cpu_blocks', ':', 'int', ')', '-', '>', 'None', ':', '# NOTE: We log here to avoid multiple logs when number of workers is', '# greater than one. We could log in the engine, but not all executors', '# have GPUs.', 'logger', '.', 'info', '(', '""', '# GPU blocks: ', '%d', ', # CPU blocks: ', '%d', '""', ',', 'num_gpu_blocks', ',', 'num_cpu_blocks', ')', 'self', '.', 'cache_config', '.', 'num_gpu_blocks', '=', 'num_gpu_blocks', 'self', '.', 'cache_config', '.', 'num_cpu_blocks', '=', 'num_cpu_blocks', 'if', 'torch', '.', 'distributed', '.', 'get_rank', '(', ')', '==', '0', ':', 'print', '(', 'f', ""'"", 'before init cache memory allocated: ', '{', 'torch', '.', 'cuda', '.', 'memory_allocated', '(', ')', '/', '1e9', '}', 'GB, reserved: ', '{', 'torch', '.', 'cuda', '.', 'memory_reserved', '(', ')', '/', '1e9', '}', 'GB', ""'"", ')', 'self', '.', 'worker', '.', 'initialize_cache', '(', 'num_gpu_blocks', '=', 'num_gpu_blocks', ',', 'num_cpu_blocks', '=', 'num_cpu_blocks', ')', 'if', 'torch', '.', 'distributed', '.', 'get_rank', '(', ')', '==', '0', ':', 'print', '(', 'f', ""'"", 'after init cache memory allocated: ', '{', 'torch', '.', 'cuda', '.', 'memory_allocated', '(', ')', '/', '1e9', '}', 'GB, reserved: ', '{', 'torch', '.', 'cuda', '.', 'memory_reserved', '(', ')', '/', '1e9', '}', 'GB', ""'"", ')']",Initialize and log cache configuration for GPU and CPU blocks in a distributed system.,"['Initialize', 'and', 'log', 'cache', 'configuration', 'for', 'GPU', 'and', 'CPU', 'blocks', 'in', 'a', 'distributed', 'system', '.']"
261,"def get_db_context():
    
    if not _db_manager:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=""Database manager not initialized"",
        )
    try:
        yield _db_manager
    except Exception as e:
        logger.error(f""Database operation failed: {str(e)}"")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=""Database operation failed"",
        ) from e","['def', 'get_db_context', '(', ')', ':', 'if', 'not', '_db_manager', ':', 'raise', 'HTTPException', '(', 'status_code', '=', 'status', '.', 'HTTP_500_INTERNAL_SERVER_ERROR', ',', 'detail', '=', '""', 'Database manager not initialized', '""', ',', ')', 'try', ':', 'yield', '_db_manager', 'except', 'Exception', 'as', 'e', ':', 'logger', '.', 'error', '(', 'f', '""', 'Database operation failed: ', '{', 'str', '(', 'e', ')', '}', '""', ')', 'raise', 'HTTPException', '(', 'status_code', '=', 'status', '.', 'HTTP_500_INTERNAL_SERVER_ERROR', ',', 'detail', '=', '""', 'Database operation failed', '""', ',', ')', 'from', 'e']",Ensure database manager is initialized and handle operation errors,"['Ensure', 'database', 'manager', 'is', 'initialized', 'and', 'handle', 'operation', 'errors']"
262,"def get_processor(cls, file_type: FileType) -> Type[BaseFileProcessor]:
        
        if not cls._initialized:
            cls.init()
        print(f""Current registered processors: {cls._processors}"")
        if file_type not in cls._processors:
            raise ValueError(f""No processor found for {file_type}"")
        return cls._processors[file_type]","['def', 'get_processor', '(', 'cls', ',', 'file_type', ':', 'FileType', ')', '-', '>', 'Type', '[', 'BaseFileProcessor', ']', ':', 'if', 'not', 'cls', '.', '_initialized', ':', 'cls', '.', 'init', '(', ')', 'print', '(', 'f', '""', 'Current registered processors: ', '{', 'cls', '.', '_processors', '}', '""', ')', 'if', 'file_type', 'not', 'in', 'cls', '.', '_processors', ':', 'raise', 'ValueError', '(', 'f', '""', 'No processor found for ', '{', 'file_type', '}', '""', ')', 'return', 'cls', '.', '_processors', '[', 'file_type', ']']","Retrieve appropriate file processor based on file type, initializing if necessary.","['Retrieve', 'appropriate', 'file', 'processor', 'based', 'on', 'file', 'type', ',', 'initializing', 'if', 'necessary', '.']"
263,"def tf_roll_out(
    agent: Agent, env: ScriptBrowserEnv, config_file: str
) -> list[Any]:
    
    obs, state_info = env.reset(options={""config_file"": config_file})

    trajectory: list[Any] = [{""observation"": obs, ""info"": state_info}]
    while True:
        action = agent.next_action(
            trajectory=trajectory, intent="""", meta_data={}
        )
        trajectory.append(action)
        if action[""action_type""] == ActionTypes.STOP:
            break

        # preceed to next action
        obs, reward, terminated, truncated, info = env.step(action)
        state_info = {""observation"": obs, ""info"": info}
        trajectory.append(state_info)

    return trajectory","['def', 'tf_roll_out', '(', 'agent', ':', 'Agent', ',', 'env', ':', 'ScriptBrowserEnv', ',', 'config_file', ':', 'str', ')', '-', '>', 'list', '[', 'Any', ']', ':', 'obs', ',', 'state_info', '=', 'env', '.', 'reset', '(', 'options', '=', '{', '""', 'config_file', '""', ':', 'config_file', '}', ')', 'trajectory', ':', 'list', '[', 'Any', ']', '=', '[', '{', '""', 'observation', '""', ':', 'obs', ',', '""', 'info', '""', ':', 'state_info', '}', ']', 'while', 'True', ':', 'action', '=', 'agent', '.', 'next_action', '(', 'trajectory', '=', 'trajectory', ',', 'intent', '=', '""', '""', ',', 'meta_data', '=', '{', '}', ')', 'trajectory', '.', 'append', '(', 'action', ')', 'if', 'action', '[', '""', 'action_type', '""', ']', '==', 'ActionTypes', '.', 'STOP', ':', 'break', '# preceed to next action', 'obs', ',', 'reward', ',', 'terminated', ',', 'truncated', ',', 'info', '=', 'env', '.', 'step', '(', 'action', ')', 'state_info', '=', '{', '""', 'observation', '""', ':', 'obs', ',', '""', 'info', '""', ':', 'info', '}', 'trajectory', '.', 'append', '(', 'state_info', ')', 'return', 'trajectory']",Simulate agent interactions in environment using configuration file,"['Simulate', 'agent', 'interactions', 'in', 'environment', 'using', 'configuration', 'file']"
264,"def show_permission_dialog(code: str, action_description: str):
    
    if platform.system() == ""Darwin"":
        result = os.system(
            f'osascript -e \'display dialog ""Do you want to execute this action?\n\n{code} which will try to {action_description}"" with title ""Action Permission"" buttons {{""Cancel"", ""OK""}} default button ""OK"" cancel button ""Cancel""\''
        )
        return result == 0
    elif platform.system() == ""Linux"":
        result = os.system(
            f'zenity --question --title=""Action Permission"" --text=""Do you want to execute this action?\n\n{code}"" --width=400 --height=200'
        )
        return result == 0
    return False","['def', 'show_permission_dialog', '(', 'code', ':', 'str', ',', 'action_description', ':', 'str', ')', ':', 'if', 'platform', '.', 'system', '(', ')', '==', '""', 'Darwin', '""', ':', 'result', '=', 'os', '.', 'system', '(', 'f', ""'"", 'osascript -e ', ""\\'"", 'display dialog ', '""', 'Do you want to execute this action?', '\\n', '\\n', '{', 'code', '}', ' which will try to ', '{', 'action_description', '}', '""', ' with title ', '""', 'Action Permission', '""', ' buttons ', '{{', '""', 'Cancel', '""', ', ', '""', 'OK', '""', '}}', ' default button ', '""', 'OK', '""', ' cancel button ', '""', 'Cancel', '""', ""\\'"", ""'"", ')', 'return', 'result', '==', '0', 'elif', 'platform', '.', 'system', '(', ')', '==', '""', 'Linux', '""', ':', 'result', '=', 'os', '.', 'system', '(', 'f', ""'"", 'zenity --question --title=', '""', 'Action Permission', '""', ' --text=', '""', 'Do you want to execute this action?', '\\n', '\\n', '{', 'code', '}', '""', ' --width=400 --height=200', ""'"", ')', 'return', 'result', '==', '0', 'return', 'False']",Display a permission dialog for executing actions on macOS and Linux,"['Display', 'a', 'permission', 'dialog', 'for', 'executing', 'actions', 'on', 'macOS', 'and', 'Linux']"
265,"def __launch_kwargs(self):
        
        launch_kwargs = {'headless': self.headless, 'ignore_default_args': self.harmful_default_args, 'channel': 'chrome' if self.real_chrome else 'chromium'}
        if self.stealth:
            launch_kwargs.update({'args': self.__set_flags(), 'chromium_sandbox': True})

        return launch_kwargs","['def', '__launch_kwargs', '(', 'self', ')', ':', 'launch_kwargs', '=', '{', ""'"", 'headless', ""'"", ':', 'self', '.', 'headless', ',', ""'"", 'ignore_default_args', ""'"", ':', 'self', '.', 'harmful_default_args', ',', ""'"", 'channel', ""'"", ':', ""'"", 'chrome', ""'"", 'if', 'self', '.', 'real_chrome', 'else', ""'"", 'chromium', ""'"", '}', 'if', 'self', '.', 'stealth', ':', 'launch_kwargs', '.', 'update', '(', '{', ""'"", 'args', ""'"", ':', 'self', '.', '__set_flags', '(', ')', ',', ""'"", 'chromium_sandbox', ""'"", ':', 'True', '}', ')', 'return', 'launch_kwargs']",Configure browser launch settings based on user preferences and stealth mode.,"['Configure', 'browser', 'launch', 'settings', 'based', 'on', 'user', 'preferences', 'and', 'stealth', 'mode', '.']"
266,"def clean_response(cls, content: str) -> Dict[str, Any]:
        
        cleaned = re.sub(r""```(?:json)?\s*"", """", content).strip()
        match = re.search(r""(\{.*\})"", cleaned, re.S)
        if not match:
            logger.error(""Failed to parse JSON from content: %r"", content)
            raise ValueError(""No JSON object found in response"")
        return json.loads(match.group(1))","['def', 'clean_response', '(', 'cls', ',', 'content', ':', 'str', ')', '-', '>', 'Dict', '[', 'str', ',', 'Any', ']', ':', 'cleaned', '=', 're', '.', 'sub', '(', 'r', '""', '```(?:json)?', '\\', 's*', '""', ',', '""', '""', ',', 'content', ')', '.', 'strip', '(', ')', 'match', '=', 're', '.', 'search', '(', 'r', '""', '(', '\\', '{', '.*', '\\', '})', '""', ',', 'cleaned', ',', 're', '.', 'S', ')', 'if', 'not', 'match', ':', 'logger', '.', 'error', '(', '""', 'Failed to parse JSON from content: ', '%r', '""', ',', 'content', ')', 'raise', 'ValueError', '(', '""', 'No JSON object found in response', '""', ')', 'return', 'json', '.', 'loads', '(', 'match', '.', 'group', '(', '1', ')', ')']","Extract and parse JSON from a string, handling errors if parsing fails.","['Extract', 'and', 'parse', 'JSON', 'from', 'a', 'string', ',', 'handling', 'errors', 'if', 'parsing', 'fails', '.']"
267,"def make_request(prompt):
    
    payload = {""input"": prompt}
    
    try:
        response = requests.post(
            API_URL,
            json=payload,
            stream=True
        )
        
        print(f""\nMaking request with prompt: '{prompt}'\n"")
        print(""Raw response:"")
        for line in response.iter_lines():
            if line:
                print(line.decode('utf-8'))
                
    except Exception as e:
        print(f""Error making request: {e}"")","['def', 'make_request', '(', 'prompt', ')', ':', 'payload', '=', '{', '""', 'input', '""', ':', 'prompt', '}', 'try', ':', 'response', '=', 'requests', '.', 'post', '(', 'API_URL', ',', 'json', '=', 'payload', ',', 'stream', '=', 'True', ')', 'print', '(', 'f', '""', '\\n', 'Making request with prompt: ', ""'"", '{', 'prompt', '}', ""'"", '\\n', '""', ')', 'print', '(', '""', 'Raw response:', '""', ')', 'for', 'line', 'in', 'response', '.', 'iter_lines', '(', ')', ':', 'if', 'line', ':', 'print', '(', 'line', '.', 'decode', '(', ""'"", 'utf-8', ""'"", ')', ')', 'except', 'Exception', 'as', 'e', ':', 'print', '(', 'f', '""', 'Error making request: ', '{', 'e', '}', '""', ')']",Send input data to API and print streaming response.,"['Send', 'input', 'data', 'to', 'API', 'and', 'print', 'streaming', 'response', '.']"
268,"def _merge_audio(files, output: str) -> bool:
    
    try:
        # Create an empty audio segment
        combined = AudioSegment.empty()
        silence = AudioSegment.silent(duration=100)  # 100ms silence
        
        # Add audio files one by one
        for file in files:
            audio = AudioSegment.from_wav(file)
            combined += audio + silence
        combined += silence
        combined.export(output, format=""wav"", parameters=[""-acodec"", ""pcm_s16le"", ""-ar"", ""16000"", ""-ac"", ""1""])
        
        if os.path.getsize(output) == 0:
            rprint(f""[red]Output file size is 0"")
            return False
            
        rprint(f""[green]Successfully merged audio files"")
        return True
        
    except Exception as e:
        rprint(f""[red]Failed to merge audio: {str(e)}"")
        return False","['def', '_merge_audio', '(', 'files', ',', 'output', ':', 'str', ')', '-', '>', 'bool', ':', 'try', ':', '# Create an empty audio segment', 'combined', '=', 'AudioSegment', '.', 'empty', '(', ')', 'silence', '=', 'AudioSegment', '.', 'silent', '(', 'duration', '=', '100', ')', '# 100ms silence', '# Add audio files one by one', 'for', 'file', 'in', 'files', ':', 'audio', '=', 'AudioSegment', '.', 'from_wav', '(', 'file', ')', 'combined', '+', '=', 'audio', '+', 'silence', 'combined', '+', '=', 'silence', 'combined', '.', 'export', '(', 'output', ',', 'format', '=', '""', 'wav', '""', ',', 'parameters', '=', '[', '""', '-acodec', '""', ',', '""', 'pcm_s16le', '""', ',', '""', '-ar', '""', ',', '""', '16000', '""', ',', '""', '-ac', '""', ',', '""', '1', '""', ']', ')', 'if', 'os', '.', 'path', '.', 'getsize', '(', 'output', ')', '==', '0', ':', 'rprint', '(', 'f', '""', '[red]Output file size is 0', '""', ')', 'return', 'False', 'rprint', '(', 'f', '""', '[green]Successfully merged audio files', '""', ')', 'return', 'True', 'except', 'Exception', 'as', 'e', ':', 'rprint', '(', 'f', '""', '[red]Failed to merge audio: ', '{', 'str', '(', 'e', ')', '}', '""', ')', 'return', 'False']",Merge multiple audio files with silence into a single WAV output.,"['Merge', 'multiple', 'audio', 'files', 'with', 'silence', 'into', 'a', 'single', 'WAV', 'output', '.']"
269,"def pop_context():
    
    if _context_stack:
        if logger.isEnabledFor(logging.DEBUG):
            logger.debug(f""Popping context for atom {ctx.atom_name}"")
        _context_stack.pop()
    else:
        logger.warning(""[DAG] Attempted to pop context, but stack was empty"")","['def', 'pop_context', '(', ')', ':', 'if', '_context_stack', ':', 'if', 'logger', '.', 'isEnabledFor', '(', 'logging', '.', 'DEBUG', ')', ':', 'logger', '.', 'debug', '(', 'f', '""', 'Popping context for atom ', '{', 'ctx', '.', 'atom_name', '}', '""', ')', '_context_stack', '.', 'pop', '(', ')', 'else', ':', 'logger', '.', 'warning', '(', '""', '[DAG] Attempted to pop context, but stack was empty', '""', ')']","Remove the top context from the stack, logging actions and warnings.","['Remove', 'the', 'top', 'context', 'from', 'the', 'stack', ',', 'logging', 'actions', 'and', 'warnings', '.']"
270,"def remove_obsolete_ckpt(path: str, global_step: int, save_limit: int = -1, directory_format: str = ""global_step_{}""):
    
    if save_limit <= 0:
        return

    if not os.path.exists(path):
        return

    pattern = re.escape(directory_format).replace(r""\{\}"", r""(\d+)"")
    ckpt_folders = []
    for folder in os.listdir(path):
        if match := re.match(pattern, folder):
            step = int(match.group(1))
            if step < global_step:
                ckpt_folders.append((step, folder))

    ckpt_folders.sort(reverse=True)
    for _, folder in ckpt_folders[save_limit - 1 :]:
        folder_path = os.path.join(path, folder)
        shutil.rmtree(folder_path, ignore_errors=True)
        print(f""Removed obsolete checkpoint: {folder_path}"")","['def', 'remove_obsolete_ckpt', '(', 'path', ':', 'str', ',', 'global_step', ':', 'int', ',', 'save_limit', ':', 'int', '=', '-', '1', ',', 'directory_format', ':', 'str', '=', '""', 'global_step_', '{}', '""', ')', ':', 'if', 'save_limit', '<', '=', '0', ':', 'return', 'if', 'not', 'os', '.', 'path', '.', 'exists', '(', 'path', ')', ':', 'return', 'pattern', '=', 're', '.', 'escape', '(', 'directory_format', ')', '.', 'replace', '(', 'r', '""', '\\', '{', '\\', '}', '""', ',', 'r', '""', '(', '\\', 'd+)', '""', ')', 'ckpt_folders', '=', '[', ']', 'for', 'folder', 'in', 'os', '.', 'listdir', '(', 'path', ')', ':', 'if', 'match', ':=', 're', '.', 'match', '(', 'pattern', ',', 'folder', ')', ':', 'step', '=', 'int', '(', 'match', '.', 'group', '(', '1', ')', ')', 'if', 'step', '<', 'global_step', ':', 'ckpt_folders', '.', 'append', '(', '(', 'step', ',', 'folder', ')', ')', 'ckpt_folders', '.', 'sort', '(', 'reverse', '=', 'True', ')', 'for', '_', ',', 'folder', 'in', 'ckpt_folders', '[', 'save_limit', '-', '1', ':', ']', ':', 'folder_path', '=', 'os', '.', 'path', '.', 'join', '(', 'path', ',', 'folder', ')', 'shutil', '.', 'rmtree', '(', 'folder_path', ',', 'ignore_errors', '=', 'True', ')', 'print', '(', 'f', '""', 'Removed obsolete checkpoint: ', '{', 'folder_path', '}', '""', ')']",Remove outdated checkpoints based on step threshold and retention limit.,"['Remove', 'outdated', 'checkpoints', 'based', 'on', 'step', 'threshold', 'and', 'retention', 'limit', '.']"
271,"def _get_endpoint_config(self):
        
        endpoint_config = CONFIG.retrieval_endpoints.get(self.endpoint_name)
        
        if not endpoint_config:
            error_msg = f""No configuration found for endpoint {self.endpoint_name}""
            logger.error(error_msg)
            raise ValueError(error_msg)
        
        # Verify this is a Qdrant endpoint
        if endpoint_config.db_type != ""qdrant"":
            error_msg = f""Endpoint {self.endpoint_name} is not a Qdrant endpoint (type: {endpoint_config.db_type})""
            logger.error(error_msg)
            raise ValueError(error_msg)
            
        return endpoint_config","['def', '_get_endpoint_config', '(', 'self', ')', ':', 'endpoint_config', '=', 'CONFIG', '.', 'retrieval_endpoints', '.', 'get', '(', 'self', '.', 'endpoint_name', ')', 'if', 'not', 'endpoint_config', ':', 'error_msg', '=', 'f', '""', 'No configuration found for endpoint ', '{', 'self', '.', 'endpoint_name', '}', '""', 'logger', '.', 'error', '(', 'error_msg', ')', 'raise', 'ValueError', '(', 'error_msg', ')', '# Verify this is a Qdrant endpoint', 'if', 'endpoint_config', '.', 'db_type', '!=', '""', 'qdrant', '""', ':', 'error_msg', '=', 'f', '""', 'Endpoint ', '{', 'self', '.', 'endpoint_name', '}', ' is not a Qdrant endpoint (type: ', '{', 'endpoint_config', '.', 'db_type', '}', ')', '""', 'logger', '.', 'error', '(', 'error_msg', ')', 'raise', 'ValueError', '(', 'error_msg', ')', 'return', 'endpoint_config']",Retrieve and validate Qdrant endpoint configuration by name,"['Retrieve', 'and', 'validate', 'Qdrant', 'endpoint', 'configuration', 'by', 'name']"
272,"def _safe_save(self, output_dir: str):
        
        if self.deepspeed:
            torch.cuda.synchronize()
            self.save_model(output_dir)
            return
    
        state_dict = self.model.state_dict()
        if self.args.should_save:
            cpu_state_dict = {
                key: value.cpu()
                for key, value in state_dict.items()
            }
            del state_dict
            self._save(output_dir, state_dict=cpu_state_dict)","['def', '_safe_save', '(', 'self', ',', 'output_dir', ':', 'str', ')', ':', 'if', 'self', '.', 'deepspeed', ':', 'torch', '.', 'cuda', '.', 'synchronize', '(', ')', 'self', '.', 'save_model', '(', 'output_dir', ')', 'return', 'state_dict', '=', 'self', '.', 'model', '.', 'state_dict', '(', ')', 'if', 'self', '.', 'args', '.', 'should_save', ':', 'cpu_state_dict', '=', '{', 'key', ':', 'value', '.', 'cpu', '(', ')', 'for', 'key', ',', 'value', 'in', 'state_dict', '.', 'items', '(', ')', '}', 'del', 'state_dict', 'self', '.', '_save', '(', 'output_dir', ',', 'state_dict', '=', 'cpu_state_dict', ')']","Safely save model state to directory, handling GPU synchronization.","['Safely', 'save', 'model', 'state', 'to', 'directory', ',', 'handling', 'GPU', 'synchronization', '.']"
273,"def validate_fold_input(fold_input: folding_input.Input):
  
  for i, chain in enumerate(fold_input.protein_chains):
    if chain.unpaired_msa is None:
      raise ValueError(f'Protein chain {i + 1} is missing unpaired MSA.')
    if chain.paired_msa is None:
      raise ValueError(f'Protein chain {i + 1} is missing paired MSA.')
    if chain.templates is None:
      raise ValueError(f'Protein chain {i + 1} is missing Templates.')
  for i, chain in enumerate(fold_input.rna_chains):
    if chain.unpaired_msa is None:
      raise ValueError(f'RNA chain {i + 1} is missing unpaired MSA.')","['def', 'validate_fold_input', '(', 'fold_input', ':', 'folding_input', '.', 'Input', ')', ':', 'for', 'i', ',', 'chain', 'in', 'enumerate', '(', 'fold_input', '.', 'protein_chains', ')', ':', 'if', 'chain', '.', 'unpaired_msa', 'is', 'None', ':', 'raise', 'ValueError', '(', 'f', ""'"", 'Protein chain ', '{', 'i', '+', '1', '}', ' is missing unpaired MSA.', ""'"", ')', 'if', 'chain', '.', 'paired_msa', 'is', 'None', ':', 'raise', 'ValueError', '(', 'f', ""'"", 'Protein chain ', '{', 'i', '+', '1', '}', ' is missing paired MSA.', ""'"", ')', 'if', 'chain', '.', 'templates', 'is', 'None', ':', 'raise', 'ValueError', '(', 'f', ""'"", 'Protein chain ', '{', 'i', '+', '1', '}', ' is missing Templates.', ""'"", ')', 'for', 'i', ',', 'chain', 'in', 'enumerate', '(', 'fold_input', '.', 'rna_chains', ')', ':', 'if', 'chain', '.', 'unpaired_msa', 'is', 'None', ':', 'raise', 'ValueError', '(', 'f', ""'"", 'RNA chain ', '{', 'i', '+', '1', '}', ' is missing unpaired MSA.', ""'"", ')']",Validate presence of MSA and templates in protein and RNA chains,"['Validate', 'presence', 'of', 'MSA', 'and', 'templates', 'in', 'protein', 'and', 'RNA', 'chains']"
274,"def get_unrealspeech_skill(
    name: str,
    store: SkillStoreABC,
) -> UnrealSpeechBaseTool:
    
    if name == ""text_to_speech"":
        if name not in _cache:
            _cache[name] = TextToSpeech(
                skill_store=store,
            )
        return _cache[name]
    else:
        raise ValueError(f""Unknown UnrealSpeech skill: {name}"")","['def', 'get_unrealspeech_skill', '(', 'name', ':', 'str', ',', 'store', ':', 'SkillStoreABC', ',', ')', '-', '>', 'UnrealSpeechBaseTool', ':', 'if', 'name', '==', '""', 'text_to_speech', '""', ':', 'if', 'name', 'not', 'in', '_cache', ':', '_cache', '[', 'name', ']', '=', 'TextToSpeech', '(', 'skill_store', '=', 'store', ',', ')', 'return', '_cache', '[', 'name', ']', 'else', ':', 'raise', 'ValueError', '(', 'f', '""', 'Unknown UnrealSpeech skill: ', '{', 'name', '}', '""', ')']",Retrieve or cache text-to-speech skill from UnrealSpeech tool repository.,"['Retrieve', 'or', 'cache', 'text-to-speech', 'skill', 'from', 'UnrealSpeech', 'tool', 'repository', '.']"
275,"def handle_money(m: re.Match[str]) -> str:
    

    bill = ""dollar"" if m.group(2) == ""$"" else ""pound""
    coin = ""cent"" if m.group(2) == ""$"" else ""pence""
    number = m.group(3)

    multiplier = m.group(4)
    try:
        number = float(number)
    except:
        return m.group()

    if m.group(1) == ""-"":
        number *= -1

    if number % 1 == 0 or multiplier != """":
        text_number = f""{INFLECT_ENGINE.number_to_words(conditional_int(number))}{multiplier} {INFLECT_ENGINE.plural(bill, count=number)}""
    else:
        sub_number = int(str(number).split(""."")[-1].ljust(2, ""0""))

        text_number = f""{INFLECT_ENGINE.number_to_words(int(round(number)))} {INFLECT_ENGINE.plural(bill, count=number)} and {INFLECT_ENGINE.number_to_words(sub_number)} {INFLECT_ENGINE.plural(coin, count=sub_number)}""

    return text_number","['def', 'handle_money', '(', 'm', ':', 're', '.', 'Match', '[', 'str', ']', ')', '-', '>', 'str', ':', 'bill', '=', '""', 'dollar', '""', 'if', 'm', '.', 'group', '(', '2', ')', '==', '""', '$', '""', 'else', '""', 'pound', '""', 'coin', '=', '""', 'cent', '""', 'if', 'm', '.', 'group', '(', '2', ')', '==', '""', '$', '""', 'else', '""', 'pence', '""', 'number', '=', 'm', '.', 'group', '(', '3', ')', 'multiplier', '=', 'm', '.', 'group', '(', '4', ')', 'try', ':', 'number', '=', 'float', '(', 'number', ')', 'except', ':', 'return', 'm', '.', 'group', '(', ')', 'if', 'm', '.', 'group', '(', '1', ')', '==', '""', '-', '""', ':', 'number', '*', '=', '-', '1', 'if', 'number', '%', '1', '==', '0', 'or', 'multiplier', '!=', '""', '""', ':', 'text_number', '=', 'f', '""', '{', 'INFLECT_ENGINE', '.', 'number_to_words', '(', 'conditional_int', '(', 'number', ')', ')', '}', '{', 'multiplier', '}', '{', 'INFLECT_ENGINE', '.', 'plural', '(', 'bill', ',', 'count', '=', 'number', ')', '}', '""', 'else', ':', 'sub_number', '=', 'int', '(', 'str', '(', 'number', ')', '.', 'split', '(', '""', '.', '""', ')', '[', '-', '1', ']', '.', 'ljust', '(', '2', ',', '""', '0', '""', ')', ')', 'text_number', '=', 'f', '""', '{', 'INFLECT_ENGINE', '.', 'number_to_words', '(', 'int', '(', 'round', '(', 'number', ')', ')', ')', '}', '{', 'INFLECT_ENGINE', '.', 'plural', '(', 'bill', ',', 'count', '=', 'number', ')', '}', ' and ', '{', 'INFLECT_ENGINE', '.', 'number_to_words', '(', 'sub_number', ')', '}', '{', 'INFLECT_ENGINE', '.', 'plural', '(', 'coin', ',', 'count', '=', 'sub_number', ')', '}', '""', 'return', 'text_number']","Converts currency amounts from symbols to words, handling dollars and pounds.","['Converts', 'currency', 'amounts', 'from', 'symbols', 'to', 'words', ',', 'handling', 'dollars', 'and', 'pounds', '.']"
277,"def print_model_size(model: nn.Module, name: Optional[str] = None) -> None:
    
    if is_rank0():
        n_params, scale = _get_model_size(model, scale=""auto"")
        if name is None:
            name = model.__class__.__name__

        print(f""{name} contains {n_params:.2f}{scale} parameters."")","['def', 'print_model_size', '(', 'model', ':', 'nn', '.', 'Module', ',', 'name', ':', 'Optional', '[', 'str', ']', '=', 'None', ')', '-', '>', 'None', ':', 'if', 'is_rank0', '(', ')', ':', 'n_params', ',', 'scale', '=', '_get_model_size', '(', 'model', ',', 'scale', '=', '""', 'auto', '""', ')', 'if', 'name', 'is', 'None', ':', 'name', '=', 'model', '.', '__class__', '.', '__name__', 'print', '(', 'f', '""', '{', 'name', '}', ' contains ', '{', 'n_params', ':', '.2f', '}', '{', 'scale', '}', ' parameters.', '""', ')']",Display neural network model size if on primary process,"['Display', 'neural', 'network', 'model', 'size', 'if', 'on', 'primary', 'process']"
279,"def _sanitize_input(self, args: tuple, kwargs: dict) -> dict:
            

            def sanitize_value(value):
                if isinstance(value, (int, float, bool, str)):
                    return value
                elif isinstance(value, list):
                    return [sanitize_value(item) for item in value]
                elif isinstance(value, dict):
                    return {key: sanitize_value(val) for key, val in value.items()}
                else:
                    return str(value)  # Convert non-standard types to string

            return {
                ""args"": [sanitize_value(arg) for arg in args],
                ""kwargs"": {key: sanitize_value(val) for key, val in kwargs.items()},
            }","['def', '_sanitize_input', '(', 'self', ',', 'args', ':', 'tuple', ',', 'kwargs', ':', 'dict', ')', '-', '>', 'dict', ':', 'def', 'sanitize_value', '(', 'value', ')', ':', 'if', 'isinstance', '(', 'value', ',', '(', 'int', ',', 'float', ',', 'bool', ',', 'str', ')', ')', ':', 'return', 'value', 'elif', 'isinstance', '(', 'value', ',', 'list', ')', ':', 'return', '[', 'sanitize_value', '(', 'item', ')', 'for', 'item', 'in', 'value', ']', 'elif', 'isinstance', '(', 'value', ',', 'dict', ')', ':', 'return', '{', 'key', ':', 'sanitize_value', '(', 'val', ')', 'for', 'key', ',', 'val', 'in', 'value', '.', 'items', '(', ')', '}', 'else', ':', 'return', 'str', '(', 'value', ')', '# Convert non-standard types to string', 'return', '{', '""', 'args', '""', ':', '[', 'sanitize_value', '(', 'arg', ')', 'for', 'arg', 'in', 'args', ']', ',', '""', 'kwargs', '""', ':', '{', 'key', ':', 'sanitize_value', '(', 'val', ')', 'for', 'key', ',', 'val', 'in', 'kwargs', '.', 'items', '(', ')', '}', ',', '}']",Sanitize and convert input arguments to standard data types,"['Sanitize', 'and', 'convert', 'input', 'arguments', 'to', 'standard', 'data', 'types']"
280,"def cache_s3_files(dataset: Dataset, pdf_cache_location: str, num_proc: int = 32) -> Dataset:
    

    # Define the download function to use in parallel processing
    def cache_file(example):
        s3_path = example[""s3_path""]
        if s3_path:
            # Download the file and cache it locally
            local_path = _cache_s3_file(s3_path, pdf_cache_location)
            return {""local_pdf_path"": local_path}
        return {""local_pdf_path"": None}

    # Map the caching function to the dataset (with parallelism if needed)
    dataset = dataset.map(cache_file, num_proc=num_proc, load_from_cache_file=False)

    return dataset","['def', 'cache_s3_files', '(', 'dataset', ':', 'Dataset', ',', 'pdf_cache_location', ':', 'str', ',', 'num_proc', ':', 'int', '=', '32', ')', '-', '>', 'Dataset', ':', '# Define the download function to use in parallel processing', 'def', 'cache_file', '(', 'example', ')', ':', 's3_path', '=', 'example', '[', '""', 's3_path', '""', ']', 'if', 's3_path', ':', '# Download the file and cache it locally', 'local_path', '=', '_cache_s3_file', '(', 's3_path', ',', 'pdf_cache_location', ')', 'return', '{', '""', 'local_pdf_path', '""', ':', 'local_path', '}', 'return', '{', '""', 'local_pdf_path', '""', ':', 'None', '}', '# Map the caching function to the dataset (with parallelism if needed)', 'dataset', '=', 'dataset', '.', 'map', '(', 'cache_file', ',', 'num_proc', '=', 'num_proc', ',', 'load_from_cache_file', '=', 'False', ')', 'return', 'dataset']",Parallelly cache S3 files locally from a dataset using specified processors.,"['Parallelly', 'cache', 'S3', 'files', 'locally', 'from', 'a', 'dataset', 'using', 'specified', 'processors', '.']"
284,"def load_configs(prefix: str) -> Dict[str, Any]:
    
    keys = [key for key in os.environ.keys() if key.startswith(f""{prefix}_"")]
    configs = {}
    for key in keys:
        config_key = key.removeprefix(f""{prefix}_"").lower()
        value = os.getenv(key)
        try:
            configs[config_key] = json.loads(value) if value.startswith(""["") else value
        except json.JSONDecodeError:
            configs[config_key] = value
    return configs","['def', 'load_configs', '(', 'prefix', ':', 'str', ')', '-', '>', 'Dict', '[', 'str', ',', 'Any', ']', ':', 'keys', '=', '[', 'key', 'for', 'key', 'in', 'os', '.', 'environ', '.', 'keys', '(', ')', 'if', 'key', '.', 'startswith', '(', 'f', '""', '{', 'prefix', '}', '_', '""', ')', ']', 'configs', '=', '{', '}', 'for', 'key', 'in', 'keys', ':', 'config_key', '=', 'key', '.', 'removeprefix', '(', 'f', '""', '{', 'prefix', '}', '_', '""', ')', '.', 'lower', '(', ')', 'value', '=', 'os', '.', 'getenv', '(', 'key', ')', 'try', ':', 'configs', '[', 'config_key', ']', '=', 'json', '.', 'loads', '(', 'value', ')', 'if', 'value', '.', 'startswith', '(', '""', '[', '""', ')', 'else', 'value', 'except', 'json', '.', 'JSONDecodeError', ':', 'configs', '[', 'config_key', ']', '=', 'value', 'return', 'configs']",Extracts and parses environment variables with a specified prefix into a configuration dictionary.,"['Extracts', 'and', 'parses', 'environment', 'variables', 'with', 'a', 'specified', 'prefix', 'into', 'a', 'configuration', 'dictionary', '.']"
287,"def delete_integration_credential(mapper, connection, target):
    
    if target.integration_credential_id:
        # Get the session
        session = Session.object_session(target)
        if session:
            # If we're in a session, use the session to delete the IntegrationCredential
            from airweave.models.integration_credential import IntegrationCredential

            credential = session.get(IntegrationCredential, target.integration_credential_id)
            if credential:
                session.delete(credential)
        else:
            # If we're not in a session, use the connection directly
            connection.execute(
                f""DELETE FROM integration_credential WHERE id = '{target.integration_credential_id}'""  # noqa: E501
            )","['def', 'delete_integration_credential', '(', 'mapper', ',', 'connection', ',', 'target', ')', ':', 'if', 'target', '.', 'integration_credential_id', ':', '# Get the session', 'session', '=', 'Session', '.', 'object_session', '(', 'target', ')', 'if', 'session', ':', ""# If we're in a session, use the session to delete the IntegrationCredential"", 'from', 'airweave', '.', 'models', '.', 'integration_credential', 'import', 'IntegrationCredential', 'credential', '=', 'session', '.', 'get', '(', 'IntegrationCredential', ',', 'target', '.', 'integration_credential_id', ')', 'if', 'credential', ':', 'session', '.', 'delete', '(', 'credential', ')', 'else', ':', ""# If we're not in a session, use the connection directly"", 'connection', '.', 'execute', '(', 'f', '""', 'DELETE FROM integration_credential WHERE id = ', ""'"", '{', 'target', '.', 'integration_credential_id', '}', ""'"", '""', '# noqa: E501', ')']",Delete integration credentials using session or direct connection based on context.,"['Delete', 'integration', 'credentials', 'using', 'session', 'or', 'direct', 'connection', 'based', 'on', 'context', '.']"
288,"def analyze_schema_types(filename: str) -> Counter:
    
    all_types = Counter()
    
    with open(filename) as f:
        for line in f:
            items = line.strip().split('\t')
            if (len(items) < 2):
                continue
            js = json.loads(items[1])
            try:
                all_types.update(extract_types(js))
            except json.JSONDecodeError:
                print(f""Warning: Could not parse JSON line: {line[:100]}..."")
                continue
            
    return all_types","['def', 'analyze_schema_types', '(', 'filename', ':', 'str', ')', '-', '>', 'Counter', ':', 'all_types', '=', 'Counter', '(', ')', 'with', 'open', '(', 'filename', ')', 'as', 'f', ':', 'for', 'line', 'in', 'f', ':', 'items', '=', 'line', '.', 'strip', '(', ')', '.', 'split', '(', ""'"", '\\t', ""'"", ')', 'if', '(', 'len', '(', 'items', ')', '<', '2', ')', ':', 'continue', 'js', '=', 'json', '.', 'loads', '(', 'items', '[', '1', ']', ')', 'try', ':', 'all_types', '.', 'update', '(', 'extract_types', '(', 'js', ')', ')', 'except', 'json', '.', 'JSONDecodeError', ':', 'print', '(', 'f', '""', 'Warning: Could not parse JSON line: ', '{', 'line', '[', ':', '100', ']', '}', '...', '""', ')', 'continue', 'return', 'all_types']","Analyze and count JSON schema types from a file, handling parsing errors.","['Analyze', 'and', 'count', 'JSON', 'schema', 'types', 'from', 'a', 'file', ',', 'handling', 'parsing', 'errors', '.']"
289,"def create_default_config(self) -> bool:
        
        try:
            if self.config_path.exists():
                return True
                
            default_config = {
                ""mcpServers"": {},
                ""log_level"": ""INFO""
            }
            
            self.config_path.parent.mkdir(parents=True, exist_ok=True)
            with open(self.config_path, 'w') as f:
                json.dump(default_config, f, indent=2)
                
            logger.info(f""Created default configuration at {self.config_path}"")
            return True
            
        except Exception as e:
            logger.error(f""Error creating default configuration: {e}"")
            return False","['def', 'create_default_config', '(', 'self', ')', '-', '>', 'bool', ':', 'try', ':', 'if', 'self', '.', 'config_path', '.', 'exists', '(', ')', ':', 'return', 'True', 'default_config', '=', '{', '""', 'mcpServers', '""', ':', '{', '}', ',', '""', 'log_level', '""', ':', '""', 'INFO', '""', '}', 'self', '.', 'config_path', '.', 'parent', '.', 'mkdir', '(', 'parents', '=', 'True', ',', 'exist_ok', '=', 'True', ')', 'with', 'open', '(', 'self', '.', 'config_path', ',', ""'"", 'w', ""'"", ')', 'as', 'f', ':', 'json', '.', 'dump', '(', 'default_config', ',', 'f', ',', 'indent', '=', '2', ')', 'logger', '.', 'info', '(', 'f', '""', 'Created default configuration at ', '{', 'self', '.', 'config_path', '}', '""', ')', 'return', 'True', 'except', 'Exception', 'as', 'e', ':', 'logger', '.', 'error', '(', 'f', '""', 'Error creating default configuration: ', '{', 'e', '}', '""', ')', 'return', 'False']","Initialize default configuration file if it doesn't exist, logging success or errors.","['Initialize', 'default', 'configuration', 'file', 'if', 'it', 'does', ""n't"", 'exist', ',', 'logging', 'success', 'or', 'errors', '.']"
291,"def save_fid_stats(paths, batch_size, device, dims, num_workers=1):
    
    if not os.path.exists(paths[0]):
        raise RuntimeError(""Invalid path: %s"" % paths[0])

    if os.path.exists(paths[1]):
        raise RuntimeError(""Existing output file: %s"" % paths[1])

    block_idx = InceptionV3.BLOCK_INDEX_BY_DIM[dims]

    model = InceptionV3([block_idx]).to(device)

    print(f""Saving statistics for {paths[0]}"")

    m1, s1 = compute_statistics_of_path(paths[0], model, batch_size, dims, device, num_workers, flag=""ref"")
    np.savez_compressed(paths[1], mu=m1, sigma=s1)","['def', 'save_fid_stats', '(', 'paths', ',', 'batch_size', ',', 'device', ',', 'dims', ',', 'num_workers', '=', '1', ')', ':', 'if', 'not', 'os', '.', 'path', '.', 'exists', '(', 'paths', '[', '0', ']', ')', ':', 'raise', 'RuntimeError', '(', '""', 'Invalid path: ', '%s', '""', '%', 'paths', '[', '0', ']', ')', 'if', 'os', '.', 'path', '.', 'exists', '(', 'paths', '[', '1', ']', ')', ':', 'raise', 'RuntimeError', '(', '""', 'Existing output file: ', '%s', '""', '%', 'paths', '[', '1', ']', ')', 'block_idx', '=', 'InceptionV3', '.', 'BLOCK_INDEX_BY_DIM', '[', 'dims', ']', 'model', '=', 'InceptionV3', '(', '[', 'block_idx', ']', ')', '.', 'to', '(', 'device', ')', 'print', '(', 'f', '""', 'Saving statistics for ', '{', 'paths', '[', '0', ']', '}', '""', ')', 'm1', ',', 's1', '=', 'compute_statistics_of_path', '(', 'paths', '[', '0', ']', ',', 'model', ',', 'batch_size', ',', 'dims', ',', 'device', ',', 'num_workers', ',', 'flag', '=', '""', 'ref', '""', ')', 'np', '.', 'savez_compressed', '(', 'paths', '[', '1', ']', ',', 'mu', '=', 'm1', ',', 'sigma', '=', 's1', ')']",Compute and save statistical data from a specified path using a neural model.,"['Compute', 'and', 'save', 'statistical', 'data', 'from', 'a', 'specified', 'path', 'using', 'a', 'neural', 'model', '.']"
292,"def pytest_sessionfinish(session, exitstatus):
    
    cov_dir = os.path.abspath(""."")
    if exitstatus == 0 and os.environ.get(""COVERAGE_PROCESS_START""):
        try:
            cov = coverage.Coverage()
            cov.combine(data_paths=[cov_dir], strict=True)
            cov.save()
        except Exception as e:
            print(f""Error combining coverage data: {e}"", file=sys.stderr)","['def', 'pytest_sessionfinish', '(', 'session', ',', 'exitstatus', ')', ':', 'cov_dir', '=', 'os', '.', 'path', '.', 'abspath', '(', '""', '.', '""', ')', 'if', 'exitstatus', '==', '0', 'and', 'os', '.', 'environ', '.', 'get', '(', '""', 'COVERAGE_PROCESS_START', '""', ')', ':', 'try', ':', 'cov', '=', 'coverage', '.', 'Coverage', '(', ')', 'cov', '.', 'combine', '(', 'data_paths', '=', '[', 'cov_dir', ']', ',', 'strict', '=', 'True', ')', 'cov', '.', 'save', '(', ')', 'except', 'Exception', 'as', 'e', ':', 'print', '(', 'f', '""', 'Error combining coverage data: ', '{', 'e', '}', '""', ',', 'file', '=', 'sys', '.', 'stderr', ')']",Finalize test session by merging coverage data if tests pass.,"['Finalize', 'test', 'session', 'by', 'merging', 'coverage', 'data', 'if', 'tests', 'pass', '.']"
293,"def _build_param_buffer(self, pp_rank):
        
        if pp_rank == self._pp_rank:
            from verl.utils.memory_buffer import MemoryBuffer

            # The code here is very hard-coded, based on the following assumptions:
            # 1. `len(_this_rank_models) == 1`
            # 2. `_this_rank_models[0]` is a instance of `DistributedDataParallel` and `use_distributed_optimizer=True`
            # 3. Only bfloat16 data type is used in parameters
            source = self._this_rank_models[0].buffers[0].param_data
            self.memory_buffers[pp_rank] = {torch.bfloat16: MemoryBuffer(source.numel(), source.numel(), torch.bfloat16, source)}
        else:
            model = self.pp_models[pp_rank]
            weight_buffer_meta = get_weight_buffer_meta_from_module(model)
            self.memory_buffers[pp_rank] = build_memory_buffer(weight_buffer_meta)","['def', '_build_param_buffer', '(', 'self', ',', 'pp_rank', ')', ':', 'if', 'pp_rank', '==', 'self', '.', '_pp_rank', ':', 'from', 'verl', '.', 'utils', '.', 'memory_buffer', 'import', 'MemoryBuffer', '# The code here is very hard-coded, based on the following assumptions:', '# 1. `len(_this_rank_models) == 1`', '# 2. `_this_rank_models[0]` is a instance of `DistributedDataParallel` and `use_distributed_optimizer=True`', '# 3. Only bfloat16 data type is used in parameters', 'source', '=', 'self', '.', '_this_rank_models', '[', '0', ']', '.', 'buffers', '[', '0', ']', '.', 'param_data', 'self', '.', 'memory_buffers', '[', 'pp_rank', ']', '=', '{', 'torch', '.', 'bfloat16', ':', 'MemoryBuffer', '(', 'source', '.', 'numel', '(', ')', ',', 'source', '.', 'numel', '(', ')', ',', 'torch', '.', 'bfloat16', ',', 'source', ')', '}', 'else', ':', 'model', '=', 'self', '.', 'pp_models', '[', 'pp_rank', ']', 'weight_buffer_meta', '=', 'get_weight_buffer_meta_from_module', '(', 'model', ')', 'self', '.', 'memory_buffers', '[', 'pp_rank', ']', '=', 'build_memory_buffer', '(', 'weight_buffer_meta', ')']",Constructs memory buffers for model parameters based on rank and data type assumptions.,"['Constructs', 'memory', 'buffers', 'for', 'model', 'parameters', 'based', 'on', 'rank', 'and', 'data', 'type', 'assumptions', '.']"
294,"def MMMU_preproc(data):
    
    print(""Preprocessing MMMU dataset..."")
    cnt = 0
    As, Bs, Ans = list(data['A']), list(data['B']), list(data['answer'])
    lt = len(data)
    for i in range(lt):
        if pd.isna(As[i]):
            As[i] = Ans[i]
            Bs[i] = 'Other Answers'
            cnt += 1
    print(f'During MMMU_preproc in Evaluation, {cnt} open questions are re-formulated to multi-choice ones.')
    data['A'] = As
    data['B'] = Bs
    return data","['def', 'MMMU_preproc', '(', 'data', ')', ':', 'print', '(', '""', 'Preprocessing MMMU dataset...', '""', ')', 'cnt', '=', '0', 'As', ',', 'Bs', ',', 'Ans', '=', 'list', '(', 'data', '[', ""'"", 'A', ""'"", ']', ')', ',', 'list', '(', 'data', '[', ""'"", 'B', ""'"", ']', ')', ',', 'list', '(', 'data', '[', ""'"", 'answer', ""'"", ']', ')', 'lt', '=', 'len', '(', 'data', ')', 'for', 'i', 'in', 'range', '(', 'lt', ')', ':', 'if', 'pd', '.', 'isna', '(', 'As', '[', 'i', ']', ')', ':', 'As', '[', 'i', ']', '=', 'Ans', '[', 'i', ']', 'Bs', '[', 'i', ']', '=', ""'"", 'Other Answers', ""'"", 'cnt', '+', '=', '1', 'print', '(', 'f', ""'"", 'During MMMU_preproc in Evaluation, ', '{', 'cnt', '}', ' open questions are re-formulated to multi-choice ones.', ""'"", ')', 'data', '[', ""'"", 'A', ""'"", ']', '=', 'As', 'data', '[', ""'"", 'B', ""'"", ']', '=', 'Bs', 'return', 'data']",Refine dataset by converting open-ended questions to multiple-choice format,"['Refine', 'dataset', 'by', 'converting', 'open-ended', 'questions', 'to', 'multiple-choice', 'format']"
295,"def parse_expected_output(cls, v):
        
        if isinstance(v, str):
            try:
                return json.loads(v)
            except json.JSONDecodeError as e:
                logger.error(f""Error parsing expected_output: {str(e)}"")
                raise ValueError(""Invalid JSON format for expected_output"")
        return v","['def', 'parse_expected_output', '(', 'cls', ',', 'v', ')', ':', 'if', 'isinstance', '(', 'v', ',', 'str', ')', ':', 'try', ':', 'return', 'json', '.', 'loads', '(', 'v', ')', 'except', 'json', '.', 'JSONDecodeError', 'as', 'e', ':', 'logger', '.', 'error', '(', 'f', '""', 'Error parsing expected_output: ', '{', 'str', '(', 'e', ')', '}', '""', ')', 'raise', 'ValueError', '(', '""', 'Invalid JSON format for expected_output', '""', ')', 'return', 'v']","Convert string input to JSON, logging errors if parsing fails.","['Convert', 'string', 'input', 'to', 'JSON', ',', 'logging', 'errors', 'if', 'parsing', 'fails', '.']"
296,"def get_latest_files(directory: str, file_types: list = ['.webm', '.zip']) -> Dict[str, Optional[str]]:
    
    latest_files: Dict[str, Optional[str]] = {ext: None for ext in file_types}

    if not os.path.exists(directory):
        os.makedirs(directory, exist_ok=True)
        return latest_files

    for file_type in file_types:
        try:
            matches = list(Path(directory).rglob(f""*{file_type}""))
            if matches:
                latest = max(matches, key=lambda p: p.stat().st_mtime)
                # Only return files that are complete (not being written)
                if time.time() - latest.stat().st_mtime > 1.0:
                    latest_files[file_type] = str(latest)
        except Exception as e:
            print(f""Error getting latest {file_type} file: {e}"")

    return latest_files","['def', 'get_latest_files', '(', 'directory', ':', 'str', ',', 'file_types', ':', 'list', '=', '[', ""'"", '.webm', ""'"", ',', ""'"", '.zip', ""'"", ']', ')', '-', '>', 'Dict', '[', 'str', ',', 'Optional', '[', 'str', ']', ']', ':', 'latest_files', ':', 'Dict', '[', 'str', ',', 'Optional', '[', 'str', ']', ']', '=', '{', 'ext', ':', 'None', 'for', 'ext', 'in', 'file_types', '}', 'if', 'not', 'os', '.', 'path', '.', 'exists', '(', 'directory', ')', ':', 'os', '.', 'makedirs', '(', 'directory', ',', 'exist_ok', '=', 'True', ')', 'return', 'latest_files', 'for', 'file_type', 'in', 'file_types', ':', 'try', ':', 'matches', '=', 'list', '(', 'Path', '(', 'directory', ')', '.', 'rglob', '(', 'f', '""', '*', '{', 'file_type', '}', '""', ')', ')', 'if', 'matches', ':', 'latest', '=', 'max', '(', 'matches', ',', 'key', '=', 'lambda', 'p', ':', 'p', '.', 'stat', '(', ')', '.', 'st_mtime', ')', '# Only return files that are complete (not being written)', 'if', 'time', '.', 'time', '(', ')', '-', 'latest', '.', 'stat', '(', ')', '.', 'st_mtime', '>', '1.0', ':', 'latest_files', '[', 'file_type', ']', '=', 'str', '(', 'latest', ')', 'except', 'Exception', 'as', 'e', ':', 'print', '(', 'f', '""', 'Error getting latest ', '{', 'file_type', '}', ' file: ', '{', 'e', '}', '""', ')', 'return', 'latest_files']",Retrieve the most recent files of specified types from a directory.,"['Retrieve', 'the', 'most', 'recent', 'files', 'of', 'specified', 'types', 'from', 'a', 'directory', '.']"
298,"def _trajectory_to_text(trajectory: Dict[str, Any]) -> str:
    
    text = ""AGENT TRAJECTORY\n"" + ""=""*50 + ""\n\n""
    
    # Add answers/responses
    if ""answer"" in trajectory and isinstance(trajectory[""answer""], list):
        for i, answer in enumerate(trajectory[""answer""]):
            text += f""STEP {i+1}:\n""
            text += f""Agent Response:\n{answer}\n\n""
    
    # Add parsed responses if available
    if ""parsed_response"" in trajectory and isinstance(trajectory[""parsed_response""], list):
        text += ""\nPARSED RESPONSES\n"" + ""-""*50 + ""\n\n""
        for i, parsed in enumerate(trajectory[""parsed_response""]):
            text += f""Step {i+1} Parsed:\n""
            for key, value in parsed.items():
                if value:
                    text += f""  {key}: {value}\n""
            text += ""\n""
    
    return text","['def', '_trajectory_to_text', '(', 'trajectory', ':', 'Dict', '[', 'str', ',', 'Any', ']', ')', '-', '>', 'str', ':', 'text', '=', '""', 'AGENT TRAJECTORY', '\\n', '""', '+', '""', '=', '""', '*', '50', '+', '""', '\\n', '\\n', '""', '# Add answers/responses', 'if', '""', 'answer', '""', 'in', 'trajectory', 'and', 'isinstance', '(', 'trajectory', '[', '""', 'answer', '""', ']', ',', 'list', ')', ':', 'for', 'i', ',', 'answer', 'in', 'enumerate', '(', 'trajectory', '[', '""', 'answer', '""', ']', ')', ':', 'text', '+', '=', 'f', '""', 'STEP ', '{', 'i', '+', '1', '}', ':', '\\n', '""', 'text', '+', '=', 'f', '""', 'Agent Response:', '\\n', '{', 'answer', '}', '\\n', '\\n', '""', '# Add parsed responses if available', 'if', '""', 'parsed_response', '""', 'in', 'trajectory', 'and', 'isinstance', '(', 'trajectory', '[', '""', 'parsed_response', '""', ']', ',', 'list', ')', ':', 'text', '+', '=', '""', '\\n', 'PARSED RESPONSES', '\\n', '""', '+', '""', '-', '""', '*', '50', '+', '""', '\\n', '\\n', '""', 'for', 'i', ',', 'parsed', 'in', 'enumerate', '(', 'trajectory', '[', '""', 'parsed_response', '""', ']', ')', ':', 'text', '+', '=', 'f', '""', 'Step ', '{', 'i', '+', '1', '}', ' Parsed:', '\\n', '""', 'for', 'key', ',', 'value', 'in', 'parsed', '.', 'items', '(', ')', ':', 'if', 'value', ':', 'text', '+', '=', 'f', '""', '{', 'key', '}', ': ', '{', 'value', '}', '\\n', '""', 'text', '+', '=', '""', '\\n', '""', 'return', 'text']",Converts agent trajectory data into a formatted text report.,"['Converts', 'agent', 'trajectory', 'data', 'into', 'a', 'formatted', 'text', 'report', '.']"
300,"def _run(self, **kwargs) -> str:
        
        if not self.server_manager.active_server:
            return ""No MCP server is currently active, so there's nothing to disconnect from.""

        server_name = self.server_manager.active_server
        try:
            # Clear the active server
            self.server_manager.active_server = None

            # Note: We're not actually closing the session here, just 'deactivating'
            # This way we keep the session cache without requiring reconnection if needed again

            return f""Successfully disconnected from MCP server '{server_name}'.""
        except Exception as e:
            logger.error(f""Error disconnecting from server '{server_name}': {e}"")
            return f""Failed to disconnect from server '{server_name}': {str(e)}""","['def', '_run', '(', 'self', ',', '*', '*', 'kwargs', ')', '-', '>', 'str', ':', 'if', 'not', 'self', '.', 'server_manager', '.', 'active_server', ':', 'return', '""', 'No MCP server is currently active, so there', ""'"", 's nothing to disconnect from.', '""', 'server_name', '=', 'self', '.', 'server_manager', '.', 'active_server', 'try', ':', '# Clear the active server', 'self', '.', 'server_manager', '.', 'active_server', '=', 'None', ""# Note: We're not actually closing the session here, just 'deactivating'"", '# This way we keep the session cache without requiring reconnection if needed again', 'return', 'f', '""', 'Successfully disconnected from MCP server ', ""'"", '{', 'server_name', '}', ""'"", '.', '""', 'except', 'Exception', 'as', 'e', ':', 'logger', '.', 'error', '(', 'f', '""', 'Error disconnecting from server ', ""'"", '{', 'server_name', '}', ""'"", ': ', '{', 'e', '}', '""', ')', 'return', 'f', '""', 'Failed to disconnect from server ', ""'"", '{', 'server_name', '}', ""'"", ': ', '{', 'str', '(', 'e', ')', '}', '""']","Disconnects from active server, handling errors and preserving session cache.","['Disconnects', 'from', 'active', 'server', ',', 'handling', 'errors', 'and', 'preserving', 'session', 'cache', '.']"
301,"def on_signal(self, signal_name: str) -> Callable:
        

        def decorator(func: Callable) -> Callable:
            unique_name = f""{signal_name}_{uuid.uuid4()}""

            async def wrapped(value: SignalValueT):
                try:
                    if asyncio.iscoroutinefunction(func):
                        await func(value)
                    else:
                        func(value)
                except Exception as e:
                    # Log the error but don't fail the entire signal handling
                    print(f""Error in signal handler {signal_name}: {str(e)}"")

            self._handlers.setdefault(signal_name, []).append((unique_name, wrapped))
            return wrapped

        return decorator","['def', 'on_signal', '(', 'self', ',', 'signal_name', ':', 'str', ')', '-', '>', 'Callable', ':', 'def', 'decorator', '(', 'func', ':', 'Callable', ')', '-', '>', 'Callable', ':', 'unique_name', '=', 'f', '""', '{', 'signal_name', '}', '_', '{', 'uuid', '.', 'uuid4', '(', ')', '}', '""', 'async', 'def', 'wrapped', '(', 'value', ':', 'SignalValueT', ')', ':', 'try', ':', 'if', 'asyncio', '.', 'iscoroutinefunction', '(', 'func', ')', ':', 'await', 'func', '(', 'value', ')', 'else', ':', 'func', '(', 'value', ')', 'except', 'Exception', 'as', 'e', ':', ""# Log the error but don't fail the entire signal handling"", 'print', '(', 'f', '""', 'Error in signal handler ', '{', 'signal_name', '}', ': ', '{', 'str', '(', 'e', ')', '}', '""', ')', 'self', '.', '_handlers', '.', 'setdefault', '(', 'signal_name', ',', '[', ']', ')', '.', 'append', '(', '(', 'unique_name', ',', 'wrapped', ')', ')', 'return', 'wrapped', 'return', 'decorator']",Decorator registers async or sync signal handlers with error logging.,"['Decorator', 'registers', 'async', 'or', 'sync', 'signal', 'handlers', 'with', 'error', 'logging', '.']"
302,"def download_model(model_name):
    
    assert model_name in pretrained_models
    local_path = f""output/pretrained_models/{model_name}""
    if not os.path.isfile(local_path):
        hf_endpoint = os.environ.get(""HF_ENDPOINT"")
        if hf_endpoint is None:
        os.makedirs(""output/pretrained_models"", exist_ok=True)
        web_path = f""""
        download_url(web_path, ""output/pretrained_models/"")
    model = torch.load(local_path, map_location=lambda storage, loc: storage)
    return model","['def', 'download_model', '(', 'model_name', ')', ':', 'assert', 'model_name', 'in', 'pretrained_models', 'local_path', '=', 'f', '""', 'output/pretrained_models/', '{', 'model_name', '}', '""', 'if', 'not', 'os', '.', 'path', '.', 'isfile', '(', 'local_path', ')', ':', 'hf_endpoint', '=', 'os', '.', 'environ', '.', 'get', '(', '""', 'HF_ENDPOINT', '""', ')', 'if', 'hf_endpoint', 'is', 'None', ':', 'os', '.', 'makedirs', '(', '""', 'output/pretrained_models', '""', ',', 'exist_ok', '=', 'True', ')', 'web_path', '=', 'f', '""', '""', 'download_url', '(', 'web_path', ',', '""', 'output/pretrained_models/', '""', ')', 'model', '=', 'torch', '.', 'load', '(', 'local_path', ',', 'map_location', '=', 'lambda', 'storage', ',', 'loc', ':', 'storage', ')', 'return', 'model']",Download and load a pretrained model from a specified source if not locally available.,"['Download', 'and', 'load', 'a', 'pretrained', 'model', 'from', 'a', 'specified', 'source', 'if', 'not', 'locally', 'available', '.']"
307,"def pdf_path():
    
    pdf_path = Path(PDF_FILE_PATH)
    if not pdf_path.exists():
        pytest.skip(f""Test PDF file not found at {pdf_path}"")
    return pdf_path","['def', 'pdf_path', '(', ')', ':', 'pdf_path', '=', 'Path', '(', 'PDF_FILE_PATH', ')', 'if', 'not', 'pdf_path', '.', 'exists', '(', ')', ':', 'pytest', '.', 'skip', '(', 'f', '""', 'Test PDF file not found at ', '{', 'pdf_path', '}', '""', ')', 'return', 'pdf_path']",Check if a PDF file exists and skip test if not found,"['Check', 'if', 'a', 'PDF', 'file', 'exists', 'and', 'skip', 'test', 'if', 'not', 'found']"
308,"def download_dataset(self) -> None:
        
        assert self.data_dir is not None
        if not os.path.exists(self.data_dir):
            os.makedirs(self.data_dir, exist_ok=True)

        logging.info(f""[WebGames] Downloading dataset into '{self.data_dir}'..."")

        # Use pandas to download and save the dataset
        df = pd.read_json(  # type: ignore
        )
        output_path = os.path.join(self.data_dir, self.TEST_FILE)
        df.to_json(output_path, orient=""records"", lines=True)  # type: ignore

        logging.info(""[WebGames] Dataset downloaded."")","['def', 'download_dataset', '(', 'self', ')', '-', '>', 'None', ':', 'assert', 'self', '.', 'data_dir', 'is', 'not', 'None', 'if', 'not', 'os', '.', 'path', '.', 'exists', '(', 'self', '.', 'data_dir', ')', ':', 'os', '.', 'makedirs', '(', 'self', '.', 'data_dir', ',', 'exist_ok', '=', 'True', ')', 'logging', '.', 'info', '(', 'f', '""', '[WebGames] Downloading dataset into ', ""'"", '{', 'self', '.', 'data_dir', '}', ""'"", '...', '""', ')', '# Use pandas to download and save the dataset', 'df', '=', 'pd', '.', 'read_json', '(', '# type: ignore', ')', 'output_path', '=', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'data_dir', ',', 'self', '.', 'TEST_FILE', ')', 'df', '.', 'to_json', '(', 'output_path', ',', 'orient', '=', '""', 'records', '""', ',', 'lines', '=', 'True', ')', '# type: ignore', 'logging', '.', 'info', '(', '""', '[WebGames] Dataset downloaded.', '""', ')']",Download and save dataset to specified directory using pandas.,"['Download', 'and', 'save', 'dataset', 'to', 'specified', 'directory', 'using', 'pandas', '.']"
310,"def get_ratelimit_middleware_instance(fastapi_app: FastAPI) -> RateLimitMiddleware:
    
    layer = fastapi_app.middleware_stack
    while layer is not None:
        if layer.__class__.__name__ == RateLimitMiddleware.__name__:
            return cast(RateLimitMiddleware, layer)
        layer = getattr(layer, ""app"", None)

    assert False, f""{RateLimitMiddleware.__name__} instance not found""","['def', 'get_ratelimit_middleware_instance', '(', 'fastapi_app', ':', 'FastAPI', ')', '-', '>', 'RateLimitMiddleware', ':', 'layer', '=', 'fastapi_app', '.', 'middleware_stack', 'while', 'layer', 'is', 'not', 'None', ':', 'if', 'layer', '.', '__class__', '.', '__name__', '==', 'RateLimitMiddleware', '.', '__name__', ':', 'return', 'cast', '(', 'RateLimitMiddleware', ',', 'layer', ')', 'layer', '=', 'getattr', '(', 'layer', ',', '""', 'app', '""', ',', 'None', ')', 'assert', 'False', ',', 'f', '""', '{', 'RateLimitMiddleware', '.', '__name__', '}', ' instance not found', '""']",Retrieve existing rate limit middleware from FastAPI app's middleware stack.,"['Retrieve', 'existing', 'rate', 'limit', 'middleware', 'from', 'FastAPI', 'app', ""'s"", 'middleware', 'stack', '.']"
311,"def post(self, shared, prep_res, exec_res):
        
        tools = exec_res
        shared[""tools""] = tools
        
        # Format tool information for later use
        tool_info = []
        for i, tool in enumerate(tools, 1):
            properties = tool.inputSchema.get('properties', {})
            required = tool.inputSchema.get('required', [])
            
            params = []
            for param_name, param_info in properties.items():
                param_type = param_info.get('type', 'unknown')
                req_status = ""(Required)"" if param_name in required else ""(Optional)""
                params.append(f""    - {param_name} ({param_type}): {req_status}"")
            
            tool_info.append(f""[{i}] {tool.name}\n  Description: {tool.description}\n  Parameters:\n"" + ""\n"".join(params))
        
        shared[""tool_info""] = ""\n"".join(tool_info)
        return ""decide""","['def', 'post', '(', 'self', ',', 'shared', ',', 'prep_res', ',', 'exec_res', ')', ':', 'tools', '=', 'exec_res', 'shared', '[', '""', 'tools', '""', ']', '=', 'tools', '# Format tool information for later use', 'tool_info', '=', '[', ']', 'for', 'i', ',', 'tool', 'in', 'enumerate', '(', 'tools', ',', '1', ')', ':', 'properties', '=', 'tool', '.', 'inputSchema', '.', 'get', '(', ""'"", 'properties', ""'"", ',', '{', '}', ')', 'required', '=', 'tool', '.', 'inputSchema', '.', 'get', '(', ""'"", 'required', ""'"", ',', '[', ']', ')', 'params', '=', '[', ']', 'for', 'param_name', ',', 'param_info', 'in', 'properties', '.', 'items', '(', ')', ':', 'param_type', '=', 'param_info', '.', 'get', '(', ""'"", 'type', ""'"", ',', ""'"", 'unknown', ""'"", ')', 'req_status', '=', '""', '(Required)', '""', 'if', 'param_name', 'in', 'required', 'else', '""', '(Optional)', '""', 'params', '.', 'append', '(', 'f', '""', '    - ', '{', 'param_name', '}', ' (', '{', 'param_type', '}', '): ', '{', 'req_status', '}', '""', ')', 'tool_info', '.', 'append', '(', 'f', '""', '[', '{', 'i', '}', '] ', '{', 'tool', '.', 'name', '}', '\\n', '  Description: ', '{', 'tool', '.', 'description', '}', '\\n', '  Parameters:', '\\n', '""', '+', '""', '\\n', '""', '.', 'join', '(', 'params', ')', ')', 'shared', '[', '""', 'tool_info', '""', ']', '=', '""', '\\n', '""', '.', 'join', '(', 'tool_info', ')', 'return', '""', 'decide', '""']",Store and format tool details for shared access and decision-making.,"['Store', 'and', 'format', 'tool', 'details', 'for', 'shared', 'access', 'and', 'decision-making', '.']"
312,"def plot_total_conversation_time(results, title, filename):
    
    df = pd.DataFrame(results)
    df = df[(df['elapsed'].notnull()) & (df['turn'] == 'ALL')]
    if df.empty:
        print(f""No successful results to plot for {title}."")
        return
    plt.figure(figsize=(10, 6))
    ax = plt.gca()
    df.boxplot(column='elapsed', by=['provider'], ax=ax)
    plt.title(title)
    plt.suptitle("""")
    plt.ylabel('Total Conversation Time (s)')
    plt.xlabel('Provider')
    plt.xticks(rotation=45, ha='right')
    plt.tight_layout()
    plt.savefig(filename)
    plt.show()","['def', 'plot_total_conversation_time', '(', 'results', ',', 'title', ',', 'filename', ')', ':', 'df', '=', 'pd', '.', 'DataFrame', '(', 'results', ')', 'df', '=', 'df', '[', '(', 'df', '[', ""'"", 'elapsed', ""'"", ']', '.', 'notnull', '(', ')', ')', '&', '(', 'df', '[', ""'"", 'turn', ""'"", ']', '==', ""'"", 'ALL', ""'"", ')', ']', 'if', 'df', '.', 'empty', ':', 'print', '(', 'f', '""', 'No successful results to plot for ', '{', 'title', '}', '.', '""', ')', 'return', 'plt', '.', 'figure', '(', 'figsize', '=', '(', '10', ',', '6', ')', ')', 'ax', '=', 'plt', '.', 'gca', '(', ')', 'df', '.', 'boxplot', '(', 'column', '=', ""'"", 'elapsed', ""'"", ',', 'by', '=', '[', ""'"", 'provider', ""'"", ']', ',', 'ax', '=', 'ax', ')', 'plt', '.', 'title', '(', 'title', ')', 'plt', '.', 'suptitle', '(', '""', '""', ')', 'plt', '.', 'ylabel', '(', ""'"", 'Total Conversation Time (s)', ""'"", ')', 'plt', '.', 'xlabel', '(', ""'"", 'Provider', ""'"", ')', 'plt', '.', 'xticks', '(', 'rotation', '=', '45', ',', 'ha', '=', ""'"", 'right', ""'"", ')', 'plt', '.', 'tight_layout', '(', ')', 'plt', '.', 'savefig', '(', 'filename', ')', 'plt', '.', 'show', '(', ')']",Visualize and save a boxplot of conversation times by provider from given data.,"['Visualize', 'and', 'save', 'a', 'boxplot', 'of', 'conversation', 'times', 'by', 'provider', 'from', 'given', 'data', '.']"
313,"def _extract_tool_calls(self, trajectory: dict) -> list:
        
        tool_calls = []
        for message in trajectory.get(""messages"", []):
            if message.get(""role"") == ""assistant"" and message.get(""tool_calls""):
                for tool_call in message.get(""tool_calls"", []):
                    if tool_call.get(""function"", {}).get(""name"") and tool_call.get(""function"", {}).get(""arguments""):
                        tool_calls.append({
                            ""name"": tool_call[""function""][""name""],
                            ""arguments"": json.loads(tool_call[""function""][""arguments""])
                        })
        return tool_calls","['def', '_extract_tool_calls', '(', 'self', ',', 'trajectory', ':', 'dict', ')', '-', '>', 'list', ':', 'tool_calls', '=', '[', ']', 'for', 'message', 'in', 'trajectory', '.', 'get', '(', '""', 'messages', '""', ',', '[', ']', ')', ':', 'if', 'message', '.', 'get', '(', '""', 'role', '""', ')', '==', '""', 'assistant', '""', 'and', 'message', '.', 'get', '(', '""', 'tool_calls', '""', ')', ':', 'for', 'tool_call', 'in', 'message', '.', 'get', '(', '""', 'tool_calls', '""', ',', '[', ']', ')', ':', 'if', 'tool_call', '.', 'get', '(', '""', 'function', '""', ',', '{', '}', ')', '.', 'get', '(', '""', 'name', '""', ')', 'and', 'tool_call', '.', 'get', '(', '""', 'function', '""', ',', '{', '}', ')', '.', 'get', '(', '""', 'arguments', '""', ')', ':', 'tool_calls', '.', 'append', '(', '{', '""', 'name', '""', ':', 'tool_call', '[', '""', 'function', '""', ']', '[', '""', 'name', '""', ']', ',', '""', 'arguments', '""', ':', 'json', '.', 'loads', '(', 'tool_call', '[', '""', 'function', '""', ']', '[', '""', 'arguments', '""', ']', ')', '}', ')', 'return', 'tool_calls']",Extracts and returns assistant tool call details from a message trajectory.,"['Extracts', 'and', 'returns', 'assistant', 'tool', 'call', 'details', 'from', 'a', 'message', 'trajectory', '.']"
314,"def close(self):
        
        try:
            # Milvus client doesn't have an explicit close method, but we can release the collection
            if hasattr(self.client, ""release_collection""):
                self.client.release_collection(collection_name=self.collection_name)
        except Exception as e:
            logger.error(f""Error closing Milvus connection: {e}"")","['def', 'close', '(', 'self', ')', ':', 'try', ':', ""# Milvus client doesn't have an explicit close method, but we can release the collection"", 'if', 'hasattr', '(', 'self', '.', 'client', ',', '""', 'release_collection', '""', ')', ':', 'self', '.', 'client', '.', 'release_collection', '(', 'collection_name', '=', 'self', '.', 'collection_name', ')', 'except', 'Exception', 'as', 'e', ':', 'logger', '.', 'error', '(', 'f', '""', 'Error closing Milvus connection: ', '{', 'e', '}', '""', ')']","Safely release Milvus collection resources, logging errors if encountered.","['Safely', 'release', 'Milvus', 'collection', 'resources', ',', 'logging', 'errors', 'if', 'encountered', '.']"
315,"def log_generations_to_mlflow(self, samples, step):
        

        import json
        import tempfile

        import mlflow

        try:
            with tempfile.TemporaryDirectory() as tmp_dir:
                validation_gen_step_file = Path(tmp_dir, f""val_step{step}.json"")
                row_data = []
                for sample in samples:
                    data = {""input"": sample[0], ""output"": sample[1], ""score"": sample[2]}
                    row_data.append(data)
                with open(validation_gen_step_file, ""w"") as file:
                    json.dump(row_data, file)
                mlflow.log_artifact(validation_gen_step_file)
        except Exception as e:
            print(f""WARNING: save validation generation file to mlflow failed with error {e}"")","['def', 'log_generations_to_mlflow', '(', 'self', ',', 'samples', ',', 'step', ')', ':', 'import', 'json', 'import', 'tempfile', 'import', 'mlflow', 'try', ':', 'with', 'tempfile', '.', 'TemporaryDirectory', '(', ')', 'as', 'tmp_dir', ':', 'validation_gen_step_file', '=', 'Path', '(', 'tmp_dir', ',', 'f', '""', 'val_step', '{', 'step', '}', '.json', '""', ')', 'row_data', '=', '[', ']', 'for', 'sample', 'in', 'samples', ':', 'data', '=', '{', '""', 'input', '""', ':', 'sample', '[', '0', ']', ',', '""', 'output', '""', ':', 'sample', '[', '1', ']', ',', '""', 'score', '""', ':', 'sample', '[', '2', ']', '}', 'row_data', '.', 'append', '(', 'data', ')', 'with', 'open', '(', 'validation_gen_step_file', ',', '""', 'w', '""', ')', 'as', 'file', ':', 'json', '.', 'dump', '(', 'row_data', ',', 'file', ')', 'mlflow', '.', 'log_artifact', '(', 'validation_gen_step_file', ')', 'except', 'Exception', 'as', 'e', ':', 'print', '(', 'f', '""', 'WARNING: save validation generation file to mlflow failed with error ', '{', 'e', '}', '""', ')']",Log sample data to MLflow as JSON artifacts for each step,"['Log', 'sample', 'data', 'to', 'MLflow', 'as', 'JSON', 'artifacts', 'for', 'each', 'step']"
316,"def register_error_handlers(app):
    

    @app.errorhandler(404)
    def not_found(error):
        return make_response(jsonify({""error"": ""Not found""}), 404)

    @app.errorhandler(500)
    def server_error(error):
        return make_response(jsonify({""error"": ""Server error""}), 500)","['def', 'register_error_handlers', '(', 'app', ')', ':', '@app', '.', 'errorhandler', '(', '404', ')', 'def', 'not_found', '(', 'error', ')', ':', 'return', 'make_response', '(', 'jsonify', '(', '{', '""', 'error', '""', ':', '""', 'Not found', '""', '}', ')', ',', '404', ')', '@app', '.', 'errorhandler', '(', '500', ')', 'def', 'server_error', '(', 'error', ')', ':', 'return', 'make_response', '(', 'jsonify', '(', '{', '""', 'error', '""', ':', '""', 'Server error', '""', '}', ')', ',', '500', ')']",Configure application to handle 404 and 500 errors with JSON responses.,"['Configure', 'application', 'to', 'handle', '404', 'and', '500', 'errors', 'with', 'JSON', 'responses', '.']"
317,"def print_summary(self):
        
        summary = self.get_summary()
        total = summary.get(""total_duration"", 0)
        
        print(""\n===== SPEED PROFILE SUMMARY ====="")
        print(f""Total execution time: {total:.2f} seconds"")
        print(""\n--- Component Breakdown ---"")
        
        # Print each component's timing
        for name, data in self.timings.items():
            if name != ""total"":
                percent = data[""total""] / total * 100 if total > 0 else 0
                print(f""{name}: {data['total']:.2f}s ({percent:.1f}%) - ""
                      f""{data['count']} calls, avg {data['total'] / data['count']:.3f}s per call"")
        
        print(""\n=============================="")","['def', 'print_summary', '(', 'self', ')', ':', 'summary', '=', 'self', '.', 'get_summary', '(', ')', 'total', '=', 'summary', '.', 'get', '(', '""', 'total_duration', '""', ',', '0', ')', 'print', '(', '""', '\\n', '===== SPEED PROFILE SUMMARY =====', '""', ')', 'print', '(', 'f', '""', 'Total execution time: ', '{', 'total', ':', '.2f', '}', ' seconds', '""', ')', 'print', '(', '""', '\\n', '--- Component Breakdown ---', '""', ')', ""# Print each component's timing"", 'for', 'name', ',', 'data', 'in', 'self', '.', 'timings', '.', 'items', '(', ')', ':', 'if', 'name', '!=', '""', 'total', '""', ':', 'percent', '=', 'data', '[', '""', 'total', '""', ']', '/', 'total', '*', '100', 'if', 'total', '>', '0', 'else', '0', 'print', '(', 'f', '""', '{', 'name', '}', ': ', '{', 'data', '[', ""'"", 'total', ""'"", ']', ':', '.2f', '}', 's (', '{', 'percent', ':', '.1f', '}', '%) - ', '""', 'f', '""', '{', 'data', '[', ""'"", 'count', ""'"", ']', '}', ' calls, avg ', '{', 'data', '[', ""'"", 'total', ""'"", ']', '/', 'data', '[', ""'"", 'count', ""'"", ']', ':', '.3f', '}', 's per call', '""', ')', 'print', '(', '""', '\\n', '==============================', '""', ')']",Generate and display a detailed execution time summary for various components.,"['Generate', 'and', 'display', 'a', 'detailed', 'execution', 'time', 'summary', 'for', 'various', 'components', '.']"
318,"def generate_cursor_checksum(token: str, translator=None) -> str:
    
    try:
        # Clean the token
        clean_token = token.strip()
        
        # Generate machineId and macMachineId
        machine_id = generate_hashed64_hex(clean_token, 'machineId')
        mac_machine_id = generate_hashed64_hex(clean_token, 'macMachineId')
        
        # Get timestamp and convert to byte array
        byte_array = bytearray(struct.pack('>Q', timestamp)[-6:])  # Take last 6 bytes
        
        # Obfuscate bytes and encode as base64
        obfuscated_bytes = obfuscate_bytes(byte_array)
        encoded_checksum = base64.b64encode(obfuscated_bytes).decode('utf-8')
        
        # Combine final checksum
        return f""{encoded_checksum}{machine_id}/{mac_machine_id}""
    except Exception as e:
        print(f""{Fore.RED}{EMOJI['ERROR']} {translator.get('auth_check.error_generating_checksum', error=str(e)) if translator else f'Error generating checksum: {str(e)}'}{Style.RESET_ALL}"")
        return """"","['def', 'generate_cursor_checksum', '(', 'token', ':', 'str', ',', 'translator', '=', 'None', ')', '-', '>', 'str', ':', 'try', ':', '# Clean the token', 'clean_token', '=', 'token', '.', 'strip', '(', ')', '# Generate machineId and macMachineId', 'machine_id', '=', 'generate_hashed64_hex', '(', 'clean_token', ',', ""'"", 'machineId', ""'"", ')', 'mac_machine_id', '=', 'generate_hashed64_hex', '(', 'clean_token', ',', ""'"", 'macMachineId', ""'"", ')', '# Get timestamp and convert to byte array', 'byte_array', '=', 'bytearray', '(', 'struct', '.', 'pack', '(', ""'"", '>Q', ""'"", ',', 'timestamp', ')', '[', '-', '6', ':', ']', ')', '# Take last 6 bytes', '# Obfuscate bytes and encode as base64', 'obfuscated_bytes', '=', 'obfuscate_bytes', '(', 'byte_array', ')', 'encoded_checksum', '=', 'base64', '.', 'b64encode', '(', 'obfuscated_bytes', ')', '.', 'decode', '(', ""'"", 'utf-8', ""'"", ')', '# Combine final checksum', 'return', 'f', '""', '{', 'encoded_checksum', '}', '{', 'machine_id', '}', '/', '{', 'mac_machine_id', '}', '""', 'except', 'Exception', 'as', 'e', ':', 'print', '(', 'f', '""', '{', 'Fore', '.', 'RED', '}', '{', 'EMOJI', '[', ""'"", 'ERROR', ""'"", ']', '}', '{', 'translator', '.', 'get', '(', ""'"", 'auth_check.error_generating_checksum', ""'"", ',', 'error', '=', 'str', '(', 'e', ')', ')', 'if', 'translator', 'else', 'f', ""'"", 'Error generating checksum: ', '{', 'str', '(', 'e', ')', '}', ""'"", '}', '{', 'Style', '.', 'RESET_ALL', '}', '""', ')', 'return', '""', '""']",Generate a secure checksum by hashing and encoding a token with error handling.,"['Generate', 'a', 'secure', 'checksum', 'by', 'hashing', 'and', 'encoding', 'a', 'token', 'with', 'error', 'handling', '.']"
320,"def generate_sub_questions(state: ResearchState) -> ResearchState:
    
    prompt = PromptTemplate(
        input_variables=[""topic""],
        template=""Given the topic '{topic}', generate 3 specific sub-questions to guide research.""
    )
    response = llm.invoke(prompt.format(topic=state[""topic""]))
    questions = [q.strip() for q in response.content.split(""\n"") if q.strip()]
    return {""sub_questions"": questions, ""status"": ""generated_questions""}","['def', 'generate_sub_questions', '(', 'state', ':', 'ResearchState', ')', '-', '>', 'ResearchState', ':', 'prompt', '=', 'PromptTemplate', '(', 'input_variables', '=', '[', '""', 'topic', '""', ']', ',', 'template', '=', '""', 'Given the topic ', ""'"", '{topic}', ""'"", ', generate 3 specific sub-questions to guide research.', '""', ')', 'response', '=', 'llm', '.', 'invoke', '(', 'prompt', '.', 'format', '(', 'topic', '=', 'state', '[', '""', 'topic', '""', ']', ')', ')', 'questions', '=', '[', 'q', '.', 'strip', '(', ')', 'for', 'q', 'in', 'response', '.', 'content', '.', 'split', '(', '""', '\\n', '""', ')', 'if', 'q', '.', 'strip', '(', ')', ']', 'return', '{', '""', 'sub_questions', '""', ':', 'questions', ',', '""', 'status', '""', ':', '""', 'generated_questions', '""', '}']",Generate sub-questions for research topics using a language model.,"['Generate', 'sub-questions', 'for', 'research', 'topics', 'using', 'a', 'language', 'model', '.']"
322,"def create_tool_result_message(tool_result, tool_name, status=""success""):
        
        from google.genai import types

        if status == ""success"":
            function_response = {""result"": tool_result}
        else:
            function_response = {""error"": tool_result}

        return types.Content(
            role=""tool"",
            parts=[
                types.Part.from_function_response(
                    name=tool_name, response=function_response
                )
            ],
        )","['def', 'create_tool_result_message', '(', 'tool_result', ',', 'tool_name', ',', 'status', '=', '""', 'success', '""', ')', ':', 'from', 'google', '.', 'genai', 'import', 'types', 'if', 'status', '==', '""', 'success', '""', ':', 'function_response', '=', '{', '""', 'result', '""', ':', 'tool_result', '}', 'else', ':', 'function_response', '=', '{', '""', 'error', '""', ':', 'tool_result', '}', 'return', 'types', '.', 'Content', '(', 'role', '=', '""', 'tool', '""', ',', 'parts', '=', '[', 'types', '.', 'Part', '.', 'from_function_response', '(', 'name', '=', 'tool_name', ',', 'response', '=', 'function_response', ')', ']', ',', ')']",Generate a structured message based on tool execution outcome and status.,"['Generate', 'a', 'structured', 'message', 'based', 'on', 'tool', 'execution', 'outcome', 'and', 'status', '.']"
323,"def _merge_by_placement(self, tensors: list[torch.Tensor], placement: Placement) -> torch.Tensor:
        
        if placement.is_replicate():
            return tensors[0]
        elif placement.is_partial():
            raise NotImplementedError(""Partial placement is not supported yet"")
        elif placement.is_shard():
            return torch.cat(tensors, dim=placement.dim).contiguous()

        raise NotImplementedError(f""Unsupported placement: {placement}"")","['def', '_merge_by_placement', '(', 'self', ',', 'tensors', ':', 'list', '[', 'torch', '.', 'Tensor', ']', ',', 'placement', ':', 'Placement', ')', '-', '>', 'torch', '.', 'Tensor', ':', 'if', 'placement', '.', 'is_replicate', '(', ')', ':', 'return', 'tensors', '[', '0', ']', 'elif', 'placement', '.', 'is_partial', '(', ')', ':', 'raise', 'NotImplementedError', '(', '""', 'Partial placement is not supported yet', '""', ')', 'elif', 'placement', '.', 'is_shard', '(', ')', ':', 'return', 'torch', '.', 'cat', '(', 'tensors', ',', 'dim', '=', 'placement', '.', 'dim', ')', '.', 'contiguous', '(', ')', 'raise', 'NotImplementedError', '(', 'f', '""', 'Unsupported placement: ', '{', 'placement', '}', '""', ')']","Merge tensors based on placement type, handling replication and sharding.","['Merge', 'tensors', 'based', 'on', 'placement', 'type', ',', 'handling', 'replication', 'and', 'sharding', '.']"
324,"def analyze_news_sentiment(news_items: list) -> str:
    
    if not news_items or len(news_items) == 0:
        return ""No news data available""
    
    # Just return a simple count for now - in a real implementation, this would use NLP
    return f""Qualitative review of {len(news_items)} recent news items would be needed""","['def', 'analyze_news_sentiment', '(', 'news_items', ':', 'list', ')', '-', '>', 'str', ':', 'if', 'not', 'news_items', 'or', 'len', '(', 'news_items', ')', '==', '0', ':', 'return', '""', 'No news data available', '""', '# Just return a simple count for now - in a real implementation, this would use NLP', 'return', 'f', '""', 'Qualitative review of ', '{', 'len', '(', 'news_items', ')', '}', ' recent news items would be needed', '""']",Determine sentiment analysis necessity for given news items list,"['Determine', 'sentiment', 'analysis', 'necessity', 'for', 'given', 'news', 'items', 'list']"
328,"def deepseek_fn(self, history, verbose=False):
        
        if self.is_local:
            raise Exception(""Deepseek (API) is not available for local use. Change config.ini"")
        try:
            response = client.chat.completions.create(
                model=""deepseek-chat"",
                messages=history,
                stream=False
            )
            thought = response.choices[0].message.content
            if verbose:
                print(thought)
            return thought
        except Exception as e:
            raise Exception(f""Deepseek API error: {str(e)}"") from e","['def', 'deepseek_fn', '(', 'self', ',', 'history', ',', 'verbose', '=', 'False', ')', ':', 'if', 'self', '.', 'is_local', ':', 'raise', 'Exception', '(', '""', 'Deepseek (API) is not available for local use. Change config.ini', '""', ')', 'try', ':', 'response', '=', 'client', '.', 'chat', '.', 'completions', '.', 'create', '(', 'model', '=', '""', 'deepseek-chat', '""', ',', 'messages', '=', 'history', ',', 'stream', '=', 'False', ')', 'thought', '=', 'response', '.', 'choices', '[', '0', ']', '.', 'message', '.', 'content', 'if', 'verbose', ':', 'print', '(', 'thought', ')', 'return', 'thought', 'except', 'Exception', 'as', 'e', ':', 'raise', 'Exception', '(', 'f', '""', 'Deepseek API error: ', '{', 'str', '(', 'e', ')', '}', '""', ')', 'from', 'e']",Function interfaces with Deepseek API to generate chat responses from message history.,"['Function', 'interfaces', 'with', 'Deepseek', 'API', 'to', 'generate', 'chat', 'responses', 'from', 'message', 'history', '.']"
329,"def copy_old_config_names_to_new(self) -> Self:
		
		if self.window_width or self.window_height:
			self.window_size = self.window_size or {}
			self.window_size['width'] = (self.window_size or {}).get('width') or self.window_width or 1280
			self.window_size['height'] = (self.window_size or {}).get('height') or self.window_height or 1100
		return self","['def', 'copy_old_config_names_to_new', '(', 'self', ')', '-', '>', 'Self', ':', 'if', 'self', '.', 'window_width', 'or', 'self', '.', 'window_height', ':', 'self', '.', 'window_size', '=', 'self', '.', 'window_size', 'or', '{', '}', 'self', '.', 'window_size', '[', ""'"", 'width', ""'"", ']', '=', '(', 'self', '.', 'window_size', 'or', '{', '}', ')', '.', 'get', '(', ""'"", 'width', ""'"", ')', 'or', 'self', '.', 'window_width', 'or', '1280', 'self', '.', 'window_size', '[', ""'"", 'height', ""'"", ']', '=', '(', 'self', '.', 'window_size', 'or', '{', '}', ')', '.', 'get', '(', ""'"", 'height', ""'"", ')', 'or', 'self', '.', 'window_height', 'or', '1100', 'return', 'self']",Update window size configuration with default values if necessary.,"['Update', 'window', 'size', 'configuration', 'with', 'default', 'values', 'if', 'necessary', '.']"
330,"def _prepare_batch_get_documents_request(
        self, document_ids: List[str], folder_name: Optional[Union[str, List[str]]], end_user_id: Optional[str]
    ) -> Dict[str, Any]:
        
        if folder_name or end_user_id:
            request = {""document_ids"": document_ids}
            if folder_name:
                request[""folder_name""] = folder_name
            if end_user_id:
                request[""end_user_id""] = end_user_id
            return request
        return document_ids","['def', '_prepare_batch_get_documents_request', '(', 'self', ',', 'document_ids', ':', 'List', '[', 'str', ']', ',', 'folder_name', ':', 'Optional', '[', 'Union', '[', 'str', ',', 'List', '[', 'str', ']', ']', ']', ',', 'end_user_id', ':', 'Optional', '[', 'str', ']', ')', '-', '>', 'Dict', '[', 'str', ',', 'Any', ']', ':', 'if', 'folder_name', 'or', 'end_user_id', ':', 'request', '=', '{', '""', 'document_ids', '""', ':', 'document_ids', '}', 'if', 'folder_name', ':', 'request', '[', '""', 'folder_name', '""', ']', '=', 'folder_name', 'if', 'end_user_id', ':', 'request', '[', '""', 'end_user_id', '""', ']', '=', 'end_user_id', 'return', 'request', 'return', 'document_ids']",Constructs a document retrieval request with optional folder and user identifiers.,"['Constructs', 'a', 'document', 'retrieval', 'request', 'with', 'optional', 'folder', 'and', 'user', 'identifiers', '.']"
334,"def load_jsonl_files(input_dir):
    
    jsonl_files = list(Path(input_dir).glob(""*.jsonl""))
    if not jsonl_files:
        print(f""No JSONL files found in {input_dir}"")
        return []

    print(f""Found {len(jsonl_files)} JSONL files: {[f.name for f in jsonl_files]}"")
    return jsonl_files","['def', 'load_jsonl_files', '(', 'input_dir', ')', ':', 'jsonl_files', '=', 'list', '(', 'Path', '(', 'input_dir', ')', '.', 'glob', '(', '""', '*.jsonl', '""', ')', ')', 'if', 'not', 'jsonl_files', ':', 'print', '(', 'f', '""', 'No JSONL files found in ', '{', 'input_dir', '}', '""', ')', 'return', '[', ']', 'print', '(', 'f', '""', 'Found ', '{', 'len', '(', 'jsonl_files', ')', '}', ' JSONL files: ', '{', '[', 'f', '.', 'name', 'for', 'f', 'in', 'jsonl_files', ']', '}', '""', ')', 'return', 'jsonl_files']","Load and list JSONL files from a specified directory, handling absence gracefully.","['Load', 'and', 'list', 'JSONL', 'files', 'from', 'a', 'specified', 'directory', ',', 'handling', 'absence', 'gracefully', '.']"
336,"def _load_environment(self) -> None:
        
        env_file = self.app_root / "".env""
        if env_file.exists():
            # logger.info(f""Loading environment variables from {env_file}"")
            load_dotenv(str(env_file))","['def', '_load_environment', '(', 'self', ')', '-', '>', 'None', ':', 'env_file', '=', 'self', '.', 'app_root', '/', '""', '.env', '""', 'if', 'env_file', '.', 'exists', '(', ')', ':', '# logger.info(f""Loading environment variables from {env_file}"")', 'load_dotenv', '(', 'str', '(', 'env_file', ')', ')']",Load environment variables from a .env file if it exists.,"['Load', 'environment', 'variables', 'from', 'a', '.env', 'file', 'if', 'it', 'exists', '.']"
337,"def add_tool(self, tool: BaseTool):
        
        if tool.name in self.tool_map:
            logger.warning(f""Tool {tool.name} already exists in collection, skipping"")
            return self

        self.tools += (tool,)
        self.tool_map[tool.name] = tool
        return self","['def', 'add_tool', '(', 'self', ',', 'tool', ':', 'BaseTool', ')', ':', 'if', 'tool', '.', 'name', 'in', 'self', '.', 'tool_map', ':', 'logger', '.', 'warning', '(', 'f', '""', 'Tool ', '{', 'tool', '.', 'name', '}', ' already exists in collection, skipping', '""', ')', 'return', 'self', 'self', '.', 'tools', '+', '=', '(', 'tool', ',', ')', 'self', '.', 'tool_map', '[', 'tool', '.', 'name', ']', '=', 'tool', 'return', 'self']",Add a tool to the collection if it doesn't already exist,"['Add', 'a', 'tool', 'to', 'the', 'collection', 'if', 'it', 'does', ""n't"", 'already', 'exist']"
338,"def _generate_batch_response(self, text, system_message, provider, model_config, api_key, api_base):
        
        MAX_RETRIES = 3
        
        for attempt in range(MAX_RETRIES):
            try:
                if provider == ""gemini"" and api_base:
                    messages = [{'role': 'user', 'content': system_message + text}]
                    response = proxy_api_completion(messages=messages, model=model_config[""model""], api_base=api_base)
                    # response = proxy_call.api_completion(messages=messages, model=model_config[""model""], api_base=api_base)
                    return pd.DataFrame(ast.literal_eval(response[0]))
                else:
                    return self._generate_llm_response(text, system_message, model_config, api_key)
            except (json.JSONDecodeError, ValueError) as e:
                if attempt == MAX_RETRIES - 1:
                    raise Exception(f""Failed to generate valid response after {MAX_RETRIES} attempts: {str(e)}"")
                continue","['def', '_generate_batch_response', '(', 'self', ',', 'text', ',', 'system_message', ',', 'provider', ',', 'model_config', ',', 'api_key', ',', 'api_base', ')', ':', 'MAX_RETRIES', '=', '3', 'for', 'attempt', 'in', 'range', '(', 'MAX_RETRIES', ')', ':', 'try', ':', 'if', 'provider', '==', '""', 'gemini', '""', 'and', 'api_base', ':', 'messages', '=', '[', '{', ""'"", 'role', ""'"", ':', ""'"", 'user', ""'"", ',', ""'"", 'content', ""'"", ':', 'system_message', '+', 'text', '}', ']', 'response', '=', 'proxy_api_completion', '(', 'messages', '=', 'messages', ',', 'model', '=', 'model_config', '[', '""', 'model', '""', ']', ',', 'api_base', '=', 'api_base', ')', '# response = proxy_call.api_completion(messages=messages, model=model_config[""model""], api_base=api_base)', 'return', 'pd', '.', 'DataFrame', '(', 'ast', '.', 'literal_eval', '(', 'response', '[', '0', ']', ')', ')', 'else', ':', 'return', 'self', '.', '_generate_llm_response', '(', 'text', ',', 'system_message', ',', 'model_config', ',', 'api_key', ')', 'except', '(', 'json', '.', 'JSONDecodeError', ',', 'ValueError', ')', 'as', 'e', ':', 'if', 'attempt', '==', 'MAX_RETRIES', '-', '1', ':', 'raise', 'Exception', '(', 'f', '""', 'Failed to generate valid response after ', '{', 'MAX_RETRIES', '}', ' attempts: ', '{', 'str', '(', 'e', ')', '}', '""', ')', 'continue']","Attempt to generate a batch response using a specified provider, retrying on failure up to three times.","['Attempt', 'to', 'generate', 'a', 'batch', 'response', 'using', 'a', 'specified', 'provider', ',', 'retrying', 'on', 'failure', 'up', 'to', 'three', 'times', '.']"
339,"def model_kwargs():
    
    inference_url = settings.TEXT2VEC_INFERENCE_URL
    if not inference_url:
        raise ValueError(""TEXT2VEC_INFERENCE_URL environment variable is not set"")
    return {""inference_url"": inference_url}","['def', 'model_kwargs', '(', ')', ':', 'inference_url', '=', 'settings', '.', 'TEXT2VEC_INFERENCE_URL', 'if', 'not', 'inference_url', ':', 'raise', 'ValueError', '(', '""', 'TEXT2VEC_INFERENCE_URL environment variable is not set', '""', ')', 'return', '{', '""', 'inference_url', '""', ':', 'inference_url', '}']",Retrieve and validate the inference URL for text-to-vector conversion.,"['Retrieve', 'and', 'validate', 'the', 'inference', 'URL', 'for', 'text-to-vector', 'conversion', '.']"
343,"def handle_js_message(client_id, message_type, data):
        
        if _service:
            asyncio.create_task(  # noqa: RUF006
                _service.handle_client_message(
                    client_id, {""type"": message_type, ""data"": data}
                )
            )
        return True","['def', 'handle_js_message', '(', 'client_id', ',', 'message_type', ',', 'data', ')', ':', 'if', '_service', ':', 'asyncio', '.', 'create_task', '(', '# noqa: RUF006', '_service', '.', 'handle_client_message', '(', 'client_id', ',', '{', '""', 'type', '""', ':', 'message_type', ',', '""', 'data', '""', ':', 'data', '}', ')', ')', 'return', 'True']",Handle asynchronous client messages based on type and data.,"['Handle', 'asynchronous', 'client', 'messages', 'based', 'on', 'type', 'and', 'data', '.']"
344,"def rename_state_dict(
    old_prefix: str, new_prefix: str, state_dict: Dict[str, torch.Tensor]
):
    
    # need this list not to break the dict iterator
    old_keys = [k for k in state_dict if k.startswith(old_prefix)]
    if len(old_keys) > 0:
        logging.warning(f""Rename: {old_prefix} -> {new_prefix}"")
    for k in old_keys:
        v = state_dict.pop(k)
        new_k = k.replace(old_prefix, new_prefix)
        state_dict[new_k] = v","['def', 'rename_state_dict', '(', 'old_prefix', ':', 'str', ',', 'new_prefix', ':', 'str', ',', 'state_dict', ':', 'Dict', '[', 'str', ',', 'torch', '.', 'Tensor', ']', ')', ':', '# need this list not to break the dict iterator', 'old_keys', '=', '[', 'k', 'for', 'k', 'in', 'state_dict', 'if', 'k', '.', 'startswith', '(', 'old_prefix', ')', ']', 'if', 'len', '(', 'old_keys', ')', '>', '0', ':', 'logging', '.', 'warning', '(', 'f', '""', 'Rename: ', '{', 'old_prefix', '}', ' -> ', '{', 'new_prefix', '}', '""', ')', 'for', 'k', 'in', 'old_keys', ':', 'v', '=', 'state_dict', '.', 'pop', '(', 'k', ')', 'new_k', '=', 'k', '.', 'replace', '(', 'old_prefix', ',', 'new_prefix', ')', 'state_dict', '[', 'new_k', ']', '=', 'v']",Rename dictionary keys by replacing old prefix with new prefix in a state dictionary.,"['Rename', 'dictionary', 'keys', 'by', 'replacing', 'old', 'prefix', 'with', 'new', 'prefix', 'in', 'a', 'state', 'dictionary', '.']"
347,"def _get_loggable_messages(self, messages: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        
        loggable_messages = []
        for msg in messages:
            if isinstance(msg.get(""content""), list):
                new_content = []
                for content in msg[""content""]:
                    if content.get(""type"") == ""image"":
                        new_content.append(
                            {""type"": ""image"", ""image_url"": {""url"": ""[BASE64_IMAGE_DATA]""}}
                        )
                    else:
                        new_content.append(content)
                loggable_messages.append({""role"": msg[""role""], ""content"": new_content})
            else:
                loggable_messages.append(msg)
        return loggable_messages","['def', '_get_loggable_messages', '(', 'self', ',', 'messages', ':', 'List', '[', 'Dict', '[', 'str', ',', 'Any', ']', ']', ')', '-', '>', 'List', '[', 'Dict', '[', 'str', ',', 'Any', ']', ']', ':', 'loggable_messages', '=', '[', ']', 'for', 'msg', 'in', 'messages', ':', 'if', 'isinstance', '(', 'msg', '.', 'get', '(', '""', 'content', '""', ')', ',', 'list', ')', ':', 'new_content', '=', '[', ']', 'for', 'content', 'in', 'msg', '[', '""', 'content', '""', ']', ':', 'if', 'content', '.', 'get', '(', '""', 'type', '""', ')', '==', '""', 'image', '""', ':', 'new_content', '.', 'append', '(', '{', '""', 'type', '""', ':', '""', 'image', '""', ',', '""', 'image_url', '""', ':', '{', '""', 'url', '""', ':', '""', '[BASE64_IMAGE_DATA]', '""', '}', '}', ')', 'else', ':', 'new_content', '.', 'append', '(', 'content', ')', 'loggable_messages', '.', 'append', '(', '{', '""', 'role', '""', ':', 'msg', '[', '""', 'role', '""', ']', ',', '""', 'content', '""', ':', 'new_content', '}', ')', 'else', ':', 'loggable_messages', '.', 'append', '(', 'msg', ')', 'return', 'loggable_messages']",Filter and sanitize messages for logging by replacing image data with placeholders.,"['Filter', 'and', 'sanitize', 'messages', 'for', 'logging', 'by', 'replacing', 'image', 'data', 'with', 'placeholders', '.']"
348,"def _build_split_documents(row, chunks: List[str]) -> List[dict[str, Any]]:
    
    documents: List[dict] = []

    for i, text in enumerate(chunks):
        if text is None or not text.strip():
            continue

        metadata = row.metadata if hasattr(row, ""metadata"") and isinstance(row.metadata, dict) else {}
        metadata = copy.deepcopy(metadata)

        metadata[""content""] = text

        documents.append({""document_type"": ContentTypeEnum.TEXT.value, ""metadata"": metadata, ""uuid"": str(uuid.uuid4())})

    return documents","['def', '_build_split_documents', '(', 'row', ',', 'chunks', ':', 'List', '[', 'str', ']', ')', '-', '>', 'List', '[', 'dict', '[', 'str', ',', 'Any', ']', ']', ':', 'documents', ':', 'List', '[', 'dict', ']', '=', '[', ']', 'for', 'i', ',', 'text', 'in', 'enumerate', '(', 'chunks', ')', ':', 'if', 'text', 'is', 'None', 'or', 'not', 'text', '.', 'strip', '(', ')', ':', 'continue', 'metadata', '=', 'row', '.', 'metadata', 'if', 'hasattr', '(', 'row', ',', '""', 'metadata', '""', ')', 'and', 'isinstance', '(', 'row', '.', 'metadata', ',', 'dict', ')', 'else', '{', '}', 'metadata', '=', 'copy', '.', 'deepcopy', '(', 'metadata', ')', 'metadata', '[', '""', 'content', '""', ']', '=', 'text', 'documents', '.', 'append', '(', '{', '""', 'document_type', '""', ':', 'ContentTypeEnum', '.', 'TEXT', '.', 'value', ',', '""', 'metadata', '""', ':', 'metadata', ',', '""', 'uuid', '""', ':', 'str', '(', 'uuid', '.', 'uuid4', '(', ')', ')', '}', ')', 'return', 'documents']",Transform text chunks into structured document dictionaries with metadata and unique identifiers.,"['Transform', 'text', 'chunks', 'into', 'structured', 'document', 'dictionaries', 'with', 'metadata', 'and', 'unique', 'identifiers', '.']"
354,"def try_add_cuda_lib_path():
    
    required_submodules = [""cuda_runtime"", ""cublas""]
    cuda_versions = [""11"", ""12""]

    module_spec = find_spec(""nvidia"")
    if not module_spec:
        return

    nvidia_lib_root = Path(module_spec.submodule_search_locations[0])

    for submodule in required_submodules:
        for ver in cuda_versions:
            try:
                package_name = f""nvidia_{submodule}_cu{ver}""
                _ = distribution(package_name)

                lib_path = nvidia_lib_root / submodule / ""bin""
                os.add_dll_directory(str(lib_path))
                os.environ[""PATH""] = str(lib_path) + \
                    os.pathsep + os.environ[""PATH""]
                logging.debug(f""Added {lib_path} to PATH"")
            except PackageNotFoundError:
                logging.debug(f""{package_name} not found"")","['def', 'try_add_cuda_lib_path', '(', ')', ':', 'required_submodules', '=', '[', '""', 'cuda_runtime', '""', ',', '""', 'cublas', '""', ']', 'cuda_versions', '=', '[', '""', '11', '""', ',', '""', '12', '""', ']', 'module_spec', '=', 'find_spec', '(', '""', 'nvidia', '""', ')', 'if', 'not', 'module_spec', ':', 'return', 'nvidia_lib_root', '=', 'Path', '(', 'module_spec', '.', 'submodule_search_locations', '[', '0', ']', ')', 'for', 'submodule', 'in', 'required_submodules', ':', 'for', 'ver', 'in', 'cuda_versions', ':', 'try', ':', 'package_name', '=', 'f', '""', 'nvidia_', '{', 'submodule', '}', '_cu', '{', 'ver', '}', '""', '_', '=', 'distribution', '(', 'package_name', ')', 'lib_path', '=', 'nvidia_lib_root', '/', 'submodule', '/', '""', 'bin', '""', 'os', '.', 'add_dll_directory', '(', 'str', '(', 'lib_path', ')', ')', 'os', '.', 'environ', '[', '""', 'PATH', '""', ']', '=', 'str', '(', 'lib_path', ')', '+', '\\\n', 'os', '.', 'pathsep', '+', 'os', '.', 'environ', '[', '""', 'PATH', '""', ']', 'logging', '.', 'debug', '(', 'f', '""', 'Added ', '{', 'lib_path', '}', ' to PATH', '""', ')', 'except', 'PackageNotFoundError', ':', 'logging', '.', 'debug', '(', 'f', '""', '{', 'package_name', '}', ' not found', '""', ')']",Add CUDA library paths to system environment if NVIDIA modules are available,"['Add', 'CUDA', 'library', 'paths', 'to', 'system', 'environment', 'if', 'NVIDIA', 'modules', 'are', 'available']"
356,"def parse_size(size):
    
    units = {""KB"": 1024, ""MB"": 1024**2, ""GB"": 1024**3}
    if size.isdigit():
        return int(size)
    unit = size[-2:].upper()
    number = size[:-2]
    if unit in units and number.isdigit():
        return int(number) * units[unit]
    raise ValueError(""Size must be in the format <number>[KB|MB|GB]."")","['def', 'parse_size', '(', 'size', ')', ':', 'units', '=', '{', '""', 'KB', '""', ':', '1024', ',', '""', 'MB', '""', ':', '1024', '*', '*', '2', ',', '""', 'GB', '""', ':', '1024', '*', '*', '3', '}', 'if', 'size', '.', 'isdigit', '(', ')', ':', 'return', 'int', '(', 'size', ')', 'unit', '=', 'size', '[', '-', '2', ':', ']', '.', 'upper', '(', ')', 'number', '=', 'size', '[', ':', '-', '2', ']', 'if', 'unit', 'in', 'units', 'and', 'number', '.', 'isdigit', '(', ')', ':', 'return', 'int', '(', 'number', ')', '*', 'units', '[', 'unit', ']', 'raise', 'ValueError', '(', '""', 'Size must be in the format <number>[KB|MB|GB].', '""', ')']","Convert size string to bytes, supporting KB, MB, GB units.","['Convert', 'size', 'string', 'to', 'bytes', ',', 'supporting', 'KB', ',', 'MB', ',', 'GB', 'units', '.']"
360,"def package_template_features(
    *,
    hit_features: Sequence[Mapping[str, Any]],
    include_ligand_features: bool,
) -> Mapping[str, Any]:
  

  features_to_include = set(_POLYMER_FEATURES)
  if include_ligand_features:
    features_to_include.update(_LIGAND_FEATURES)

  features = {
      feat: [single_hit_features[feat] for single_hit_features in hit_features]
      for feat in features_to_include
  }

  stacked_features = {}
  for k, v in features.items():
    if k in _POLYMER_FEATURES:
      v = np.stack(v, axis=0) if v else np.array([], dtype=_POLYMER_FEATURES[k])
    stacked_features[k] = v

  return stacked_features","['def', 'package_template_features', '(', '*', ',', 'hit_features', ':', 'Sequence', '[', 'Mapping', '[', 'str', ',', 'Any', ']', ']', ',', 'include_ligand_features', ':', 'bool', ',', ')', '-', '>', 'Mapping', '[', 'str', ',', 'Any', ']', ':', 'features_to_include', '=', 'set', '(', '_POLYMER_FEATURES', ')', 'if', 'include_ligand_features', ':', 'features_to_include', '.', 'update', '(', '_LIGAND_FEATURES', ')', 'features', '=', '{', 'feat', ':', '[', 'single_hit_features', '[', 'feat', ']', 'for', 'single_hit_features', 'in', 'hit_features', ']', 'for', 'feat', 'in', 'features_to_include', '}', 'stacked_features', '=', '{', '}', 'for', 'k', ',', 'v', 'in', 'features', '.', 'items', '(', ')', ':', 'if', 'k', 'in', '_POLYMER_FEATURES', ':', 'v', '=', 'np', '.', 'stack', '(', 'v', ',', 'axis', '=', '0', ')', 'if', 'v', 'else', 'np', '.', 'array', '(', '[', ']', ',', 'dtype', '=', '_POLYMER_FEATURES', '[', 'k', ']', ')', 'stacked_features', '[', 'k', ']', '=', 'v', 'return', 'stacked_features']",Aggregate and stack selected polymer and ligand features from input data.,"['Aggregate', 'and', 'stack', 'selected', 'polymer', 'and', 'ligand', 'features', 'from', 'input', 'data', '.']"
361,"def rewrite_schema_for_sqlglot(
        cls, schema: str | SQLGlotSchemaType | BirdSampleType
    ) -> SQLGlotSchemaType:
        
        schema_dict = None
        if schema:
            if isinstance(schema, str):
                schema = cls.extract_schema_from_ddls(schema)
                schema_dict = cls.format_schema(schema)
            elif _isinstance_sqlglot_schema_type(schema):
                schema_dict = schema
            elif _isinstance_bird_sample_type(schema):
                schema_dict = cls._get_schema_from_bird_sample(schema)
            elif _isinstance_ddl_schema_type(schema):
                schema_dict = cls.format_schema(schema)
            else:
                raise TypeError(f""Unsupported schema type: {type(schema)}"")
        return schema_dict","['def', 'rewrite_schema_for_sqlglot', '(', 'cls', ',', 'schema', ':', 'str', '|', 'SQLGlotSchemaType', '|', 'BirdSampleType', ')', '-', '>', 'SQLGlotSchemaType', ':', 'schema_dict', '=', 'None', 'if', 'schema', ':', 'if', 'isinstance', '(', 'schema', ',', 'str', ')', ':', 'schema', '=', 'cls', '.', 'extract_schema_from_ddls', '(', 'schema', ')', 'schema_dict', '=', 'cls', '.', 'format_schema', '(', 'schema', ')', 'elif', '_isinstance_sqlglot_schema_type', '(', 'schema', ')', ':', 'schema_dict', '=', 'schema', 'elif', '_isinstance_bird_sample_type', '(', 'schema', ')', ':', 'schema_dict', '=', 'cls', '.', '_get_schema_from_bird_sample', '(', 'schema', ')', 'elif', '_isinstance_ddl_schema_type', '(', 'schema', ')', ':', 'schema_dict', '=', 'cls', '.', 'format_schema', '(', 'schema', ')', 'else', ':', 'raise', 'TypeError', '(', 'f', '""', 'Unsupported schema type: ', '{', 'type', '(', 'schema', ')', '}', '""', ')', 'return', 'schema_dict']",Convert various schema formats into a standardized SQLGlot schema type.,"['Convert', 'various', 'schema', 'formats', 'into', 'a', 'standardized', 'SQLGlot', 'schema', 'type', '.']"
362,"def get_config_defaults() -> Dict[str, Any]:
    
    defaults = {
        'site': 'all',
        'model': 'gpt-4o-mini',
        'generate_mode': 'list', 
        'retrieval_backend': CONFIG.preferred_retrieval_endpoint,
        'prev': []
    }
    
    # Try to get preferred model from LLM config
    if hasattr(CONFIG, 'preferred_llm_provider') and CONFIG.preferred_llm_provider:
        llm_provider = CONFIG.get_llm_provider()
        if llm_provider and llm_provider.models:
            # Use the 'low' model as default for testing
            defaults['model'] = llm_provider.models.low or defaults['model']
    
    return defaults","['def', 'get_config_defaults', '(', ')', '-', '>', 'Dict', '[', 'str', ',', 'Any', ']', ':', 'defaults', '=', '{', ""'"", 'site', ""'"", ':', ""'"", 'all', ""'"", ',', ""'"", 'model', ""'"", ':', ""'"", 'gpt-4o-mini', ""'"", ',', ""'"", 'generate_mode', ""'"", ':', ""'"", 'list', ""'"", ',', ""'"", 'retrieval_backend', ""'"", ':', 'CONFIG', '.', 'preferred_retrieval_endpoint', ',', ""'"", 'prev', ""'"", ':', '[', ']', '}', '# Try to get preferred model from LLM config', 'if', 'hasattr', '(', 'CONFIG', ',', ""'"", 'preferred_llm_provider', ""'"", ')', 'and', 'CONFIG', '.', 'preferred_llm_provider', ':', 'llm_provider', '=', 'CONFIG', '.', 'get_llm_provider', '(', ')', 'if', 'llm_provider', 'and', 'llm_provider', '.', 'models', ':', ""# Use the 'low' model as default for testing"", 'defaults', '[', ""'"", 'model', ""'"", ']', '=', 'llm_provider', '.', 'models', '.', 'low', 'or', 'defaults', '[', ""'"", 'model', ""'"", ']', 'return', 'defaults']",Initialize default configuration settings with potential overrides from a preferred model provider.,"['Initialize', 'default', 'configuration', 'settings', 'with', 'potential', 'overrides', 'from', 'a', 'preferred', 'model', 'provider', '.']"
365,"def format(self) -> str:
        
        if len(self.ingestions) == 0 and len(self.queries) == 0:
            return ""There were no ingestion or query tasks detected.""

        instructions = ""Tasks:\n\n""
        order = 1

        if len(self.ingestions) > 0:
            instructions += ""Ingestions:\n""
            for ingestion in self.ingestions:
                instructions += f""{order}: {ingestion.path_or_url}\n""
                order += 1

            instructions += ""\n""

        if len(self.queries) > 0:
            instructions += ""Queries:\n""
            for query in self.queries:
                instructions += f""{order}: {query.query}\n""
                order += 1

        return instructions","['def', 'format', '(', 'self', ')', '-', '>', 'str', ':', 'if', 'len', '(', 'self', '.', 'ingestions', ')', '==', '0', 'and', 'len', '(', 'self', '.', 'queries', ')', '==', '0', ':', 'return', '""', 'There were no ingestion or query tasks detected.', '""', 'instructions', '=', '""', 'Tasks:', '\\n', '\\n', '""', 'order', '=', '1', 'if', 'len', '(', 'self', '.', 'ingestions', ')', '>', '0', ':', 'instructions', '+', '=', '""', 'Ingestions:', '\\n', '""', 'for', 'ingestion', 'in', 'self', '.', 'ingestions', ':', 'instructions', '+', '=', 'f', '""', '{', 'order', '}', ': ', '{', 'ingestion', '.', 'path_or_url', '}', '\\n', '""', 'order', '+', '=', '1', 'instructions', '+', '=', '""', '\\n', '""', 'if', 'len', '(', 'self', '.', 'queries', ')', '>', '0', ':', 'instructions', '+', '=', '""', 'Queries:', '\\n', '""', 'for', 'query', 'in', 'self', '.', 'queries', ':', 'instructions', '+', '=', 'f', '""', '{', 'order', '}', ': ', '{', 'query', '.', 'query', '}', '\\n', '""', 'order', '+', '=', '1', 'return', 'instructions']",Generate a formatted task list from ingestion and query data.,"['Generate', 'a', 'formatted', 'task', 'list', 'from', 'ingestion', 'and', 'query', 'data', '.']"
367,"def _log_debug(self, message: str, **kwargs) -> None:
        
        if self.server.debug:
            print(f""DEBUG: {message}"")
            if kwargs:
                print(json.dumps(kwargs, indent=2))","['def', '_log_debug', '(', 'self', ',', 'message', ':', 'str', ',', '*', '*', 'kwargs', ')', '-', '>', 'None', ':', 'if', 'self', '.', 'server', '.', 'debug', ':', 'print', '(', 'f', '""', 'DEBUG: ', '{', 'message', '}', '""', ')', 'if', 'kwargs', ':', 'print', '(', 'json', '.', 'dumps', '(', 'kwargs', ',', 'indent', '=', '2', ')', ')']",Logs debug messages and optional data if debugging is enabled.,"['Logs', 'debug', 'messages', 'and', 'optional', 'data', 'if', 'debugging', 'is', 'enabled', '.']"
368,"def load_and_preprocess_image(image_path, target_img_size):
    
    image = cv2.imread(image_path)
    if image is None:
        raise ValueError(f""Failed to load image from path: {image_path}"")

    # Resize and pad the image to the target size
    resized_image = resize_image(image, target_img_size)

    # Normalize the image (assuming model expects normalized input)
    normalized_image = resized_image.astype(np.float32) / 255.0

    # Expand dimensions to match the model's input shape
    input_data = np.expand_dims(normalized_image, axis=0)  # Add batch dimension
    return resized_image, input_data","['def', 'load_and_preprocess_image', '(', 'image_path', ',', 'target_img_size', ')', ':', 'image', '=', 'cv2', '.', 'imread', '(', 'image_path', ')', 'if', 'image', 'is', 'None', ':', 'raise', 'ValueError', '(', 'f', '""', 'Failed to load image from path: ', '{', 'image_path', '}', '""', ')', '# Resize and pad the image to the target size', 'resized_image', '=', 'resize_image', '(', 'image', ',', 'target_img_size', ')', '# Normalize the image (assuming model expects normalized input)', 'normalized_image', '=', 'resized_image', '.', 'astype', '(', 'np', '.', 'float32', ')', '/', '255.0', ""# Expand dimensions to match the model's input shape"", 'input_data', '=', 'np', '.', 'expand_dims', '(', 'normalized_image', ',', 'axis', '=', '0', ')', '# Add batch dimension', 'return', 'resized_image', ',', 'input_data']","Load, resize, normalize, and prepare image for model input processing.","['Load', ',', 'resize', ',', 'normalize', ',', 'and', 'prepare', 'image', 'for', 'model', 'input', 'processing', '.']"
370,"def _prepare_toolset(self) -> None:
    
    # For each API, get the first version and the first spec of that version.
    spec_str = self._apihub_client.get_spec_content(self._apihub_resource_name)
    spec_dict = yaml.safe_load(spec_str)
    if not spec_dict:
      return

    self.name = self.name or _to_snake_case(
        spec_dict.get('info', {}).get('title', 'unnamed')
    )
    self.description = self.description or spec_dict.get('info', {}).get(
        'description', ''
    )
    self._openapi_toolset = OpenAPIToolset(
        spec_dict=spec_dict,
        auth_credential=self._auth_credential,
        auth_scheme=self._auth_scheme,
        tool_filter=self.tool_filter,
    )","['def', '_prepare_toolset', '(', 'self', ')', '-', '>', 'None', ':', '# For each API, get the first version and the first spec of that version.', 'spec_str', '=', 'self', '.', '_apihub_client', '.', 'get_spec_content', '(', 'self', '.', '_apihub_resource_name', ')', 'spec_dict', '=', 'yaml', '.', 'safe_load', '(', 'spec_str', ')', 'if', 'not', 'spec_dict', ':', 'return', 'self', '.', 'name', '=', 'self', '.', 'name', 'or', '_to_snake_case', '(', 'spec_dict', '.', 'get', '(', ""'"", 'info', ""'"", ',', '{', '}', ')', '.', 'get', '(', ""'"", 'title', ""'"", ',', ""'"", 'unnamed', ""'"", ')', ')', 'self', '.', 'description', '=', 'self', '.', 'description', 'or', 'spec_dict', '.', 'get', '(', ""'"", 'info', ""'"", ',', '{', '}', ')', '.', 'get', '(', ""'"", 'description', ""'"", ',', ""'"", ""'"", ')', 'self', '.', '_openapi_toolset', '=', 'OpenAPIToolset', '(', 'spec_dict', '=', 'spec_dict', ',', 'auth_credential', '=', 'self', '.', '_auth_credential', ',', 'auth_scheme', '=', 'self', '.', '_auth_scheme', ',', 'tool_filter', '=', 'self', '.', 'tool_filter', ',', ')']",Initialize API toolset with specifications and authentication details.,"['Initialize', 'API', 'toolset', 'with', 'specifications', 'and', 'authentication', 'details', '.']"
371,"def add_message_pair(self, user_message: str, assistant_message: str):
        
        if not any(entry[""user""] == user_message for entry in self.cache):
            self.cache.append({""user"": user_message, ""assistant"": assistant_message})
            self._save()","['def', 'add_message_pair', '(', 'self', ',', 'user_message', ':', 'str', ',', 'assistant_message', ':', 'str', ')', ':', 'if', 'not', 'any', '(', 'entry', '[', '""', 'user', '""', ']', '==', 'user_message', 'for', 'entry', 'in', 'self', '.', 'cache', ')', ':', 'self', '.', 'cache', '.', 'append', '(', '{', '""', 'user', '""', ':', 'user_message', ',', '""', 'assistant', '""', ':', 'assistant_message', '}', ')', 'self', '.', '_save', '(', ')']",Add unique user-assistant message pairs to cache and save changes.,"['Add', 'unique', 'user-assistant', 'message', 'pairs', 'to', 'cache', 'and', 'save', 'changes', '.']"
372,"def clean_response(cls, content: str) -> Dict[str, Any]:
        
        cleaned = re.sub(r""```(?:json)?\s*"", """", content).strip()
        match = re.search(r""(\{.*\})"", cleaned, re.S)
        if not match:
            logger.error(""Failed to parse JSON from content: %r"", content)
            return {}
        return json.loads(match.group(1))","['def', 'clean_response', '(', 'cls', ',', 'content', ':', 'str', ')', '-', '>', 'Dict', '[', 'str', ',', 'Any', ']', ':', 'cleaned', '=', 're', '.', 'sub', '(', 'r', '""', '```(?:json)?', '\\', 's*', '""', ',', '""', '""', ',', 'content', ')', '.', 'strip', '(', ')', 'match', '=', 're', '.', 'search', '(', 'r', '""', '(', '\\', '{', '.*', '\\', '})', '""', ',', 'cleaned', ',', 're', '.', 'S', ')', 'if', 'not', 'match', ':', 'logger', '.', 'error', '(', '""', 'Failed to parse JSON from content: ', '%r', '""', ',', 'content', ')', 'return', '{', '}', 'return', 'json', '.', 'loads', '(', 'match', '.', 'group', '(', '1', ')', ')']","Extract and parse JSON from a string, logging errors if parsing fails.","['Extract', 'and', 'parse', 'JSON', 'from', 'a', 'string', ',', 'logging', 'errors', 'if', 'parsing', 'fails', '.']"
373,"def _create_session(self) -> requests.Session:
        
        session = requests.Session()

        # Configure automatic retries with exponential backoff
        retry_strategy = Retry(
            total=self.max_retries,
            backoff_factor=self.retry_backoff_factor,
            status_forcelist=[429, 500, 502, 503, 504],
            allowed_methods={""HEAD"", ""GET"", ""POST"", ""OPTIONS""},
        )

        adapter = HTTPAdapter(max_retries=retry_strategy)

        # Set up headers
        headers = {""Accept"": ""application/json""}
        if self.api_key:
            headers[""x-api-key""] = self.api_key

        session.headers.update(headers)

        return session","['def', '_create_session', '(', 'self', ')', '-', '>', 'requests', '.', 'Session', ':', 'session', '=', 'requests', '.', 'Session', '(', ')', '# Configure automatic retries with exponential backoff', 'retry_strategy', '=', 'Retry', '(', 'total', '=', 'self', '.', 'max_retries', ',', 'backoff_factor', '=', 'self', '.', 'retry_backoff_factor', ',', 'status_forcelist', '=', '[', '429', ',', '500', ',', '502', ',', '503', ',', '504', ']', ',', 'allowed_methods', '=', '{', '""', 'HEAD', '""', ',', '""', 'GET', '""', ',', '""', 'POST', '""', ',', '""', 'OPTIONS', '""', '}', ',', ')', 'adapter', '=', 'HTTPAdapter', '(', 'max_retries', '=', 'retry_strategy', ')', '# Set up headers', 'headers', '=', '{', '""', 'Accept', '""', ':', '""', 'application/json', '""', '}', 'if', 'self', '.', 'api_key', ':', 'headers', '[', '""', 'x-api-key', '""', ']', '=', 'self', '.', 'api_key', 'session', '.', 'headers', '.', 'update', '(', 'headers', ')', 'return', 'session']",Establishes a configured HTTP session with retry logic and custom headers.,"['Establishes', 'a', 'configured', 'HTTP', 'session', 'with', 'retry', 'logic', 'and', 'custom', 'headers', '.']"
374,"def _snowflake_to_iso(snowflake: str) -> str:
        
        if not DiscordRetrieveTool._is_snowflake(snowflake):
            raise ValueError(f""Invalid snowflake ID: {snowflake}"")

        # Discord epoch (2015-01-01)
        discord_epoch = 1420070400000

        # Convert ID to int and shift right 22 bits to get timestamp
        timestamp_ms = (int(snowflake) >> 22) + discord_epoch

        # Convert to datetime and format as ISO string
        dt = datetime.fromtimestamp(timestamp_ms / 1000.0, tz=timezone.utc)
        return dt.isoformat()","['def', '_snowflake_to_iso', '(', 'snowflake', ':', 'str', ')', '-', '>', 'str', ':', 'if', 'not', 'DiscordRetrieveTool', '.', '_is_snowflake', '(', 'snowflake', ')', ':', 'raise', 'ValueError', '(', 'f', '""', 'Invalid snowflake ID: ', '{', 'snowflake', '}', '""', ')', '# Discord epoch (2015-01-01)', 'discord_epoch', '=', '1420070400000', '# Convert ID to int and shift right 22 bits to get timestamp', 'timestamp_ms', '=', '(', 'int', '(', 'snowflake', ')', '>>', '22', ')', '+', 'discord_epoch', '# Convert to datetime and format as ISO string', 'dt', '=', 'datetime', '.', 'fromtimestamp', '(', 'timestamp_ms', '/', '1000.0', ',', 'tz', '=', 'timezone', '.', 'utc', ')', 'return', 'dt', '.', 'isoformat', '(', ')']",Convert Discord snowflake ID to ISO 8601 timestamp format.,"['Convert', 'Discord', 'snowflake', 'ID', 'to', 'ISO', '8601', 'timestamp', 'format', '.']"
375,"def wait_for_health(url: str, timeout: int = 300) -> bool:
    
    print(f""Waiting for {url} to be healthy..."")
    start_time = time.time()

    while time.time() - start_time < timeout:
        try:
            response = requests.get(f""{url}/health"", timeout=5)
            if response.status_code == 200:
                print(""âœ“ Service is healthy"")
                return True
        except requests.exceptions.RequestException:
            pass

        time.sleep(2)
        print(""."", end="""", flush=True)

    print(""\nâœ— Service health check timed out"")
    return False","['def', 'wait_for_health', '(', 'url', ':', 'str', ',', 'timeout', ':', 'int', '=', '300', ')', '-', '>', 'bool', ':', 'print', '(', 'f', '""', 'Waiting for ', '{', 'url', '}', ' to be healthy...', '""', ')', 'start_time', '=', 'time', '.', 'time', '(', ')', 'while', 'time', '.', 'time', '(', ')', '-', 'start_time', '<', 'timeout', ':', 'try', ':', 'response', '=', 'requests', '.', 'get', '(', 'f', '""', '{', 'url', '}', '/health', '""', ',', 'timeout', '=', '5', ')', 'if', 'response', '.', 'status_code', '==', '200', ':', 'print', '(', '""', 'âœ“ Service is healthy', '""', ')', 'return', 'True', 'except', 'requests', '.', 'exceptions', '.', 'RequestException', ':', 'pass', 'time', '.', 'sleep', '(', '2', ')', 'print', '(', '""', '.', '""', ',', 'end', '=', '""', '""', ',', 'flush', '=', 'True', ')', 'print', '(', '""', '\\n', 'âœ— Service health check timed out', '""', ')', 'return', 'False']",Check if a service is healthy within a specified timeout period.,"['Check', 'if', 'a', 'service', 'is', 'healthy', 'within', 'a', 'specified', 'timeout', 'period', '.']"
377,"def configure(cls, **kwargs):
        
        for key, value in kwargs.items():
            key = key.strip().lower()
            if hasattr(cls, key):
                if key in cls.parser_keywords:
                    setattr(cls, key, value)
                else:
                    # Yup, no fun allowed LOL
                    raise AttributeError(f'Unknown parser argument: ""{key}""; maybe you meant {cls.parser_keywords}?')
            else:
                raise ValueError(f'Unknown parser argument: ""{key}""; maybe you meant {cls.parser_keywords}?')

        if not kwargs:
            raise AttributeError(f'You must pass a keyword to configure, current keywords: {cls.parser_keywords}?')","['def', 'configure', '(', 'cls', ',', '*', '*', 'kwargs', ')', ':', 'for', 'key', ',', 'value', 'in', 'kwargs', '.', 'items', '(', ')', ':', 'key', '=', 'key', '.', 'strip', '(', ')', '.', 'lower', '(', ')', 'if', 'hasattr', '(', 'cls', ',', 'key', ')', ':', 'if', 'key', 'in', 'cls', '.', 'parser_keywords', ':', 'setattr', '(', 'cls', ',', 'key', ',', 'value', ')', 'else', ':', '# Yup, no fun allowed LOL', 'raise', 'AttributeError', '(', 'f', ""'"", 'Unknown parser argument: ', '""', '{', 'key', '}', '""', '; maybe you meant ', '{', 'cls', '.', 'parser_keywords', '}', '?', ""'"", ')', 'else', ':', 'raise', 'ValueError', '(', 'f', ""'"", 'Unknown parser argument: ', '""', '{', 'key', '}', '""', '; maybe you meant ', '{', 'cls', '.', 'parser_keywords', '}', '?', ""'"", ')', 'if', 'not', 'kwargs', ':', 'raise', 'AttributeError', '(', 'f', ""'"", 'You must pass a keyword to configure, current keywords: ', '{', 'cls', '.', 'parser_keywords', '}', '?', ""'"", ')']","Configure class attributes using validated keyword arguments, raising errors for unknown keys.","['Configure', 'class', 'attributes', 'using', 'validated', 'keyword', 'arguments', ',', 'raising', 'errors', 'for', 'unknown', 'keys', '.']"
379,"def list_available(cls) -> List[str]:
        
        keys = list(Registrable._registry[cls].keys())
        default = cls.default_implementation

        if default is None:
            return keys
        elif default not in keys:
            raise ConfigurationError(
                f""Default implementation {default} is not registered""
            )
        else:
            return [default] + [k for k in keys if k != default]","['def', 'list_available', '(', 'cls', ')', '-', '>', 'List', '[', 'str', ']', ':', 'keys', '=', 'list', '(', 'Registrable', '.', '_registry', '[', 'cls', ']', '.', 'keys', '(', ')', ')', 'default', '=', 'cls', '.', 'default_implementation', 'if', 'default', 'is', 'None', ':', 'return', 'keys', 'elif', 'default', 'not', 'in', 'keys', ':', 'raise', 'ConfigurationError', '(', 'f', '""', 'Default implementation ', '{', 'default', '}', ' is not registered', '""', ')', 'else', ':', 'return', '[', 'default', ']', '+', '[', 'k', 'for', 'k', 'in', 'keys', 'if', 'k', '!=', 'default', ']']",Retrieve and prioritize registered implementations for a class,"['Retrieve', 'and', 'prioritize', 'registered', 'implementations', 'for', 'a', 'class']"
382,"def __str__(self) -> str:
        
        plan_str = """"
        if self.task is not None:
            plan_str += f""Task: {self.task}\n""
        for i, step in enumerate(self.steps):
            plan_str += f""{i}. {step.agent_name}: {step.title}\n   {step.details}\n""
        return plan_str","['def', '__str__', '(', 'self', ')', '-', '>', 'str', ':', 'plan_str', '=', '""', '""', 'if', 'self', '.', 'task', 'is', 'not', 'None', ':', 'plan_str', '+', '=', 'f', '""', 'Task: ', '{', 'self', '.', 'task', '}', '\\n', '""', 'for', 'i', ',', 'step', 'in', 'enumerate', '(', 'self', '.', 'steps', ')', ':', 'plan_str', '+', '=', 'f', '""', '{', 'i', '}', '. ', '{', 'step', '.', 'agent_name', '}', ': ', '{', 'step', '.', 'title', '}', '\\n', '{', 'step', '.', 'details', '}', '\\n', '""', 'return', 'plan_str']",Formats and returns a string representation of a task plan with steps.,"['Formats', 'and', 'returns', 'a', 'string', 'representation', 'of', 'a', 'task', 'plan', 'with', 'steps', '.']"
385,"def cleanup_memory(self, force: bool = False) -> None:
        
        # Run Python garbage collection
        gc.collect()
        
        # Empty CUDA cache if available
        if self.cuda_available:
            torch.cuda.empty_cache()
            
        # Log memory status after cleanup
        if force:
            info = self.get_memory_info()
            logger.info(
                f""Memory after cleanup: RAM: {info['ram_used_gb']:.2f}GB / {info['ram_total_gb']:.2f}GB, ""
                f""VRAM: {info.get('vram_used_gb', 0):.2f}GB / {info.get('vram_total_gb', 0):.2f}GB""
            )","['def', 'cleanup_memory', '(', 'self', ',', 'force', ':', 'bool', '=', 'False', ')', '-', '>', 'None', ':', '# Run Python garbage collection', 'gc', '.', 'collect', '(', ')', '# Empty CUDA cache if available', 'if', 'self', '.', 'cuda_available', ':', 'torch', '.', 'cuda', '.', 'empty_cache', '(', ')', '# Log memory status after cleanup', 'if', 'force', ':', 'info', '=', 'self', '.', 'get_memory_info', '(', ')', 'logger', '.', 'info', '(', 'f', '""', 'Memory after cleanup: RAM: ', '{', 'info', '[', ""'"", 'ram_used_gb', ""'"", ']', ':', '.2f', '}', 'GB / ', '{', 'info', '[', ""'"", 'ram_total_gb', ""'"", ']', ':', '.2f', '}', 'GB, ', '""', 'f', '""', 'VRAM: ', '{', 'info', '.', 'get', '(', ""'"", 'vram_used_gb', ""'"", ',', '0', ')', ':', '.2f', '}', 'GB / ', '{', 'info', '.', 'get', '(', ""'"", 'vram_total_gb', ""'"", ',', '0', ')', ':', '.2f', '}', 'GB', '""', ')']",Optimize system memory by clearing caches and logging status if forced,"['Optimize', 'system', 'memory', 'by', 'clearing', 'caches', 'and', 'logging', 'status', 'if', 'forced']"
386,"def add_prompt(
        self,
        prompt: Prompt,
    ) -> Prompt:
        

        # Check for duplicates
        existing = self._prompts.get(prompt.name)
        if existing:
            if self.warn_on_duplicate_prompts:
                logger.warning(f""Prompt already exists: {prompt.name}"")
            return existing

        self._prompts[prompt.name] = prompt
        return prompt","['def', 'add_prompt', '(', 'self', ',', 'prompt', ':', 'Prompt', ',', ')', '-', '>', 'Prompt', ':', '# Check for duplicates', 'existing', '=', 'self', '.', '_prompts', '.', 'get', '(', 'prompt', '.', 'name', ')', 'if', 'existing', ':', 'if', 'self', '.', 'warn_on_duplicate_prompts', ':', 'logger', '.', 'warning', '(', 'f', '""', 'Prompt already exists: ', '{', 'prompt', '.', 'name', '}', '""', ')', 'return', 'existing', 'self', '.', '_prompts', '[', 'prompt', '.', 'name', ']', '=', 'prompt', 'return', 'prompt']","Add unique prompt to collection, warn if duplicate exists","['Add', 'unique', 'prompt', 'to', 'collection', ',', 'warn', 'if', 'duplicate', 'exists']"
387,"def save_model_settings(model, state):
    
    if model == 'None':
        yield (""Not saving the settings because no model is selected in the menu."")
        return

    user_config = shared.load_user_config()
    model_regex = model + '$'  # For exact matches
    if model_regex not in user_config:
        user_config[model_regex] = {}

    for k in ui.list_model_elements():
        if k == 'loader' or k in loaders.loaders_and_params[state['loader']]:
            user_config[model_regex][k] = state[k]

    shared.user_config = user_config

    output = yaml.dump(user_config, sort_keys=False)
    p = Path(f'{shared.args.model_dir}/config-user.yaml')
    with open(p, 'w') as f:
        f.write(output)

    yield (f""Settings for `{model}` saved to `{p}`."")","['def', 'save_model_settings', '(', 'model', ',', 'state', ')', ':', 'if', 'model', '==', ""'"", 'None', ""'"", ':', 'yield', '(', '""', 'Not saving the settings because no model is selected in the menu.', '""', ')', 'return', 'user_config', '=', 'shared', '.', 'load_user_config', '(', ')', 'model_regex', '=', 'model', '+', ""'"", '$', ""'"", '# For exact matches', 'if', 'model_regex', 'not', 'in', 'user_config', ':', 'user_config', '[', 'model_regex', ']', '=', '{', '}', 'for', 'k', 'in', 'ui', '.', 'list_model_elements', '(', ')', ':', 'if', 'k', '==', ""'"", 'loader', ""'"", 'or', 'k', 'in', 'loaders', '.', 'loaders_and_params', '[', 'state', '[', ""'"", 'loader', ""'"", ']', ']', ':', 'user_config', '[', 'model_regex', ']', '[', 'k', ']', '=', 'state', '[', 'k', ']', 'shared', '.', 'user_config', '=', 'user_config', 'output', '=', 'yaml', '.', 'dump', '(', 'user_config', ',', 'sort_keys', '=', 'False', ')', 'p', '=', 'Path', '(', 'f', ""'"", '{', 'shared', '.', 'args', '.', 'model_dir', '}', '/config-user.yaml', ""'"", ')', 'with', 'open', '(', 'p', ',', ""'"", 'w', ""'"", ')', 'as', 'f', ':', 'f', '.', 'write', '(', 'output', ')', 'yield', '(', 'f', '""', 'Settings for `', '{', 'model', '}', '` saved to `', '{', 'p', '}', '`.', '""', ')']",Save model configuration to a YAML file if a model is selected.,"['Save', 'model', 'configuration', 'to', 'a', 'YAML', 'file', 'if', 'a', 'model', 'is', 'selected', '.']"
390,"def get_target_module(obj: object) -> Optional[str]:
    
    if not hasattr(obj, ""__module__""):
        return None

    fqn = f""{obj.__module__}.{obj.__name__}""
    return _PDOC_MODULE_EXPORT_MAPPINGS.get(fqn)","['def', 'get_target_module', '(', 'obj', ':', 'object', ')', '-', '>', 'Optional', '[', 'str', ']', ':', 'if', 'not', 'hasattr', '(', 'obj', ',', '""', '__module__', '""', ')', ':', 'return', 'None', 'fqn', '=', 'f', '""', '{', 'obj', '.', '__module__', '}', '.', '{', 'obj', '.', '__name__', '}', '""', 'return', '_PDOC_MODULE_EXPORT_MAPPINGS', '.', 'get', '(', 'fqn', ')']",Determine the module name of an object using its fully qualified name.,"['Determine', 'the', 'module', 'name', 'of', 'an', 'object', 'using', 'its', 'fully', 'qualified', 'name', '.']"
391,"def validate_graph_fields(self) -> ""GraphPromptOverrides"":
        
        allowed_fields = {""entity_extraction"", ""entity_resolution""}
        for field in self.model_fields:
            if field not in allowed_fields and getattr(self, field, None) is not None:
                raise ValueError(f""Field '{field}' is not allowed in graph prompt overrides"")
        return self","['def', 'validate_graph_fields', '(', 'self', ')', '-', '>', '""', 'GraphPromptOverrides', '""', ':', 'allowed_fields', '=', '{', '""', 'entity_extraction', '""', ',', '""', 'entity_resolution', '""', '}', 'for', 'field', 'in', 'self', '.', 'model_fields', ':', 'if', 'field', 'not', 'in', 'allowed_fields', 'and', 'getattr', '(', 'self', ',', 'field', ',', 'None', ')', 'is', 'not', 'None', ':', 'raise', 'ValueError', '(', 'f', '""', 'Field ', ""'"", '{', 'field', '}', ""'"", ' is not allowed in graph prompt overrides', '""', ')', 'return', 'self']",Ensure graph fields are valid by checking against allowed set,"['Ensure', 'graph', 'fields', 'are', 'valid', 'by', 'checking', 'against', 'allowed', 'set']"
393,"def _truncate_multimodal_text(self, contents: list[dict[str, Any]], n_tokens: int) -> list[dict[str, Any]]:
        
        tmp_contents = []
        for content in contents:
            if content[""type""] == ""text"":
                truncated_text = self._truncate_tokens(content[""text""], n_tokens)
                tmp_contents.append({""type"": ""text"", ""text"": truncated_text})
            else:
                tmp_contents.append(content)
        return tmp_contents","['def', '_truncate_multimodal_text', '(', 'self', ',', 'contents', ':', 'list', '[', 'dict', '[', 'str', ',', 'Any', ']', ']', ',', 'n_tokens', ':', 'int', ')', '-', '>', 'list', '[', 'dict', '[', 'str', ',', 'Any', ']', ']', ':', 'tmp_contents', '=', '[', ']', 'for', 'content', 'in', 'contents', ':', 'if', 'content', '[', '""', 'type', '""', ']', '==', '""', 'text', '""', ':', 'truncated_text', '=', 'self', '.', '_truncate_tokens', '(', 'content', '[', '""', 'text', '""', ']', ',', 'n_tokens', ')', 'tmp_contents', '.', 'append', '(', '{', '""', 'type', '""', ':', '""', 'text', '""', ',', '""', 'text', '""', ':', 'truncated_text', '}', ')', 'else', ':', 'tmp_contents', '.', 'append', '(', 'content', ')', 'return', 'tmp_contents']",Truncate text content in a multimodal list to a specified token limit.,"['Truncate', 'text', 'content', 'in', 'a', 'multimodal', 'list', 'to', 'a', 'specified', 'token', 'limit', '.']"
394,"def format_validation_errors(errors: list) -> str:
    
    formatted_errors = []

    for error in errors:
        loc = error.get(""loc"", [])
        msg = error.get(""msg"", """")
        error_type = error.get(""type"", """")

        # Build field path
        field_path = "" -> "".join(str(part) for part in loc if part != ""body"")

        # Format the error message with type information
        if field_path:
            if error_type:
                formatted_error = f""Field '{field_path}' ({error_type}): {msg}""
            else:
                formatted_error = f""Field '{field_path}': {msg}""
        else:
            formatted_error = msg

        formatted_errors.append(formatted_error)

    return ""; "".join(formatted_errors)","['def', 'format_validation_errors', '(', 'errors', ':', 'list', ')', '-', '>', 'str', ':', 'formatted_errors', '=', '[', ']', 'for', 'error', 'in', 'errors', ':', 'loc', '=', 'error', '.', 'get', '(', '""', 'loc', '""', ',', '[', ']', ')', 'msg', '=', 'error', '.', 'get', '(', '""', 'msg', '""', ',', '""', '""', ')', 'error_type', '=', 'error', '.', 'get', '(', '""', 'type', '""', ',', '""', '""', ')', '# Build field path', 'field_path', '=', '""', ' -> ', '""', '.', 'join', '(', 'str', '(', 'part', ')', 'for', 'part', 'in', 'loc', 'if', 'part', '!=', '""', 'body', '""', ')', '# Format the error message with type information', 'if', 'field_path', ':', 'if', 'error_type', ':', 'formatted_error', '=', 'f', '""', 'Field ', ""'"", '{', 'field_path', '}', ""'"", ' (', '{', 'error_type', '}', '): ', '{', 'msg', '}', '""', 'else', ':', 'formatted_error', '=', 'f', '""', 'Field ', ""'"", '{', 'field_path', '}', ""'"", ': ', '{', 'msg', '}', '""', 'else', ':', 'formatted_error', '=', 'msg', 'formatted_errors', '.', 'append', '(', 'formatted_error', ')', 'return', '""', '; ', '""', '.', 'join', '(', 'formatted_errors', ')']",Format a list of validation errors into a structured string message.,"['Format', 'a', 'list', 'of', 'validation', 'errors', 'into', 'a', 'structured', 'string', 'message', '.']"
395,"def check_hub_revision_exists(training_args: SFTConfig | GRPOConfig):
    
    if repo_exists(training_args.hub_model_id):
        if training_args.push_to_hub_revision is True:
            # First check if the revision exists
            revisions = [rev.name for rev in list_repo_refs(training_args.hub_model_id).branches]
            # If the revision exists, we next check it has a README file
            if training_args.hub_model_revision in revisions:
                repo_files = list_repo_files(
                    repo_id=training_args.hub_model_id, revision=training_args.hub_model_revision
                )
                if ""README.md"" in repo_files and training_args.overwrite_hub_revision is False:
                    raise ValueError(
                        f""Revision {training_args.hub_model_revision} already exists. ""
                        ""Use --overwrite_hub_revision to overwrite it.""
                    )","['def', 'check_hub_revision_exists', '(', 'training_args', ':', 'SFTConfig', '|', 'GRPOConfig', ')', ':', 'if', 'repo_exists', '(', 'training_args', '.', 'hub_model_id', ')', ':', 'if', 'training_args', '.', 'push_to_hub_revision', 'is', 'True', ':', '# First check if the revision exists', 'revisions', '=', '[', 'rev', '.', 'name', 'for', 'rev', 'in', 'list_repo_refs', '(', 'training_args', '.', 'hub_model_id', ')', '.', 'branches', ']', '# If the revision exists, we next check it has a README file', 'if', 'training_args', '.', 'hub_model_revision', 'in', 'revisions', ':', 'repo_files', '=', 'list_repo_files', '(', 'repo_id', '=', 'training_args', '.', 'hub_model_id', ',', 'revision', '=', 'training_args', '.', 'hub_model_revision', ')', 'if', '""', 'README.md', '""', 'in', 'repo_files', 'and', 'training_args', '.', 'overwrite_hub_revision', 'is', 'False', ':', 'raise', 'ValueError', '(', 'f', '""', 'Revision ', '{', 'training_args', '.', 'hub_model_revision', '}', ' already exists. ', '""', '""', 'Use --overwrite_hub_revision to overwrite it.', '""', ')']",Check if a specific model revision exists on the hub and validate its README presence.,"['Check', 'if', 'a', 'specific', 'model', 'revision', 'exists', 'on', 'the', 'hub', 'and', 'validate', 'its', 'README', 'presence', '.']"
396,"def save_partial_results(self, agent, task):
        
        try:
            if task in self.tasks and agent in self.completions and task in self.completions[agent]:
                task_client = self.tasks[task]
                overall = task_client.calculate_overall(self.completions[agent][task])
                output_dir = self.get_output_dir(agent, task)
                os.makedirs(output_dir, exist_ok=True)
                with open(os.path.join(output_dir, ""overall.json""), ""w"") as f:
                    f.write(json.dumps(overall, indent=4, ensure_ascii=False))
                return overall
        except Exception as e:
            print(ColorMessage.yellow(f""Warning: Failed to save partial results for {agent}/{task}: {str(e)}""))
        return None","['def', 'save_partial_results', '(', 'self', ',', 'agent', ',', 'task', ')', ':', 'try', ':', 'if', 'task', 'in', 'self', '.', 'tasks', 'and', 'agent', 'in', 'self', '.', 'completions', 'and', 'task', 'in', 'self', '.', 'completions', '[', 'agent', ']', ':', 'task_client', '=', 'self', '.', 'tasks', '[', 'task', ']', 'overall', '=', 'task_client', '.', 'calculate_overall', '(', 'self', '.', 'completions', '[', 'agent', ']', '[', 'task', ']', ')', 'output_dir', '=', 'self', '.', 'get_output_dir', '(', 'agent', ',', 'task', ')', 'os', '.', 'makedirs', '(', 'output_dir', ',', 'exist_ok', '=', 'True', ')', 'with', 'open', '(', 'os', '.', 'path', '.', 'join', '(', 'output_dir', ',', '""', 'overall.json', '""', ')', ',', '""', 'w', '""', ')', 'as', 'f', ':', 'f', '.', 'write', '(', 'json', '.', 'dumps', '(', 'overall', ',', 'indent', '=', '4', ',', 'ensure_ascii', '=', 'False', ')', ')', 'return', 'overall', 'except', 'Exception', 'as', 'e', ':', 'print', '(', 'ColorMessage', '.', 'yellow', '(', 'f', '""', 'Warning: Failed to save partial results for ', '{', 'agent', '}', '/', '{', 'task', '}', ': ', '{', 'str', '(', 'e', ')', '}', '""', ')', ')', 'return', 'None']","Save and log task completion results for agents, handling exceptions.","['Save', 'and', 'log', 'task', 'completion', 'results', 'for', 'agents', ',', 'handling', 'exceptions', '.']"
397,"def do_Tm(
        self,
        a: PDFStackT,
        b: PDFStackT,
        c: PDFStackT,
        d: PDFStackT,
        e: PDFStackT,
        f: PDFStackT,
    ) -> None:
        
        values = (a, b, c, d, e, f)
        matrix = safe_matrix(*values)

        if matrix is None:
            log.warning(
                f""Could not set text matrix because not all values in {values!r} can be parsed as floats""
            )
        else:
            self.textstate.matrix = matrix
            self.textstate.linematrix = (0, 0)","['def', 'do_Tm', '(', 'self', ',', 'a', ':', 'PDFStackT', ',', 'b', ':', 'PDFStackT', ',', 'c', ':', 'PDFStackT', ',', 'd', ':', 'PDFStackT', ',', 'e', ':', 'PDFStackT', ',', 'f', ':', 'PDFStackT', ',', ')', '-', '>', 'None', ':', 'values', '=', '(', 'a', ',', 'b', ',', 'c', ',', 'd', ',', 'e', ',', 'f', ')', 'matrix', '=', 'safe_matrix', '(', '*', 'values', ')', 'if', 'matrix', 'is', 'None', ':', 'log', '.', 'warning', '(', 'f', '""', 'Could not set text matrix because not all values in ', '{', 'values', '!r}', ' can be parsed as floats', '""', ')', 'else', ':', 'self', '.', 'textstate', '.', 'matrix', '=', 'matrix', 'self', '.', 'textstate', '.', 'linematrix', '=', '(', '0', ',', '0', ')']","Set text transformation matrix if values are valid floats, else log warning.","['Set', 'text', 'transformation', 'matrix', 'if', 'values', 'are', 'valid', 'floats', ',', 'else', 'log', 'warning', '.']"
398,"def get_tool_metadata(tool_name_or_schema: str | ToolSchema) -> ToolMetadata:
    
    tool_name: str | None = (
        tool_name_or_schema
        if isinstance(tool_name_or_schema, str)
        else tool_name_or_schema.get(""name"")
    )

    metadata = _tool_metadata.get(tool_name)

    if metadata is None:
        raise ValueError(f""Tool {tool_name} not found in metadata."")

    return metadata","['def', 'get_tool_metadata', '(', 'tool_name_or_schema', ':', 'str', '|', 'ToolSchema', ')', '-', '>', 'ToolMetadata', ':', 'tool_name', ':', 'str', '|', 'None', '=', '(', 'tool_name_or_schema', 'if', 'isinstance', '(', 'tool_name_or_schema', ',', 'str', ')', 'else', 'tool_name_or_schema', '.', 'get', '(', '""', 'name', '""', ')', ')', 'metadata', '=', '_tool_metadata', '.', 'get', '(', 'tool_name', ')', 'if', 'metadata', 'is', 'None', ':', 'raise', 'ValueError', '(', 'f', '""', 'Tool ', '{', 'tool_name', '}', ' not found in metadata.', '""', ')', 'return', 'metadata']","Retrieve tool metadata using name or schema, raising error if not found.","['Retrieve', 'tool', 'metadata', 'using', 'name', 'or', 'schema', ',', 'raising', 'error', 'if', 'not', 'found', '.']"
399,"def fetch_branches(owner: str, repo: str):
        

        response = requests.get(url, headers=headers)

        if response.status_code == 404:
            if not token:
                print(f""Error 404: Repository not found or is private.\n""
                      f""If this is a private repository, please provide a valid GitHub token via the 'token' argument or set the GITHUB_TOKEN environment variable."")
            else:
                print(f""Error 404: Repository not found or insufficient permissions with the provided token.\n""
                      f""Please verify the repository exists and the token has access to this repository."")
            return []
            
        if response.status_code != 200:
            print(f""Error fetching the branches of {owner}/{repo}: {response.status_code} - {response.text}"")
            return []

        return response.json()","['def', 'fetch_branches', '(', 'owner', ':', 'str', ',', 'repo', ':', 'str', ')', ':', 'response', '=', 'requests', '.', 'get', '(', 'url', ',', 'headers', '=', 'headers', ')', 'if', 'response', '.', 'status_code', '==', '404', ':', 'if', 'not', 'token', ':', 'print', '(', 'f', '""', 'Error 404: Repository not found or is private.', '\\n', '""', 'f', '""', 'If this is a private repository, please provide a valid GitHub token via the ', ""'"", 'token', ""'"", ' argument or set the GITHUB_TOKEN environment variable.', '""', ')', 'else', ':', 'print', '(', 'f', '""', 'Error 404: Repository not found or insufficient permissions with the provided token.', '\\n', '""', 'f', '""', 'Please verify the repository exists and the token has access to this repository.', '""', ')', 'return', '[', ']', 'if', 'response', '.', 'status_code', '!=', '200', ':', 'print', '(', 'f', '""', 'Error fetching the branches of ', '{', 'owner', '}', '/', '{', 'repo', '}', ': ', '{', 'response', '.', 'status_code', '}', ' - ', '{', 'response', '.', 'text', '}', '""', ')', 'return', '[', ']', 'return', 'response', '.', 'json', '(', ')']","Fetch GitHub repository branches, handling errors for access and existence.","['Fetch', 'GitHub', 'repository', 'branches', ',', 'handling', 'errors', 'for', 'access', 'and', 'existence', '.']"
400,"def from_runnable_config(
        cls, config: Optional[RunnableConfig] = None
    ) -> ""Configuration"":
        
        configurable = (
            config[""configurable""] if config and ""configurable"" in config else {}
        )
        
        # Get raw values from environment or config
        raw_values: dict[str, Any] = {
            name: os.environ.get(name.upper(), configurable.get(name))
            for name in cls.model_fields.keys()
        }
        
        # Filter out None values
        values = {k: v for k, v in raw_values.items() if v is not None}
        
        return cls(**values)","['def', 'from_runnable_config', '(', 'cls', ',', 'config', ':', 'Optional', '[', 'RunnableConfig', ']', '=', 'None', ')', '-', '>', '""', 'Configuration', '""', ':', 'configurable', '=', '(', 'config', '[', '""', 'configurable', '""', ']', 'if', 'config', 'and', '""', 'configurable', '""', 'in', 'config', 'else', '{', '}', ')', '# Get raw values from environment or config', 'raw_values', ':', 'dict', '[', 'str', ',', 'Any', ']', '=', '{', 'name', ':', 'os', '.', 'environ', '.', 'get', '(', 'name', '.', 'upper', '(', ')', ',', 'configurable', '.', 'get', '(', 'name', ')', ')', 'for', 'name', 'in', 'cls', '.', 'model_fields', '.', 'keys', '(', ')', '}', '# Filter out None values', 'values', '=', '{', 'k', ':', 'v', 'for', 'k', ',', 'v', 'in', 'raw_values', '.', 'items', '(', ')', 'if', 'v', 'is', 'not', 'None', '}', 'return', 'cls', '(', '*', '*', 'values', ')']",Create a configuration object using environment variables and optional settings.,"['Create', 'a', 'configuration', 'object', 'using', 'environment', 'variables', 'and', 'optional', 'settings', '.']"
401,"def get_model(cls, base_id, task_type):
        
        if base_id in cls.registry[""model_specific""]:
            return cls.registry[""model_specific""][base_id]
        elif task_type in cls.registry[""task_type""]:
            return cls.registry[""task_type""][task_type]
        else:
            raise ValueError(f""No model class found for model '{base_id}' or task type '{task_type}'"")","['def', 'get_model', '(', 'cls', ',', 'base_id', ',', 'task_type', ')', ':', 'if', 'base_id', 'in', 'cls', '.', 'registry', '[', '""', 'model_specific', '""', ']', ':', 'return', 'cls', '.', 'registry', '[', '""', 'model_specific', '""', ']', '[', 'base_id', ']', 'elif', 'task_type', 'in', 'cls', '.', 'registry', '[', '""', 'task_type', '""', ']', ':', 'return', 'cls', '.', 'registry', '[', '""', 'task_type', '""', ']', '[', 'task_type', ']', 'else', ':', 'raise', 'ValueError', '(', 'f', '""', 'No model class found for model ', ""'"", '{', 'base_id', '}', ""'"", ' or task type ', ""'"", '{', 'task_type', '}', ""'"", '""', ')']",Determine model class based on base ID or task type from registry,"['Determine', 'model', 'class', 'based', 'on', 'base', 'ID', 'or', 'task', 'type', 'from', 'registry']"
403,"def _compile_dependencies():
    
    if torch.distributed.get_rank() == 0:
        start_time = time.time()
        logger.info(""> Compiling dataset index builder..."")
        try:
            from core.datasets.utils import compile_helpers
            compile_helpers()
            logger.info(
                f"">>> Done with dataset index builder. Compilation time: {time.time() - start_time:.3f} seconds""
            )
        except Exception as e:
            logger.error(f""Failed to compile helpers: {e}"")
            raise","['def', '_compile_dependencies', '(', ')', ':', 'if', 'torch', '.', 'distributed', '.', 'get_rank', '(', ')', '==', '0', ':', 'start_time', '=', 'time', '.', 'time', '(', ')', 'logger', '.', 'info', '(', '""', '> Compiling dataset index builder...', '""', ')', 'try', ':', 'from', 'core', '.', 'datasets', '.', 'utils', 'import', 'compile_helpers', 'compile_helpers', '(', ')', 'logger', '.', 'info', '(', 'f', '""', '>>> Done with dataset index builder. Compilation time: ', '{', 'time', '.', 'time', '(', ')', '-', 'start_time', ':', '.3f', '}', ' seconds', '""', ')', 'except', 'Exception', 'as', 'e', ':', 'logger', '.', 'error', '(', 'f', '""', 'Failed to compile helpers: ', '{', 'e', '}', '""', ')', 'raise']",Compile dataset index builder on the main process and log the duration.,"['Compile', 'dataset', 'index', 'builder', 'on', 'the', 'main', 'process', 'and', 'log', 'the', 'duration', '.']"
404,"def crawl(self) -> List[Dict]:
        
        to_visit = [self.base_url]
        results = []
        
        while to_visit and len(self.visited) < self.max_pages:
            url = to_visit.pop(0)
            
            if url in self.visited:
                continue
                
            print(f""Crawling: {url}"")
            content = self.extract_page_content(url)
            
            if content:
                self.visited.add(url)
                results.append(content)
                
                # Add new URLs to visit
                new_urls = [url for url in content[""links""] 
                          if url not in self.visited 
                          and url not in to_visit]
                to_visit.extend(new_urls)
        
        return results","['def', 'crawl', '(', 'self', ')', '-', '>', 'List', '[', 'Dict', ']', ':', 'to_visit', '=', '[', 'self', '.', 'base_url', ']', 'results', '=', '[', ']', 'while', 'to_visit', 'and', 'len', '(', 'self', '.', 'visited', ')', '<', 'self', '.', 'max_pages', ':', 'url', '=', 'to_visit', '.', 'pop', '(', '0', ')', 'if', 'url', 'in', 'self', '.', 'visited', ':', 'continue', 'print', '(', 'f', '""', 'Crawling: ', '{', 'url', '}', '""', ')', 'content', '=', 'self', '.', 'extract_page_content', '(', 'url', ')', 'if', 'content', ':', 'self', '.', 'visited', '.', 'add', '(', 'url', ')', 'results', '.', 'append', '(', 'content', ')', '# Add new URLs to visit', 'new_urls', '=', '[', 'url', 'for', 'url', 'in', 'content', '[', '""', 'links', '""', ']', 'if', 'url', 'not', 'in', 'self', '.', 'visited', 'and', 'url', 'not', 'in', 'to_visit', ']', 'to_visit', '.', 'extend', '(', 'new_urls', ')', 'return', 'results']",Web crawler iteratively collects page content and discovers new links.,"['Web', 'crawler', 'iteratively', 'collects', 'page', 'content', 'and', 'discovers', 'new', 'links', '.']"
405,"def clean_response(cls, content: str) -> Dict[str, Any]:
        
        cleaned = re.sub(r""```(?:json)?\s*"", """", content).strip()
        match = re.search(r""(\{.*\})"", cleaned, re.S)
        if not match:
            return {}
        return json.loads(match.group(1))","['def', 'clean_response', '(', 'cls', ',', 'content', ':', 'str', ')', '-', '>', 'Dict', '[', 'str', ',', 'Any', ']', ':', 'cleaned', '=', 're', '.', 'sub', '(', 'r', '""', '```(?:json)?', '\\', 's*', '""', ',', '""', '""', ',', 'content', ')', '.', 'strip', '(', ')', 'match', '=', 're', '.', 'search', '(', 'r', '""', '(', '\\', '{', '.*', '\\', '})', '""', ',', 'cleaned', ',', 're', '.', 'S', ')', 'if', 'not', 'match', ':', 'return', '{', '}', 'return', 'json', '.', 'loads', '(', 'match', '.', 'group', '(', '1', ')', ')']",Extracts and parses JSON data from a formatted string response.,"['Extracts', 'and', 'parses', 'JSON', 'data', 'from', 'a', 'formatted', 'string', 'response', '.']"
406,"def replace_message_at(
        self, index, text_content, image_content=None, image_detail=""high""
    ):
        
        if index < len(self.messages):
            self.messages[index] = {
                ""role"": self.messages[index][""role""],
                ""content"": [{""type"": ""text"", ""text"": text_content}],
            }
            if image_content:
                base64_image = self.encode_image(image_content)
                self.messages[index][""content""].append(
                    {
                        ""type"": ""image_url"",
                        ""image_url"": {
                            ""url"": f""data:image/png;base64,{base64_image}"",
                            ""detail"": image_detail,
                        },
                    }
                )","['def', 'replace_message_at', '(', 'self', ',', 'index', ',', 'text_content', ',', 'image_content', '=', 'None', ',', 'image_detail', '=', '""', 'high', '""', ')', ':', 'if', 'index', '<', 'len', '(', 'self', '.', 'messages', ')', ':', 'self', '.', 'messages', '[', 'index', ']', '=', '{', '""', 'role', '""', ':', 'self', '.', 'messages', '[', 'index', ']', '[', '""', 'role', '""', ']', ',', '""', 'content', '""', ':', '[', '{', '""', 'type', '""', ':', '""', 'text', '""', ',', '""', 'text', '""', ':', 'text_content', '}', ']', ',', '}', 'if', 'image_content', ':', 'base64_image', '=', 'self', '.', 'encode_image', '(', 'image_content', ')', 'self', '.', 'messages', '[', 'index', ']', '[', '""', 'content', '""', ']', '.', 'append', '(', '{', '""', 'type', '""', ':', '""', 'image_url', '""', ',', '""', 'image_url', '""', ':', '{', '""', 'url', '""', ':', 'f', '""', 'data:image/png;base64,', '{', 'base64_image', '}', '""', ',', '""', 'detail', '""', ':', 'image_detail', ',', '}', ',', '}', ')']",Update message content with optional image in a message list,"['Update', 'message', 'content', 'with', 'optional', 'image', 'in', 'a', 'message', 'list']"
407,"def _make_output(
        self,
        file_content: str,
        file_descriptor: str,
        init_line: int = 1,
        expand_tabs: bool = True,
    ) -> str:
        
        file_content = maybe_truncate(file_content)
        if expand_tabs:
            file_content = file_content.expandtabs()
        file_content = ""\n"".join(
            [f""{i + init_line:6}\t{line}"" for i, line in enumerate(file_content.split(""\n""))]
        )
        return (
            f""Here's the result of running `cat -n` on {file_descriptor}:\n"" + file_content + ""\n""
        )","['def', '_make_output', '(', 'self', ',', 'file_content', ':', 'str', ',', 'file_descriptor', ':', 'str', ',', 'init_line', ':', 'int', '=', '1', ',', 'expand_tabs', ':', 'bool', '=', 'True', ',', ')', '-', '>', 'str', ':', 'file_content', '=', 'maybe_truncate', '(', 'file_content', ')', 'if', 'expand_tabs', ':', 'file_content', '=', 'file_content', '.', 'expandtabs', '(', ')', 'file_content', '=', '""', '\\n', '""', '.', 'join', '(', '[', 'f', '""', '{', 'i', '+', 'init_line', ':', '6', '}', '\\t', '{', 'line', '}', '""', 'for', 'i', ',', 'line', 'in', 'enumerate', '(', 'file_content', '.', 'split', '(', '""', '\\n', '""', ')', ')', ']', ')', 'return', '(', 'f', '""', 'Here', ""'"", 's the result of running `cat -n` on ', '{', 'file_descriptor', '}', ':', '\\n', '""', '+', 'file_content', '+', '""', '\\n', '""', ')']",Format file content with line numbers and optional tab expansion,"['Format', 'file', 'content', 'with', 'line', 'numbers', 'and', 'optional', 'tab', 'expansion']"
408,"def get_transcript_object(self) -> aai.Transcript:
        
        logger.info(""Starting video transcription"")
        transcript = self.transcriber.transcribe(self.video_path)
        if transcript.status == ""error"":
            logger.error(f""Transcription failed: {transcript.error}"")
            raise ValueError(f""Transcription failed: {transcript.error}"")
        if not transcript.words:
            logger.warning(""No words found in transcript"")
        logger.info(""Transcription completed successfully!"")

        return transcript","['def', 'get_transcript_object', '(', 'self', ')', '-', '>', 'aai', '.', 'Transcript', ':', 'logger', '.', 'info', '(', '""', 'Starting video transcription', '""', ')', 'transcript', '=', 'self', '.', 'transcriber', '.', 'transcribe', '(', 'self', '.', 'video_path', ')', 'if', 'transcript', '.', 'status', '==', '""', 'error', '""', ':', 'logger', '.', 'error', '(', 'f', '""', 'Transcription failed: ', '{', 'transcript', '.', 'error', '}', '""', ')', 'raise', 'ValueError', '(', 'f', '""', 'Transcription failed: ', '{', 'transcript', '.', 'error', '}', '""', ')', 'if', 'not', 'transcript', '.', 'words', ':', 'logger', '.', 'warning', '(', '""', 'No words found in transcript', '""', ')', 'logger', '.', 'info', '(', '""', 'Transcription completed successfully!', '""', ')', 'return', 'transcript']","Transcribe video, handle errors, and return transcript object.","['Transcribe', 'video', ',', 'handle', 'errors', ',', 'and', 'return', 'transcript', 'object', '.']"
409,"def update_status(self, agent_name: str, ticker: Optional[str] = None, status: str = """", analysis: Optional[str] = None):
        
        if agent_name not in self.agent_status:
            self.agent_status[agent_name] = {""status"": """", ""ticker"": None}

        if ticker:
            self.agent_status[agent_name][""ticker""] = ticker
        if status:
            self.agent_status[agent_name][""status""] = status
        if analysis:
            self.agent_status[agent_name][""analysis""] = analysis
        
        # Set the timestamp as UTC datetime
        timestamp = datetime.now(timezone.utc).isoformat()
        self.agent_status[agent_name][""timestamp""] = timestamp

        # Notify all registered handlers
        for handler in self.update_handlers:
            handler(agent_name, ticker, status, analysis, timestamp)

        self._refresh_display()","['def', 'update_status', '(', 'self', ',', 'agent_name', ':', 'str', ',', 'ticker', ':', 'Optional', '[', 'str', ']', '=', 'None', ',', 'status', ':', 'str', '=', '""', '""', ',', 'analysis', ':', 'Optional', '[', 'str', ']', '=', 'None', ')', ':', 'if', 'agent_name', 'not', 'in', 'self', '.', 'agent_status', ':', 'self', '.', 'agent_status', '[', 'agent_name', ']', '=', '{', '""', 'status', '""', ':', '""', '""', ',', '""', 'ticker', '""', ':', 'None', '}', 'if', 'ticker', ':', 'self', '.', 'agent_status', '[', 'agent_name', ']', '[', '""', 'ticker', '""', ']', '=', 'ticker', 'if', 'status', ':', 'self', '.', 'agent_status', '[', 'agent_name', ']', '[', '""', 'status', '""', ']', '=', 'status', 'if', 'analysis', ':', 'self', '.', 'agent_status', '[', 'agent_name', ']', '[', '""', 'analysis', '""', ']', '=', 'analysis', '# Set the timestamp as UTC datetime', 'timestamp', '=', 'datetime', '.', 'now', '(', 'timezone', '.', 'utc', ')', '.', 'isoformat', '(', ')', 'self', '.', 'agent_status', '[', 'agent_name', ']', '[', '""', 'timestamp', '""', ']', '=', 'timestamp', '# Notify all registered handlers', 'for', 'handler', 'in', 'self', '.', 'update_handlers', ':', 'handler', '(', 'agent_name', ',', 'ticker', ',', 'status', ',', 'analysis', ',', 'timestamp', ')', 'self', '.', '_refresh_display', '(', ')']",Update agent status and notify handlers with timestamped changes,"['Update', 'agent', 'status', 'and', 'notify', 'handlers', 'with', 'timestamped', 'changes']"
411,"def register_mimetype_component_type(mimetype: str, component_type: Optional[str] = None):
    
    if not re.match(r""^[^/]+/[^/]+$"", mimetype):
        logger.warning(f""[registry] Suspicious mimetype format: {mimetype}"")
    _mimetype_to_component_type[mimetype] = component_type or ""generic""","['def', 'register_mimetype_component_type', '(', 'mimetype', ':', 'str', ',', 'component_type', ':', 'Optional', '[', 'str', ']', '=', 'None', ')', ':', 'if', 'not', 're', '.', 'match', '(', 'r', '""', '^[^/]+/[^/]+$', '""', ',', 'mimetype', ')', ':', 'logger', '.', 'warning', '(', 'f', '""', '[registry] Suspicious mimetype format: ', '{', 'mimetype', '}', '""', ')', '_mimetype_to_component_type', '[', 'mimetype', ']', '=', 'component_type', 'or', '""', 'generic', '""']","Validate and map mimetype to component type, defaulting to ""generic"".","['Validate', 'and', 'map', 'mimetype', 'to', 'component', 'type', ',', 'defaulting', 'to', '``', 'generic', ""''"", '.']"
412,"def is_ecr_image(image_uri: str) -> bool:
    
    import re

    try:
        else:
            parse_uri = urlparse(image_uri)

        hostname = parse_uri.netloc.lower()

        # Check for malformed hostnames (double dots, etc.)
        if "".."" in hostname or hostname.startswith(""."") or hostname.endswith("".""):
            return False

        # Ensure the hostname ends with amazonaws.com (proper domain validation)
        if not hostname.endswith("".amazonaws.com""):
            return False

        # Check for proper ECR hostname structure: account-id.dkr.ecr.region.amazonaws.com
        ecr_pattern = r""^\d{12}\.dkr\.ecr\.[a-z0-9-]+\.amazonaws\.com$""

        return bool(re.match(ecr_pattern, hostname))

    except Exception:
        return False","['def', 'is_ecr_image', '(', 'image_uri', ':', 'str', ')', '-', '>', 'bool', ':', 'import', 're', 'try', ':', 'else', ':', 'parse_uri', '=', 'urlparse', '(', 'image_uri', ')', 'hostname', '=', 'parse_uri', '.', 'netloc', '.', 'lower', '(', ')', '# Check for malformed hostnames (double dots, etc.)', 'if', '""', '..', '""', 'in', 'hostname', 'or', 'hostname', '.', 'startswith', '(', '""', '.', '""', ')', 'or', 'hostname', '.', 'endswith', '(', '""', '.', '""', ')', ':', 'return', 'False', '# Ensure the hostname ends with amazonaws.com (proper domain validation)', 'if', 'not', 'hostname', '.', 'endswith', '(', '""', '.amazonaws.com', '""', ')', ':', 'return', 'False', '# Check for proper ECR hostname structure: account-id.dkr.ecr.region.amazonaws.com', 'ecr_pattern', '=', 'r', '""', '^', '\\', 'd', '{12}', '\\', '.dkr', '\\', '.ecr', '\\', '.[a-z0-9-]+', '\\', '.amazonaws', '\\', '.com$', '""', 'return', 'bool', '(', 're', '.', 'match', '(', 'ecr_pattern', ',', 'hostname', ')', ')', 'except', 'Exception', ':', 'return', 'False']",Determine if a given URI is a valid Amazon ECR image URL.,"['Determine', 'if', 'a', 'given', 'URI', 'is', 'a', 'valid', 'Amazon', 'ECR', 'image', 'URL', '.']"
417,"def is_unsubscribe_allowed(
    mcp: FastMCP, sns_client: Any, kwargs: Dict[str, Any]
) -> Tuple[bool, str]:
    
    subscription_arn = kwargs.get('SubscriptionArn')

    if subscription_arn is None or subscription_arn == '':
        return False, 'SubscriptionArn is not passed to the tool'

    try:
        # Get subscription attributes to find the TopicArn
        attributes = sns_client.get_subscription_attributes(SubscriptionArn=subscription_arn)
        topic_arn = attributes.get('Attributes', {}).get('TopicArn')

        return is_mutative_action_allowed(mcp, sns_client, {'TopicArn': topic_arn})

    except Exception as e:
        return False, str(e)","['def', 'is_unsubscribe_allowed', '(', 'mcp', ':', 'FastMCP', ',', 'sns_client', ':', 'Any', ',', 'kwargs', ':', 'Dict', '[', 'str', ',', 'Any', ']', ')', '-', '>', 'Tuple', '[', 'bool', ',', 'str', ']', ':', 'subscription_arn', '=', 'kwargs', '.', 'get', '(', ""'"", 'SubscriptionArn', ""'"", ')', 'if', 'subscription_arn', 'is', 'None', 'or', 'subscription_arn', '==', ""'"", ""'"", ':', 'return', 'False', ',', ""'"", 'SubscriptionArn is not passed to the tool', ""'"", 'try', ':', '# Get subscription attributes to find the TopicArn', 'attributes', '=', 'sns_client', '.', 'get_subscription_attributes', '(', 'SubscriptionArn', '=', 'subscription_arn', ')', 'topic_arn', '=', 'attributes', '.', 'get', '(', ""'"", 'Attributes', ""'"", ',', '{', '}', ')', '.', 'get', '(', ""'"", 'TopicArn', ""'"", ')', 'return', 'is_mutative_action_allowed', '(', 'mcp', ',', 'sns_client', ',', '{', ""'"", 'TopicArn', ""'"", ':', 'topic_arn', '}', ')', 'except', 'Exception', 'as', 'e', ':', 'return', 'False', ',', 'str', '(', 'e', ')']",Determine if unsubscription is permitted based on subscription attributes and conditions.,"['Determine', 'if', 'unsubscription', 'is', 'permitted', 'based', 'on', 'subscription', 'attributes', 'and', 'conditions', '.']"
419,"def get_usage(response: ChatCompletion) -> dict:
        
        return {
            ""prompt_tokens"": response.usage.prompt_tokens if response.usage is not None else 0,
            ""completion_tokens"": response.usage.completion_tokens if response.usage is not None else 0,
            ""total_tokens"": response.usage.total_tokens if response.usage is not None else 0,
            ""cost"": response.cost if hasattr(response, ""cost"") else 0.0,
            ""model"": response.model,
        }","['def', 'get_usage', '(', 'response', ':', 'ChatCompletion', ')', '-', '>', 'dict', ':', 'return', '{', '""', 'prompt_tokens', '""', ':', 'response', '.', 'usage', '.', 'prompt_tokens', 'if', 'response', '.', 'usage', 'is', 'not', 'None', 'else', '0', ',', '""', 'completion_tokens', '""', ':', 'response', '.', 'usage', '.', 'completion_tokens', 'if', 'response', '.', 'usage', 'is', 'not', 'None', 'else', '0', ',', '""', 'total_tokens', '""', ':', 'response', '.', 'usage', '.', 'total_tokens', 'if', 'response', '.', 'usage', 'is', 'not', 'None', 'else', '0', ',', '""', 'cost', '""', ':', 'response', '.', 'cost', 'if', 'hasattr', '(', 'response', ',', '""', 'cost', '""', ')', 'else', '0.0', ',', '""', 'model', '""', ':', 'response', '.', 'model', ',', '}']",Extracts token usage and cost details from a chat response object.,"['Extracts', 'token', 'usage', 'and', 'cost', 'details', 'from', 'a', 'chat', 'response', 'object', '.']"
420,"def increment_and_enforce_llm_calls_limit(
      self, run_config: Optional[RunConfig]
  ):
    
    # We first increment the counter and then check the conditions.
    self._number_of_llm_calls += 1

    if (
        run_config
        and run_config.max_llm_calls > 0
        and self._number_of_llm_calls > run_config.max_llm_calls
    ):
      # We only enforce the limit if the limit is a positive number.
      raise LlmCallsLimitExceededError(
          ""Max number of llm calls limit of""
          f"" `{run_config.max_llm_calls}` exceeded""
      )","['def', 'increment_and_enforce_llm_calls_limit', '(', 'self', ',', 'run_config', ':', 'Optional', '[', 'RunConfig', ']', ')', ':', '# We first increment the counter and then check the conditions.', 'self', '.', '_number_of_llm_calls', '+', '=', '1', 'if', '(', 'run_config', 'and', 'run_config', '.', 'max_llm_calls', '>', '0', 'and', 'self', '.', '_number_of_llm_calls', '>', 'run_config', '.', 'max_llm_calls', ')', ':', '# We only enforce the limit if the limit is a positive number.', 'raise', 'LlmCallsLimitExceededError', '(', '""', 'Max number of llm calls limit of', '""', 'f', '""', ' `', '{', 'run_config', '.', 'max_llm_calls', '}', '` exceeded', '""', ')']",Increment call count and enforce maximum limit for LLM usage,"['Increment', 'call', 'count', 'and', 'enforce', 'maximum', 'limit', 'for', 'LLM', 'usage']"
421,"def process_llm_request(self, llm_request: LlmRequest) -> None:
    
    if llm_request.model and llm_request.model.startswith(""gemini-2""):
      llm_request.config = llm_request.config or types.GenerateContentConfig()
      llm_request.config.tools = llm_request.config.tools or []
      llm_request.config.tools.append(
          types.Tool(code_execution=types.ToolCodeExecution())
      )
      return
    raise ValueError(
        ""Gemini code execution tool is not supported for model""
        f"" {llm_request.model}""
    )","['def', 'process_llm_request', '(', 'self', ',', 'llm_request', ':', 'LlmRequest', ')', '-', '>', 'None', ':', 'if', 'llm_request', '.', 'model', 'and', 'llm_request', '.', 'model', '.', 'startswith', '(', '""', 'gemini-2', '""', ')', ':', 'llm_request', '.', 'config', '=', 'llm_request', '.', 'config', 'or', 'types', '.', 'GenerateContentConfig', '(', ')', 'llm_request', '.', 'config', '.', 'tools', '=', 'llm_request', '.', 'config', '.', 'tools', 'or', '[', ']', 'llm_request', '.', 'config', '.', 'tools', '.', 'append', '(', 'types', '.', 'Tool', '(', 'code_execution', '=', 'types', '.', 'ToolCodeExecution', '(', ')', ')', ')', 'return', 'raise', 'ValueError', '(', '""', 'Gemini code execution tool is not supported for model', '""', 'f', '""', '{', 'llm_request', '.', 'model', '}', '""', ')']",Configure code execution tools for specific AI model requests or raise an error.,"['Configure', 'code', 'execution', 'tools', 'for', 'specific', 'AI', 'model', 'requests', 'or', 'raise', 'an', 'error', '.']"
422,"def _megatron_calc_global_rank(tp_rank: int = 0, dp_rank: int = 0, pp_rank: int = 0):
    

    args = get_args()
    tp_size = mpu.get_tensor_model_parallel_world_size()
    dp_size = mpu.get_data_parallel_world_size()
    pp_size = mpu.get_pipeline_model_parallel_world_size()
    assert (tp_size * dp_size * pp_size == torch.distributed.get_world_size()
           ), f""{tp_size} x {dp_size} x {pp_size} != {torch.distributed.get_world_size()}""
    if args.switch_dp_and_pp_grouping:
        # TP-PP-DP grouping
        return (dp_rank * pp_size + pp_rank) * tp_size + tp_rank
    else:
        # TP-DP-PP grouping
        return (pp_rank * dp_size + dp_rank) * tp_size + tp_rank","['def', '_megatron_calc_global_rank', '(', 'tp_rank', ':', 'int', '=', '0', ',', 'dp_rank', ':', 'int', '=', '0', ',', 'pp_rank', ':', 'int', '=', '0', ')', ':', 'args', '=', 'get_args', '(', ')', 'tp_size', '=', 'mpu', '.', 'get_tensor_model_parallel_world_size', '(', ')', 'dp_size', '=', 'mpu', '.', 'get_data_parallel_world_size', '(', ')', 'pp_size', '=', 'mpu', '.', 'get_pipeline_model_parallel_world_size', '(', ')', 'assert', '(', 'tp_size', '*', 'dp_size', '*', 'pp_size', '==', 'torch', '.', 'distributed', '.', 'get_world_size', '(', ')', ')', ',', 'f', '""', '{', 'tp_size', '}', ' x ', '{', 'dp_size', '}', ' x ', '{', 'pp_size', '}', ' != ', '{', 'torch', '.', 'distributed', '.', 'get_world_size', '(', ')', '}', '""', 'if', 'args', '.', 'switch_dp_and_pp_grouping', ':', '# TP-PP-DP grouping', 'return', '(', 'dp_rank', '*', 'pp_size', '+', 'pp_rank', ')', '*', 'tp_size', '+', 'tp_rank', 'else', ':', '# TP-DP-PP grouping', 'return', '(', 'pp_rank', '*', 'dp_size', '+', 'dp_rank', ')', '*', 'tp_size', '+', 'tp_rank']","Calculate global rank based on tensor, data, and pipeline parallelism settings.","['Calculate', 'global', 'rank', 'based', 'on', 'tensor', ',', 'data', ',', 'and', 'pipeline', 'parallelism', 'settings', '.']"
423,"def infer_transport_type_from_url(
    url: str | AnyUrl,
) -> Literal[""streamable-http"", ""sse""]:
    
    url = str(url)
    if not url.startswith(""http""):
        raise ValueError(f""Invalid URL: {url}"")

    parsed_url = urlparse(url)
    path = parsed_url.path

    if ""/sse/"" in path or path.rstrip(""/"").endswith(""/sse""):
        return ""sse""
    else:
        return ""streamable-http""","['def', 'infer_transport_type_from_url', '(', 'url', ':', 'str', '|', 'AnyUrl', ',', ')', '-', '>', 'Literal', '[', '""', 'streamable-http', '""', ',', '""', 'sse', '""', ']', ':', 'url', '=', 'str', '(', 'url', ')', 'if', 'not', 'url', '.', 'startswith', '(', '""', 'http', '""', ')', ':', 'raise', 'ValueError', '(', 'f', '""', 'Invalid URL: ', '{', 'url', '}', '""', ')', 'parsed_url', '=', 'urlparse', '(', 'url', ')', 'path', '=', 'parsed_url', '.', 'path', 'if', '""', '/sse/', '""', 'in', 'path', 'or', 'path', '.', 'rstrip', '(', '""', '/', '""', ')', '.', 'endswith', '(', '""', '/sse', '""', ')', ':', 'return', '""', 'sse', '""', 'else', ':', 'return', '""', 'streamable-http', '""']",Determine transport type from URL path for HTTP or SSE,"['Determine', 'transport', 'type', 'from', 'URL', 'path', 'for', 'HTTP', 'or', 'SSE']"
424,"def load_local_model(local_path: str):
    
    try:
        st.session_state.messages = []
        nexa_model = NexaTextInference(
            model_path=""local_model"",
            local_path=local_path,
            **DEFAULT_PARAMS
        )
        # update options after successful local model load
        update_model_options(specified_run_type, model_map)
        return nexa_model
    except Exception as e:
        st.error(f""Error loading local model: {str(e)}"")
        return None","['def', 'load_local_model', '(', 'local_path', ':', 'str', ')', ':', 'try', ':', 'st', '.', 'session_state', '.', 'messages', '=', '[', ']', 'nexa_model', '=', 'NexaTextInference', '(', 'model_path', '=', '""', 'local_model', '""', ',', 'local_path', '=', 'local_path', ',', '*', '*', 'DEFAULT_PARAMS', ')', '# update options after successful local model load', 'update_model_options', '(', 'specified_run_type', ',', 'model_map', ')', 'return', 'nexa_model', 'except', 'Exception', 'as', 'e', ':', 'st', '.', 'error', '(', 'f', '""', 'Error loading local model: ', '{', 'str', '(', 'e', ')', '}', '""', ')', 'return', 'None']",Load a local machine learning model and update configuration options,"['Load', 'a', 'local', 'machine', 'learning', 'model', 'and', 'update', 'configuration', 'options']"
426,"def log_trainable_parameters(model: torch.nn.Module, logger: Optional[Logger] = None):
    
    trainable_params = 0
    all_param = 0
    for name, param in model.named_parameters():
        all_param += param.numel()
        if param.requires_grad:
            (logger or get_logger(__name__)).info(f""training with {name}"")
            trainable_params += param.numel()

    (logger or get_logger(__name__)).info(
        ""trainable params: %s || all params: %s || trainable%%: %s"",
        f""{trainable_params:,}"",
        f""{all_param:,}"",
        f""{trainable_params / all_param:.2%}"",
    )","['def', 'log_trainable_parameters', '(', 'model', ':', 'torch', '.', 'nn', '.', 'Module', ',', 'logger', ':', 'Optional', '[', 'Logger', ']', '=', 'None', ')', ':', 'trainable_params', '=', '0', 'all_param', '=', '0', 'for', 'name', ',', 'param', 'in', 'model', '.', 'named_parameters', '(', ')', ':', 'all_param', '+', '=', 'param', '.', 'numel', '(', ')', 'if', 'param', '.', 'requires_grad', ':', '(', 'logger', 'or', 'get_logger', '(', '__name__', ')', ')', '.', 'info', '(', 'f', '""', 'training with ', '{', 'name', '}', '""', ')', 'trainable_params', '+', '=', 'param', '.', 'numel', '(', ')', '(', 'logger', 'or', 'get_logger', '(', '__name__', ')', ')', '.', 'info', '(', '""', 'trainable params: ', '%s', ' || all params: ', '%s', ' || trainable', '%%', ': ', '%s', '""', ',', 'f', '""', '{', 'trainable_params', ':', ',', '}', '""', ',', 'f', '""', '{', 'all_param', ':', ',', '}', '""', ',', 'f', '""', '{', 'trainable_params', '/', 'all_param', ':', '.2%', '}', '""', ',', ')']",Log the count and percentage of trainable parameters in a neural network model.,"['Log', 'the', 'count', 'and', 'percentage', 'of', 'trainable', 'parameters', 'in', 'a', 'neural', 'network', 'model', '.']"
429,"def _get_user_query(self, task_send_params: TaskSendParams) -> str | None:
        
        if not task_send_params.message or not task_send_params.message.parts:
            logger.warning(f""No message parts found for task {task_send_params.id}"")
            return None
        for part in task_send_params.message.parts:
            # Ensure part is treated as a dictionary if it came from JSON
            part_dict = part if isinstance(part, dict) else part.model_dump()
            if part_dict.get(""type"") == ""text"" and ""text"" in part_dict:
                 return part_dict[""text""]
        logger.warning(f""No text part found in message for task {task_send_params.id}"")
        return None","['def', '_get_user_query', '(', 'self', ',', 'task_send_params', ':', 'TaskSendParams', ')', '-', '>', 'str', '|', 'None', ':', 'if', 'not', 'task_send_params', '.', 'message', 'or', 'not', 'task_send_params', '.', 'message', '.', 'parts', ':', 'logger', '.', 'warning', '(', 'f', '""', 'No message parts found for task ', '{', 'task_send_params', '.', 'id', '}', '""', ')', 'return', 'None', 'for', 'part', 'in', 'task_send_params', '.', 'message', '.', 'parts', ':', '# Ensure part is treated as a dictionary if it came from JSON', 'part_dict', '=', 'part', 'if', 'isinstance', '(', 'part', ',', 'dict', ')', 'else', 'part', '.', 'model_dump', '(', ')', 'if', 'part_dict', '.', 'get', '(', '""', 'type', '""', ')', '==', '""', 'text', '""', 'and', '""', 'text', '""', 'in', 'part_dict', ':', 'return', 'part_dict', '[', '""', 'text', '""', ']', 'logger', '.', 'warning', '(', 'f', '""', 'No text part found in message for task ', '{', 'task_send_params', '.', 'id', '}', '""', ')', 'return', 'None']",Extracts text content from message parts in task parameters if available,"['Extracts', 'text', 'content', 'from', 'message', 'parts', 'in', 'task', 'parameters', 'if', 'available']"
430,"def calculate_cost(input_tokens: int, output_tokens: int, model_id: str) -> float:
    
    if model_id in PRICES_PER_K_TOKENS:
        input_cost_per_k, output_cost_per_k = PRICES_PER_K_TOKENS[model_id]
        input_cost = (input_tokens / 1000) * input_cost_per_k
        output_cost = (output_tokens / 1000) * output_cost_per_k
        return input_cost + output_cost
    else:
        warnings.warn(
            f'Cannot get the costs for {model_id}. The cost will be 0. In your config_list, add field {{""price"" : [prompt_price_per_1k, completion_token_price_per_1k]}} for customized pricing.',
            UserWarning,
        )
        return 0","['def', 'calculate_cost', '(', 'input_tokens', ':', 'int', ',', 'output_tokens', ':', 'int', ',', 'model_id', ':', 'str', ')', '-', '>', 'float', ':', 'if', 'model_id', 'in', 'PRICES_PER_K_TOKENS', ':', 'input_cost_per_k', ',', 'output_cost_per_k', '=', 'PRICES_PER_K_TOKENS', '[', 'model_id', ']', 'input_cost', '=', '(', 'input_tokens', '/', '1000', ')', '*', 'input_cost_per_k', 'output_cost', '=', '(', 'output_tokens', '/', '1000', ')', '*', 'output_cost_per_k', 'return', 'input_cost', '+', 'output_cost', 'else', ':', 'warnings', '.', 'warn', '(', 'f', ""'"", 'Cannot get the costs for ', '{', 'model_id', '}', '. The cost will be 0. In your config_list, add field ', '{{', '""', 'price', '""', ' : [prompt_price_per_1k, completion_token_price_per_1k]', '}}', ' for customized pricing.', ""'"", ',', 'UserWarning', ',', ')', 'return', '0']",Calculate total cost based on token usage and model pricing information.,"['Calculate', 'total', 'cost', 'based', 'on', 'token', 'usage', 'and', 'model', 'pricing', 'information', '.']"
431,"def get_step_footnote_content(step_log: ActionStep | PlanningStep, step_name: str) -> str:
    
    step_footnote = f""**{step_name}**""
    if step_log.token_usage is not None:
        step_footnote += f"" | Input tokens: {step_log.token_usage.input_tokens:,} | Output tokens: {step_log.token_usage.output_tokens:,}""
    step_footnote += f"" | Duration: {round(float(step_log.timing.duration), 2)}s"" if step_log.timing.duration else """"
    step_footnote_content = f
    return step_footnote_content","['def', 'get_step_footnote_content', '(', 'step_log', ':', 'ActionStep', '|', 'PlanningStep', ',', 'step_name', ':', 'str', ')', '-', '>', 'str', ':', 'step_footnote', '=', 'f', '""', '**', '{', 'step_name', '}', '**', '""', 'if', 'step_log', '.', 'token_usage', 'is', 'not', 'None', ':', 'step_footnote', '+', '=', 'f', '""', ' | Input tokens: ', '{', 'step_log', '.', 'token_usage', '.', 'input_tokens', ':', ',', '}', ' | Output tokens: ', '{', 'step_log', '.', 'token_usage', '.', 'output_tokens', ':', ',', '}', '""', 'step_footnote', '+', '=', 'f', '""', ' | Duration: ', '{', 'round', '(', 'float', '(', 'step_log', '.', 'timing', '.', 'duration', ')', ',', '2', ')', '}', 's', '""', 'if', 'step_log', '.', 'timing', '.', 'duration', 'else', '""', '""', 'step_footnote_content', '=', 'f', 'return', 'step_footnote_content']",Generate a formatted footnote summarizing step details including tokens and duration.,"['Generate', 'a', 'formatted', 'footnote', 'summarizing', 'step', 'details', 'including', 'tokens', 'and', 'duration', '.']"
432,"def _cancel_interruptible_tasks(self) -> None:
		
		current_task = asyncio.current_task(self.loop)
		for task in asyncio.all_tasks(self.loop):
			if task != current_task and not task.done():
				task_name = task.get_name() if hasattr(task, 'get_name') else str(task)
				# Cancel tasks that match certain patterns
				if any(pattern in task_name for pattern in self.interruptible_task_patterns):
					logger.debug(f'Cancelling task: {task_name}')
					task.cancel()
					# Add exception handler to silence ""Task exception was never retrieved"" warnings
					task.add_done_callback(lambda t: t.exception() if t.cancelled() else None)

		# Also cancel the current task if it's interruptible
		if current_task and not current_task.done():
			task_name = current_task.get_name() if hasattr(current_task, 'get_name') else str(current_task)
			if any(pattern in task_name for pattern in self.interruptible_task_patterns):
				logger.debug(f'Cancelling current task: {task_name}')
				current_task.cancel()","['def', '_cancel_interruptible_tasks', '(', 'self', ')', '-', '>', 'None', ':', 'current_task', '=', 'asyncio', '.', 'current_task', '(', 'self', '.', 'loop', ')', 'for', 'task', 'in', 'asyncio', '.', 'all_tasks', '(', 'self', '.', 'loop', ')', ':', 'if', 'task', '!=', 'current_task', 'and', 'not', 'task', '.', 'done', '(', ')', ':', 'task_name', '=', 'task', '.', 'get_name', '(', ')', 'if', 'hasattr', '(', 'task', ',', ""'"", 'get_name', ""'"", ')', 'else', 'str', '(', 'task', ')', '# Cancel tasks that match certain patterns', 'if', 'any', '(', 'pattern', 'in', 'task_name', 'for', 'pattern', 'in', 'self', '.', 'interruptible_task_patterns', ')', ':', 'logger', '.', 'debug', '(', 'f', ""'"", 'Cancelling task: ', '{', 'task_name', '}', ""'"", ')', 'task', '.', 'cancel', '(', ')', '# Add exception handler to silence ""Task exception was never retrieved"" warnings', 'task', '.', 'add_done_callback', '(', 'lambda', 't', ':', 't', '.', 'exception', '(', ')', 'if', 't', '.', 'cancelled', '(', ')', 'else', 'None', ')', ""# Also cancel the current task if it's interruptible"", 'if', 'current_task', 'and', 'not', 'current_task', '.', 'done', '(', ')', ':', 'task_name', '=', 'current_task', '.', 'get_name', '(', ')', 'if', 'hasattr', '(', 'current_task', ',', ""'"", 'get_name', ""'"", ')', 'else', 'str', '(', 'current_task', ')', 'if', 'any', '(', 'pattern', 'in', 'task_name', 'for', 'pattern', 'in', 'self', '.', 'interruptible_task_patterns', ')', ':', 'logger', '.', 'debug', '(', 'f', ""'"", 'Cancelling current task: ', '{', 'task_name', '}', ""'"", ')', 'current_task', '.', 'cancel', '(', ')']",Cancel interruptible asynchronous tasks based on predefined name patterns.,"['Cancel', 'interruptible', 'asynchronous', 'tasks', 'based', 'on', 'predefined', 'name', 'patterns', '.']"
433,"def compare_hashes_gcs(blob, local_file_path: str) -> bool:
    
    if os.path.exists(local_file_path):
        remote_md5_base64 = blob.md5_hash
        hash_md5 = hashlib.md5()
        with open(local_file_path, ""rb"") as f:
            for chunk in iter(lambda: f.read(8192), b""""):
                hash_md5.update(chunk)
        local_md5 = hash_md5.digest()
        remote_md5 = base64.b64decode(remote_md5_base64)
        if remote_md5 == local_md5:
            logger.info(f""File '{local_file_path}' already up-to-date. Skipping download."")
            return False
        else:
            logger.info(f""File '{local_file_path}' differs from GCS. Downloading."")
            return True
    else:
        logger.info(f""File '{local_file_path}' does not exist locally. Downloading."")
        return True","['def', 'compare_hashes_gcs', '(', 'blob', ',', 'local_file_path', ':', 'str', ')', '-', '>', 'bool', ':', 'if', 'os', '.', 'path', '.', 'exists', '(', 'local_file_path', ')', ':', 'remote_md5_base64', '=', 'blob', '.', 'md5_hash', 'hash_md5', '=', 'hashlib', '.', 'md5', '(', ')', 'with', 'open', '(', 'local_file_path', ',', '""', 'rb', '""', ')', 'as', 'f', ':', 'for', 'chunk', 'in', 'iter', '(', 'lambda', ':', 'f', '.', 'read', '(', '8192', ')', ',', 'b', '""', '""', ')', ':', 'hash_md5', '.', 'update', '(', 'chunk', ')', 'local_md5', '=', 'hash_md5', '.', 'digest', '(', ')', 'remote_md5', '=', 'base64', '.', 'b64decode', '(', 'remote_md5_base64', ')', 'if', 'remote_md5', '==', 'local_md5', ':', 'logger', '.', 'info', '(', 'f', '""', 'File ', ""'"", '{', 'local_file_path', '}', ""'"", ' already up-to-date. Skipping download.', '""', ')', 'return', 'False', 'else', ':', 'logger', '.', 'info', '(', 'f', '""', 'File ', ""'"", '{', 'local_file_path', '}', ""'"", ' differs from GCS. Downloading.', '""', ')', 'return', 'True', 'else', ':', 'logger', '.', 'info', '(', 'f', '""', 'File ', ""'"", '{', 'local_file_path', '}', ""'"", ' does not exist locally. Downloading.', '""', ')', 'return', 'True']",Compares local and remote file hashes to determine if download is necessary.,"['Compares', 'local', 'and', 'remote', 'file', 'hashes', 'to', 'determine', 'if', 'download', 'is', 'necessary', '.']"
435,"def _megatron_calc_global_rank(tp_rank: int = 0, dp_rank: int = 0, pp_rank: int = 0, cp_rank: int = 0, ep_rank: int = 0):
    

    # Get parallel sizes for each dimension
    tp_size = mpu.get_tensor_model_parallel_world_size()
    dp_size = mpu.get_data_parallel_world_size()
    pp_size = mpu.get_pipeline_model_parallel_world_size()
    cp_size = mpu.get_context_parallel_world_size()
    # ep_size = mpu.get_expert_model_parallel_world_size()

    # Verify total GPU count matches (must be consistent with parallel_state.py)
    total_size = tp_size * dp_size * pp_size * cp_size
    assert total_size == torch.distributed.get_world_size(), f""{tp_size}x{dp_size}x{pp_size}x{cp_size} != {torch.distributed.get_world_size()}""

    # Core calculation logic (corresponds to RankGenerator order parameter)
    # Assumes default order is ""tp-cp-ep-dp-pp""
    return ((pp_rank * dp_size + dp_rank) * cp_size + cp_rank) * tp_size + tp_rank","['def', '_megatron_calc_global_rank', '(', 'tp_rank', ':', 'int', '=', '0', ',', 'dp_rank', ':', 'int', '=', '0', ',', 'pp_rank', ':', 'int', '=', '0', ',', 'cp_rank', ':', 'int', '=', '0', ',', 'ep_rank', ':', 'int', '=', '0', ')', ':', '# Get parallel sizes for each dimension', 'tp_size', '=', 'mpu', '.', 'get_tensor_model_parallel_world_size', '(', ')', 'dp_size', '=', 'mpu', '.', 'get_data_parallel_world_size', '(', ')', 'pp_size', '=', 'mpu', '.', 'get_pipeline_model_parallel_world_size', '(', ')', 'cp_size', '=', 'mpu', '.', 'get_context_parallel_world_size', '(', ')', '# ep_size = mpu.get_expert_model_parallel_world_size()', '# Verify total GPU count matches (must be consistent with parallel_state.py)', 'total_size', '=', 'tp_size', '*', 'dp_size', '*', 'pp_size', '*', 'cp_size', 'assert', 'total_size', '==', 'torch', '.', 'distributed', '.', 'get_world_size', '(', ')', ',', 'f', '""', '{', 'tp_size', '}', 'x', '{', 'dp_size', '}', 'x', '{', 'pp_size', '}', 'x', '{', 'cp_size', '}', ' != ', '{', 'torch', '.', 'distributed', '.', 'get_world_size', '(', ')', '}', '""', '# Core calculation logic (corresponds to RankGenerator order parameter)', '# Assumes default order is ""tp-cp-ep-dp-pp""', 'return', '(', '(', 'pp_rank', '*', 'dp_size', '+', 'dp_rank', ')', '*', 'cp_size', '+', 'cp_rank', ')', '*', 'tp_size', '+', 'tp_rank']",Calculate global rank in a distributed system using parallel dimensions.,"['Calculate', 'global', 'rank', 'in', 'a', 'distributed', 'system', 'using', 'parallel', 'dimensions', '.']"
436,"def load_model_tokenizer(cls,model_name):
        
        model_dir = os.path.join(shared.args.model_dir,model_name)

        tokenizer = AutoTokenizer.from_pretrained(model_dir,
                                                  local_files_only=True)
        
        model = AutoModelForSequenceClassification.from_pretrained(
            model_dir,
            trust_remote_code=True,
            local_files_only=True
        )
        
        print(f""Model '{model_name}' loaded successfully "")
        return tokenizer, model","['def', 'load_model_tokenizer', '(', 'cls', ',', 'model_name', ')', ':', 'model_dir', '=', 'os', '.', 'path', '.', 'join', '(', 'shared', '.', 'args', '.', 'model_dir', ',', 'model_name', ')', 'tokenizer', '=', 'AutoTokenizer', '.', 'from_pretrained', '(', 'model_dir', ',', 'local_files_only', '=', 'True', ')', 'model', '=', 'AutoModelForSequenceClassification', '.', 'from_pretrained', '(', 'model_dir', ',', 'trust_remote_code', '=', 'True', ',', 'local_files_only', '=', 'True', ')', 'print', '(', 'f', '""', 'Model ', ""'"", '{', 'model_name', '}', ""'"", ' loaded successfully ', '""', ')', 'return', 'tokenizer', ',', 'model']",Load tokenizer and model for sequence classification from local directory,"['Load', 'tokenizer', 'and', 'model', 'for', 'sequence', 'classification', 'from', 'local', 'directory']"
438,"def __str__(self) -> str:
        
        info = """"
        info += ""Audio Extraction Task:\n""

        if self._auth_token:
            info += ""  auth_token: [redacted]\n""
        if self._grpc_endpoint:
            info += f""  grpc_endpoint: {self._grpc_endpoint}\n""
        if self._infer_protocol:
            info += f""  infer_protocol: {self._infer_protocol}\n""
        if self._function_id:
            info += ""  function_id: [redacted]\n""
        if self._use_ssl:
            info += f""  use_ssl: {self._use_ssl}\n""
        if self._ssl_cert:
            info += ""  ssl_cert: [redacted]\n""

        return info","['def', '__str__', '(', 'self', ')', '-', '>', 'str', ':', 'info', '=', '""', '""', 'info', '+', '=', '""', 'Audio Extraction Task:', '\\n', '""', 'if', 'self', '.', '_auth_token', ':', 'info', '+', '=', '""', '  auth_token: [redacted]', '\\n', '""', 'if', 'self', '.', '_grpc_endpoint', ':', 'info', '+', '=', 'f', '""', '  grpc_endpoint: ', '{', 'self', '.', '_grpc_endpoint', '}', '\\n', '""', 'if', 'self', '.', '_infer_protocol', ':', 'info', '+', '=', 'f', '""', '  infer_protocol: ', '{', 'self', '.', '_infer_protocol', '}', '\\n', '""', 'if', 'self', '.', '_function_id', ':', 'info', '+', '=', '""', '  function_id: [redacted]', '\\n', '""', 'if', 'self', '.', '_use_ssl', ':', 'info', '+', '=', 'f', '""', '  use_ssl: ', '{', 'self', '.', '_use_ssl', '}', '\\n', '""', 'if', 'self', '.', '_ssl_cert', ':', 'info', '+', '=', '""', '  ssl_cert: [redacted]', '\\n', '""', 'return', 'info']",Generate a string summary of audio extraction task configuration,"['Generate', 'a', 'string', 'summary', 'of', 'audio', 'extraction', 'task', 'configuration']"
440,"def trim_voice_tensor(tensor):
    
    if tensor.shape[0] != 511:
        raise ValueError(f""Expected tensor with first dimension 511, got {tensor.shape[0]}"")
    
    # Analyze variance contribution of each row
    variance = analyze_voice_content(tensor)
    
    # Determine which end has lower variance (less information)
    start_var = variance[:5].mean().item()
    end_var = variance[-5:].mean().item()
    
    # Remove from the end with lower variance
    if end_var < start_var:
        logger.info(""Trimming last row (lower variance at end)"")
        return tensor[:-1]
    else:
        logger.info(""Trimming first row (lower variance at start)"")
        return tensor[1:]","['def', 'trim_voice_tensor', '(', 'tensor', ')', ':', 'if', 'tensor', '.', 'shape', '[', '0', ']', '!=', '511', ':', 'raise', 'ValueError', '(', 'f', '""', 'Expected tensor with first dimension 511, got ', '{', 'tensor', '.', 'shape', '[', '0', ']', '}', '""', ')', '# Analyze variance contribution of each row', 'variance', '=', 'analyze_voice_content', '(', 'tensor', ')', '# Determine which end has lower variance (less information)', 'start_var', '=', 'variance', '[', ':', '5', ']', '.', 'mean', '(', ')', '.', 'item', '(', ')', 'end_var', '=', 'variance', '[', '-', '5', ':', ']', '.', 'mean', '(', ')', '.', 'item', '(', ')', '# Remove from the end with lower variance', 'if', 'end_var', '<', 'start_var', ':', 'logger', '.', 'info', '(', '""', 'Trimming last row (lower variance at end)', '""', ')', 'return', 'tensor', '[', ':', '-', '1', ']', 'else', ':', 'logger', '.', 'info', '(', '""', 'Trimming first row (lower variance at start)', '""', ')', 'return', 'tensor', '[', '1', ':', ']']",Trim a voice data tensor by removing the row with the least variance.,"['Trim', 'a', 'voice', 'data', 'tensor', 'by', 'removing', 'the', 'row', 'with', 'the', 'least', 'variance', '.']"
441,"def clear_all(cls, cache_dir: Path | None = None) -> None:
        
        cache_dir = cache_dir or default_cache_dir()
        if not cache_dir.exists():
            return

        file_types: list[Literal[""client_info"", ""tokens""]] = [""client_info"", ""tokens""]
        for file_type in file_types:
            for file in cache_dir.glob(f""*_{file_type}.json""):
                file.unlink(missing_ok=True)
        logger.info(""Cleared all OAuth client cache data."")","['def', 'clear_all', '(', 'cls', ',', 'cache_dir', ':', 'Path', '|', 'None', '=', 'None', ')', '-', '>', 'None', ':', 'cache_dir', '=', 'cache_dir', 'or', 'default_cache_dir', '(', ')', 'if', 'not', 'cache_dir', '.', 'exists', '(', ')', ':', 'return', 'file_types', ':', 'list', '[', 'Literal', '[', '""', 'client_info', '""', ',', '""', 'tokens', '""', ']', ']', '=', '[', '""', 'client_info', '""', ',', '""', 'tokens', '""', ']', 'for', 'file_type', 'in', 'file_types', ':', 'for', 'file', 'in', 'cache_dir', '.', 'glob', '(', 'f', '""', '*_', '{', 'file_type', '}', '.json', '""', ')', ':', 'file', '.', 'unlink', '(', 'missing_ok', '=', 'True', ')', 'logger', '.', 'info', '(', '""', 'Cleared all OAuth client cache data.', '""', ')']",Clear specified cache directory of OAuth client data files.,"['Clear', 'specified', 'cache', 'directory', 'of', 'OAuth', 'client', 'data', 'files', '.']"
444,"def convert_to_regular_types(obj):
    
    from omegaconf import ListConfig, DictConfig
    if isinstance(obj, (ListConfig, DictConfig)):
        return {k: convert_to_regular_types(v) for k, v in obj.items()} if isinstance(obj, DictConfig) else list(obj)
    elif isinstance(obj, (list, tuple)):
        return [convert_to_regular_types(x) for x in obj]
    elif isinstance(obj, dict):
        return {k: convert_to_regular_types(v) for k, v in obj.items()}
    return obj","['def', 'convert_to_regular_types', '(', 'obj', ')', ':', 'from', 'omegaconf', 'import', 'ListConfig', ',', 'DictConfig', 'if', 'isinstance', '(', 'obj', ',', '(', 'ListConfig', ',', 'DictConfig', ')', ')', ':', 'return', '{', 'k', ':', 'convert_to_regular_types', '(', 'v', ')', 'for', 'k', ',', 'v', 'in', 'obj', '.', 'items', '(', ')', '}', 'if', 'isinstance', '(', 'obj', ',', 'DictConfig', ')', 'else', 'list', '(', 'obj', ')', 'elif', 'isinstance', '(', 'obj', ',', '(', 'list', ',', 'tuple', ')', ')', ':', 'return', '[', 'convert_to_regular_types', '(', 'x', ')', 'for', 'x', 'in', 'obj', ']', 'elif', 'isinstance', '(', 'obj', ',', 'dict', ')', ':', 'return', '{', 'k', ':', 'convert_to_regular_types', '(', 'v', ')', 'for', 'k', ',', 'v', 'in', 'obj', '.', 'items', '(', ')', '}', 'return', 'obj']",Converts nested OmegaConf objects to standard Python data types recursively.,"['Converts', 'nested', 'OmegaConf', 'objects', 'to', 'standard', 'Python', 'data', 'types', 'recursively', '.']"
445,"def save_account_info(self, email, password, token, total_usage):
        
        try:
            with open(self.accounts_file, 'a', encoding='utf-8') as f:
                f.write(f""\n{'='*50}\n"")
                f.write(f""Email: {email}\n"")
                f.write(f""Password: {password}\n"")
                f.write(f""Token: {token}\n"")
                f.write(f""Usage Limit: {total_usage}\n"")
                f.write(f""{'='*50}\n"")
                
            print(f""{Fore.GREEN}{EMOJI['SUCCESS']} {self.translator.get('register.account_info_saved') if self.translator else 'Account information saved'}...{Style.RESET_ALL}"")
            return True
            
        except Exception as e:
            error_msg = self.translator.get('register.save_account_info_failed', error=str(e)) if self.translator else f'Failed to save account information: {str(e)}'
            print(f""{Fore.RED}{EMOJI['ERROR']} {error_msg}{Style.RESET_ALL}"")
            return False","['def', 'save_account_info', '(', 'self', ',', 'email', ',', 'password', ',', 'token', ',', 'total_usage', ')', ':', 'try', ':', 'with', 'open', '(', 'self', '.', 'accounts_file', ',', ""'"", 'a', ""'"", ',', 'encoding', '=', ""'"", 'utf-8', ""'"", ')', 'as', 'f', ':', 'f', '.', 'write', '(', 'f', '""', '\\n', '{', ""'"", '=', ""'"", '*', '50', '}', '\\n', '""', ')', 'f', '.', 'write', '(', 'f', '""', 'Email: ', '{', 'email', '}', '\\n', '""', ')', 'f', '.', 'write', '(', 'f', '""', 'Password: ', '{', 'password', '}', '\\n', '""', ')', 'f', '.', 'write', '(', 'f', '""', 'Token: ', '{', 'token', '}', '\\n', '""', ')', 'f', '.', 'write', '(', 'f', '""', 'Usage Limit: ', '{', 'total_usage', '}', '\\n', '""', ')', 'f', '.', 'write', '(', 'f', '""', '{', ""'"", '=', ""'"", '*', '50', '}', '\\n', '""', ')', 'print', '(', 'f', '""', '{', 'Fore', '.', 'GREEN', '}', '{', 'EMOJI', '[', ""'"", 'SUCCESS', ""'"", ']', '}', '{', 'self', '.', 'translator', '.', 'get', '(', ""'"", 'register.account_info_saved', ""'"", ')', 'if', 'self', '.', 'translator', 'else', ""'"", 'Account information saved', ""'"", '}', '...', '{', 'Style', '.', 'RESET_ALL', '}', '""', ')', 'return', 'True', 'except', 'Exception', 'as', 'e', ':', 'error_msg', '=', 'self', '.', 'translator', '.', 'get', '(', ""'"", 'register.save_account_info_failed', ""'"", ',', 'error', '=', 'str', '(', 'e', ')', ')', 'if', 'self', '.', 'translator', 'else', 'f', ""'"", 'Failed to save account information: ', '{', 'str', '(', 'e', ')', '}', ""'"", 'print', '(', 'f', '""', '{', 'Fore', '.', 'RED', '}', '{', 'EMOJI', '[', ""'"", 'ERROR', ""'"", ']', '}', '{', 'error_msg', '}', '{', 'Style', '.', 'RESET_ALL', '}', '""', ')', 'return', 'False']",Function logs account details to a file and handles success or error messages.,"['Function', 'logs', 'account', 'details', 'to', 'a', 'file', 'and', 'handles', 'success', 'or', 'error', 'messages', '.']"
446,"def load_pipelines(self) -> Dict[str, Type[pipeline]]:
        
        animate_thinking(""Loading zero-shot pipeline..."", color=""status"")
        return {
            ""bart"": pipeline(""zero-shot-classification"", model=""facebook/bart-large-mnli"")
        }","['def', 'load_pipelines', '(', 'self', ')', '-', '>', 'Dict', '[', 'str', ',', 'Type', '[', 'pipeline', ']', ']', ':', 'animate_thinking', '(', '""', 'Loading zero-shot pipeline...', '""', ',', 'color', '=', '""', 'status', '""', ')', 'return', '{', '""', 'bart', '""', ':', 'pipeline', '(', '""', 'zero-shot-classification', '""', ',', 'model', '=', '""', 'facebook/bart-large-mnli', '""', ')', '}']",Load a zero-shot classification pipeline using a BART model.,"['Load', 'a', 'zero-shot', 'classification', 'pipeline', 'using', 'a', 'BART', 'model', '.']"
447,"def load_environment():
    
    try:
        # Read and execute set_keys.sh
        with open('set_keys.sh', 'r') as f:
            lines = f.readlines()
        
        for line in lines:
            line = line.strip()
            if line.startswith('export ') and '=' in line:
                # Parse export statements
                line = line.replace('export ', '')
                key, value = line.split('=', 1)
                # Remove quotes if present
                value = value.strip('""').strip(""'"")
                os.environ[key] = value
                print(f""Loaded: {key}"")
        
        print(""Environment variables loaded successfully"")
    except Exception as e:
        print(f""Error loading environment: {e}"")","['def', 'load_environment', '(', ')', ':', 'try', ':', '# Read and execute set_keys.sh', 'with', 'open', '(', ""'"", 'set_keys.sh', ""'"", ',', ""'"", 'r', ""'"", ')', 'as', 'f', ':', 'lines', '=', 'f', '.', 'readlines', '(', ')', 'for', 'line', 'in', 'lines', ':', 'line', '=', 'line', '.', 'strip', '(', ')', 'if', 'line', '.', 'startswith', '(', ""'"", 'export ', ""'"", ')', 'and', ""'"", '=', ""'"", 'in', 'line', ':', '# Parse export statements', 'line', '=', 'line', '.', 'replace', '(', ""'"", 'export ', ""'"", ',', ""'"", ""'"", ')', 'key', ',', 'value', '=', 'line', '.', 'split', '(', ""'"", '=', ""'"", ',', '1', ')', '# Remove quotes if present', 'value', '=', 'value', '.', 'strip', '(', ""'"", '""', ""'"", ')', '.', 'strip', '(', '""', ""'"", '""', ')', 'os', '.', 'environ', '[', 'key', ']', '=', 'value', 'print', '(', 'f', '""', 'Loaded: ', '{', 'key', '}', '""', ')', 'print', '(', '""', 'Environment variables loaded successfully', '""', ')', 'except', 'Exception', 'as', 'e', ':', 'print', '(', 'f', '""', 'Error loading environment: ', '{', 'e', '}', '""', ')']",Load environment variables from a script file into the system environment.,"['Load', 'environment', 'variables', 'from', 'a', 'script', 'file', 'into', 'the', 'system', 'environment', '.']"
448,"def _run(self, **kwargs) -> str:
        
        servers = self.server_manager.client.get_server_names()
        if not servers:
            return ""No MCP servers are currently defined.""

        result = ""Available MCP servers:\n""
        for i, server_name in enumerate(servers):
            active_marker = "" (ACTIVE)"" if server_name == self.server_manager.active_server else """"
            result += f""{i + 1}. {server_name}{active_marker}\n""

            tools: list = []
            try:
                # Check cache first
                if server_name in self.server_manager._server_tools:
                    tools = self.server_manager._server_tools[server_name]
                    tool_count = len(tools)
                    result += f""   {tool_count} tools available for this server\n""
            except Exception as e:
                logger.error(f""Unexpected error listing tools for server '{server_name}': {e}"")

        return result","['def', '_run', '(', 'self', ',', '*', '*', 'kwargs', ')', '-', '>', 'str', ':', 'servers', '=', 'self', '.', 'server_manager', '.', 'client', '.', 'get_server_names', '(', ')', 'if', 'not', 'servers', ':', 'return', '""', 'No MCP servers are currently defined.', '""', 'result', '=', '""', 'Available MCP servers:', '\\n', '""', 'for', 'i', ',', 'server_name', 'in', 'enumerate', '(', 'servers', ')', ':', 'active_marker', '=', '""', ' (ACTIVE)', '""', 'if', 'server_name', '==', 'self', '.', 'server_manager', '.', 'active_server', 'else', '""', '""', 'result', '+', '=', 'f', '""', '{', 'i', '+', '1', '}', '. ', '{', 'server_name', '}', '{', 'active_marker', '}', '\\n', '""', 'tools', ':', 'list', '=', '[', ']', 'try', ':', '# Check cache first', 'if', 'server_name', 'in', 'self', '.', 'server_manager', '.', '_server_tools', ':', 'tools', '=', 'self', '.', 'server_manager', '.', '_server_tools', '[', 'server_name', ']', 'tool_count', '=', 'len', '(', 'tools', ')', 'result', '+', '=', 'f', '""', '{', 'tool_count', '}', ' tools available for this server', '\\n', '""', 'except', 'Exception', 'as', 'e', ':', 'logger', '.', 'error', '(', 'f', '""', 'Unexpected error listing tools for server ', ""'"", '{', 'server_name', '}', ""'"", ': ', '{', 'e', '}', '""', ')', 'return', 'result']","List available servers and their tools, highlighting the active server.","['List', 'available', 'servers', 'and', 'their', 'tools', ',', 'highlighting', 'the', 'active', 'server', '.']"
450,"def update_presigned_url(presigned_url, base_url):
    
    #To Do: If Proxy URL has domain name how do we handle such cases? Engineering Dependency.
    
    presigned_parts = urlparse(presigned_url)
    base_parts = urlparse(base_url)
    # Check if base_url contains localhost or an IP address
    if re.match(r'^(localhost|\d{1,3}(\.\d{1,3}){3})$', base_parts.hostname):
        new_netloc = base_parts.hostname  # Extract domain from base_url
        if base_parts.port:  # Add port if present in base_url
            new_netloc += f"":{base_parts.port}""
        updated_parts = presigned_parts._replace(netloc=new_netloc)
        return urlunparse(updated_parts)
    return presigned_url","['def', 'update_presigned_url', '(', 'presigned_url', ',', 'base_url', ')', ':', '#To Do: If Proxy URL has domain name how do we handle such cases? Engineering Dependency.', 'presigned_parts', '=', 'urlparse', '(', 'presigned_url', ')', 'base_parts', '=', 'urlparse', '(', 'base_url', ')', '# Check if base_url contains localhost or an IP address', 'if', 're', '.', 'match', '(', 'r', ""'"", '^(localhost|', '\\', 'd', '{', '1,3}(', '\\', '.', '\\', 'd', '{', '1,3})', '{3}', ')$', ""'"", ',', 'base_parts', '.', 'hostname', ')', ':', 'new_netloc', '=', 'base_parts', '.', 'hostname', '# Extract domain from base_url', 'if', 'base_parts', '.', 'port', ':', '# Add port if present in base_url', 'new_netloc', '+', '=', 'f', '""', ':', '{', 'base_parts', '.', 'port', '}', '""', 'updated_parts', '=', 'presigned_parts', '.', '_replace', '(', 'netloc', '=', 'new_netloc', ')', 'return', 'urlunparse', '(', 'updated_parts', ')', 'return', 'presigned_url']",Update presigned URL's domain if base URL is localhost or IP address.,"['Update', 'presigned', 'URL', ""'s"", 'domain', 'if', 'base', 'URL', 'is', 'localhost', 'or', 'IP', 'address', '.']"
451,"def _get_current_mem_info(unit: str = ""GB"", precision: int = 2) -> Tuple[str]:
    
    assert unit in [""GB"", ""MB"", ""KB""]
    divisor = 1024**3 if unit == ""GB"" else 1024**2 if unit == ""MB"" else 1024
    mem_allocated = get_torch_device().memory_allocated()
    mem_reserved = get_torch_device().memory_reserved()
    # use get_torch_device().mem_get_info to profile device memory
    # since vllm's sleep mode works below pytorch
    mem_free, mem_total = get_torch_device().mem_get_info()
    mem_used = mem_total - mem_free
    mem_allocated = f""{mem_allocated / divisor:.{precision}f}""
    mem_reserved = f""{mem_reserved / divisor:.{precision}f}""
    mem_used = f""{mem_used / divisor:.{precision}f}""
    mem_total = f""{mem_total / divisor:.{precision}f}""
    return mem_allocated, mem_reserved, mem_used, mem_total","['def', '_get_current_mem_info', '(', 'unit', ':', 'str', '=', '""', 'GB', '""', ',', 'precision', ':', 'int', '=', '2', ')', '-', '>', 'Tuple', '[', 'str', ']', ':', 'assert', 'unit', 'in', '[', '""', 'GB', '""', ',', '""', 'MB', '""', ',', '""', 'KB', '""', ']', 'divisor', '=', '1024', '*', '*', '3', 'if', 'unit', '==', '""', 'GB', '""', 'else', '1024', '*', '*', '2', 'if', 'unit', '==', '""', 'MB', '""', 'else', '1024', 'mem_allocated', '=', 'get_torch_device', '(', ')', '.', 'memory_allocated', '(', ')', 'mem_reserved', '=', 'get_torch_device', '(', ')', '.', 'memory_reserved', '(', ')', '# use get_torch_device().mem_get_info to profile device memory', ""# since vllm's sleep mode works below pytorch"", 'mem_free', ',', 'mem_total', '=', 'get_torch_device', '(', ')', '.', 'mem_get_info', '(', ')', 'mem_used', '=', 'mem_total', '-', 'mem_free', 'mem_allocated', '=', 'f', '""', '{', 'mem_allocated', '/', 'divisor', ':', '.', '{', 'precision', '}', 'f', '}', '""', 'mem_reserved', '=', 'f', '""', '{', 'mem_reserved', '/', 'divisor', ':', '.', '{', 'precision', '}', 'f', '}', '""', 'mem_used', '=', 'f', '""', '{', 'mem_used', '/', 'divisor', ':', '.', '{', 'precision', '}', 'f', '}', '""', 'mem_total', '=', 'f', '""', '{', 'mem_total', '/', 'divisor', ':', '.', '{', 'precision', '}', 'f', '}', '""', 'return', 'mem_allocated', ',', 'mem_reserved', ',', 'mem_used', ',', 'mem_total']",Retrieve and format current device memory statistics in specified units.,"['Retrieve', 'and', 'format', 'current', 'device', 'memory', 'statistics', 'in', 'specified', 'units', '.']"
453,"def get_aws_session(region_name=None):
    
    profile_name = os.environ.get('AWS_PROFILE')
    region = region_name or get_region()

    if profile_name:
        logger.debug(f'Using AWS profile: {profile_name}')
        return boto3.Session(profile_name=profile_name, region_name=region)
    else:
        logger.debug('Using default AWS credential chain')
        return boto3.Session(region_name=region)","['def', 'get_aws_session', '(', 'region_name', '=', 'None', ')', ':', 'profile_name', '=', 'os', '.', 'environ', '.', 'get', '(', ""'"", 'AWS_PROFILE', ""'"", ')', 'region', '=', 'region_name', 'or', 'get_region', '(', ')', 'if', 'profile_name', ':', 'logger', '.', 'debug', '(', 'f', ""'"", 'Using AWS profile: ', '{', 'profile_name', '}', ""'"", ')', 'return', 'boto3', '.', 'Session', '(', 'profile_name', '=', 'profile_name', ',', 'region_name', '=', 'region', ')', 'else', ':', 'logger', '.', 'debug', '(', ""'"", 'Using default AWS credential chain', ""'"", ')', 'return', 'boto3', '.', 'Session', '(', 'region_name', '=', 'region', ')']",Establish an AWS session using environment profile or default credentials.,"['Establish', 'an', 'AWS', 'session', 'using', 'environment', 'profile', 'or', 'default', 'credentials', '.']"
454,"def handle_phue_error(
    light_or_group: str, operation: str, error: Exception
) -> dict[str, Any]:
    
    base_info = {""target"": light_or_group, ""operation"": operation, ""success"": False}
    if isinstance(error, KeyError):
        base_info[""error""] = f""Target '{light_or_group}' not found""
    elif isinstance(error, PhueException):
        base_info[""error""] = f""phue2 error during {operation}: {error}""
    else:
        base_info[""error""] = f""Unexpected error during {operation}: {error}""
    return base_info","['def', 'handle_phue_error', '(', 'light_or_group', ':', 'str', ',', 'operation', ':', 'str', ',', 'error', ':', 'Exception', ')', '-', '>', 'dict', '[', 'str', ',', 'Any', ']', ':', 'base_info', '=', '{', '""', 'target', '""', ':', 'light_or_group', ',', '""', 'operation', '""', ':', 'operation', ',', '""', 'success', '""', ':', 'False', '}', 'if', 'isinstance', '(', 'error', ',', 'KeyError', ')', ':', 'base_info', '[', '""', 'error', '""', ']', '=', 'f', '""', 'Target ', ""'"", '{', 'light_or_group', '}', ""'"", ' not found', '""', 'elif', 'isinstance', '(', 'error', ',', 'PhueException', ')', ':', 'base_info', '[', '""', 'error', '""', ']', '=', 'f', '""', 'phue2 error during ', '{', 'operation', '}', ': ', '{', 'error', '}', '""', 'else', ':', 'base_info', '[', '""', 'error', '""', ']', '=', 'f', '""', 'Unexpected error during ', '{', 'operation', '}', ': ', '{', 'error', '}', '""', 'return', 'base_info']","Handle errors in smart lighting operations, returning structured error details.","['Handle', 'errors', 'in', 'smart', 'lighting', 'operations', ',', 'returning', 'structured', 'error', 'details', '.']"
456,"def to_dict(self) -> Dict:
        
        task_properties = {}

        if self._api_key:
            task_properties[""api_key""] = self._api_key

        if self._endpoint_url:
            task_properties[""endpoint_url""] = self._endpoint_url

        if self._prompt:
            task_properties[""prompt""] = self._prompt

        if self._model_name:
            task_properties[""model_name""] = self._model_name

        return {""type"": ""caption"", ""task_properties"": task_properties}","['def', 'to_dict', '(', 'self', ')', '-', '>', 'Dict', ':', 'task_properties', '=', '{', '}', 'if', 'self', '.', '_api_key', ':', 'task_properties', '[', '""', 'api_key', '""', ']', '=', 'self', '.', '_api_key', 'if', 'self', '.', '_endpoint_url', ':', 'task_properties', '[', '""', 'endpoint_url', '""', ']', '=', 'self', '.', '_endpoint_url', 'if', 'self', '.', '_prompt', ':', 'task_properties', '[', '""', 'prompt', '""', ']', '=', 'self', '.', '_prompt', 'if', 'self', '.', '_model_name', ':', 'task_properties', '[', '""', 'model_name', '""', ']', '=', 'self', '.', '_model_name', 'return', '{', '""', 'type', '""', ':', '""', 'caption', '""', ',', '""', 'task_properties', '""', ':', 'task_properties', '}']",Convert object attributes into a dictionary for a caption task configuration.,"['Convert', 'object', 'attributes', 'into', 'a', 'dictionary', 'for', 'a', 'caption', 'task', 'configuration', '.']"
457,"def _exists(file_path: str):
    
    if file_path.startswith(""hdfs""):
        return _run_cmd(_hdfs_cmd(f""-test -e {file_path}"")) == 0
    return os.path.exists(file_path)","['def', '_exists', '(', 'file_path', ':', 'str', ')', ':', 'if', 'file_path', '.', 'startswith', '(', '""', 'hdfs', '""', ')', ':', 'return', '_run_cmd', '(', '_hdfs_cmd', '(', 'f', '""', '-test -e ', '{', 'file_path', '}', '""', ')', ')', '==', '0', 'return', 'os', '.', 'path', '.', 'exists', '(', 'file_path', ')']",Check if a file exists locally or on HDFS based on its path.,"['Check', 'if', 'a', 'file', 'exists', 'locally', 'or', 'on', 'HDFS', 'based', 'on', 'its', 'path', '.']"
461,"def language_token(self) -> int:
        
        if self.language is None:
            raise ValueError(f""This tokenizer does not have language token configured"")

        additional_tokens = dict(
            zip(
                self.tokenizer.additional_special_tokens,
                self.tokenizer.additional_special_tokens_ids,
            )
        )
        candidate = f""<|{self.language}|>""
        if candidate in additional_tokens:
            return additional_tokens[candidate]

        raise KeyError(f""Language {self.language} not found in tokenizer."")","['def', 'language_token', '(', 'self', ')', '-', '>', 'int', ':', 'if', 'self', '.', 'language', 'is', 'None', ':', 'raise', 'ValueError', '(', 'f', '""', 'This tokenizer does not have language token configured', '""', ')', 'additional_tokens', '=', 'dict', '(', 'zip', '(', 'self', '.', 'tokenizer', '.', 'additional_special_tokens', ',', 'self', '.', 'tokenizer', '.', 'additional_special_tokens_ids', ',', ')', ')', 'candidate', '=', 'f', '""', '<|', '{', 'self', '.', 'language', '}', '|>', '""', 'if', 'candidate', 'in', 'additional_tokens', ':', 'return', 'additional_tokens', '[', 'candidate', ']', 'raise', 'KeyError', '(', 'f', '""', 'Language ', '{', 'self', '.', 'language', '}', ' not found in tokenizer.', '""', ')']",Determine language token ID from tokenizer configuration or raise error if absent.,"['Determine', 'language', 'token', 'ID', 'from', 'tokenizer', 'configuration', 'or', 'raise', 'error', 'if', 'absent', '.']"
462,"def _format_search_results_as_context(self, search_results):
        
        context_snippets = []

        for i, result in enumerate(
            search_results[:10]
        ):  # Limit to prevent context overflow
            title = result.get(""title"", ""Untitled"")
            snippet = result.get(""snippet"", """")
            url = result.get(""link"", """")

            if snippet:
                context_snippets.append(
                    f""Source {i + 1}: {title}\nURL: {url}\nSnippet: {snippet}""
                )

        return ""\n\n"".join(context_snippets)","['def', '_format_search_results_as_context', '(', 'self', ',', 'search_results', ')', ':', 'context_snippets', '=', '[', ']', 'for', 'i', ',', 'result', 'in', 'enumerate', '(', 'search_results', '[', ':', '10', ']', ')', ':', '# Limit to prevent context overflow', 'title', '=', 'result', '.', 'get', '(', '""', 'title', '""', ',', '""', 'Untitled', '""', ')', 'snippet', '=', 'result', '.', 'get', '(', '""', 'snippet', '""', ',', '""', '""', ')', 'url', '=', 'result', '.', 'get', '(', '""', 'link', '""', ',', '""', '""', ')', 'if', 'snippet', ':', 'context_snippets', '.', 'append', '(', 'f', '""', 'Source ', '{', 'i', '+', '1', '}', ': ', '{', 'title', '}', '\\n', 'URL: ', '{', 'url', '}', '\\n', 'Snippet: ', '{', 'snippet', '}', '""', ')', 'return', '""', '\\n', '\\n', '""', '.', 'join', '(', 'context_snippets', ')']","Format top search results into a structured context summary with titles, URLs, and snippets.","['Format', 'top', 'search', 'results', 'into', 'a', 'structured', 'context', 'summary', 'with', 'titles', ',', 'URLs', ',', 'and', 'snippets', '.']"
464,"def create_html_redline(text1: str, text2: str) -> str:
    
    d = dmp.diff_match_patch()
    diffs = d.diff_main(text2, text1)
    d.diff_cleanupSemantic(diffs)

    html_output = """"
    for op, text in diffs:
        if op == -1:  # Deletion
            html_output += (
                f'<del style=""background-color: #ffcccc;"">{text}</del>'
            )
        elif op == 1:  # Insertion
            html_output += (
                f'<ins style=""background-color: #ccffcc;"">{text}</ins>'
            )
        else:  # Unchanged
            html_output += text

    return html_output","['def', 'create_html_redline', '(', 'text1', ':', 'str', ',', 'text2', ':', 'str', ')', '-', '>', 'str', ':', 'd', '=', 'dmp', '.', 'diff_match_patch', '(', ')', 'diffs', '=', 'd', '.', 'diff_main', '(', 'text2', ',', 'text1', ')', 'd', '.', 'diff_cleanupSemantic', '(', 'diffs', ')', 'html_output', '=', '""', '""', 'for', 'op', ',', 'text', 'in', 'diffs', ':', 'if', 'op', '==', '-', '1', ':', '# Deletion', 'html_output', '+', '=', '(', 'f', ""'"", '<del style=', '""', 'background-color: #ffcccc;', '""', '>', '{', 'text', '}', '</del>', ""'"", ')', 'elif', 'op', '==', '1', ':', '# Insertion', 'html_output', '+', '=', '(', 'f', ""'"", '<ins style=', '""', 'background-color: #ccffcc;', '""', '>', '{', 'text', '}', '</ins>', ""'"", ')', 'else', ':', '# Unchanged', 'html_output', '+', '=', 'text', 'return', 'html_output']",Generate HTML redline markup to highlight text differences.,"['Generate', 'HTML', 'redline', 'markup', 'to', 'highlight', 'text', 'differences', '.']"
465,"def __init__(self, config, actor_module: nn.Module, actor_optimizer: torch.optim.Optimizer = None):
        
        super().__init__(config)
        self.actor_module = actor_module
        self.actor_optimizer = actor_optimizer

        self.use_remove_padding = self.config.get(""use_remove_padding"", False)
        if torch.distributed.get_rank() == 0:
            print(f""Actor use_remove_padding={self.use_remove_padding}"")
        self.use_fused_kernels = self.config.get(""use_fused_kernels"", False)
        if torch.distributed.get_rank() == 0:
            print(f""Actor use_fused_kernels={self.use_fused_kernels}"")

        self.ulysses_sequence_parallel_size = self.config.ulysses_sequence_parallel_size
        self.use_ulysses_sp = self.ulysses_sequence_parallel_size > 1

        self.compute_entropy_from_logits = (
            torch.compile(verl_F.entropy_from_logits, dynamic=True)
            if self.config.get(""use_torch_compile"", True)  #  use torch compile by default
            else verl_F.entropy_from_logits
        )
        self.device_name = get_device_name()","['def', '__init__', '(', 'self', ',', 'config', ',', 'actor_module', ':', 'nn', '.', 'Module', ',', 'actor_optimizer', ':', 'torch', '.', 'optim', '.', 'Optimizer', '=', 'None', ')', ':', 'super', '(', ')', '.', '__init__', '(', 'config', ')', 'self', '.', 'actor_module', '=', 'actor_module', 'self', '.', 'actor_optimizer', '=', 'actor_optimizer', 'self', '.', 'use_remove_padding', '=', 'self', '.', 'config', '.', 'get', '(', '""', 'use_remove_padding', '""', ',', 'False', ')', 'if', 'torch', '.', 'distributed', '.', 'get_rank', '(', ')', '==', '0', ':', 'print', '(', 'f', '""', 'Actor use_remove_padding=', '{', 'self', '.', 'use_remove_padding', '}', '""', ')', 'self', '.', 'use_fused_kernels', '=', 'self', '.', 'config', '.', 'get', '(', '""', 'use_fused_kernels', '""', ',', 'False', ')', 'if', 'torch', '.', 'distributed', '.', 'get_rank', '(', ')', '==', '0', ':', 'print', '(', 'f', '""', 'Actor use_fused_kernels=', '{', 'self', '.', 'use_fused_kernels', '}', '""', ')', 'self', '.', 'ulysses_sequence_parallel_size', '=', 'self', '.', 'config', '.', 'ulysses_sequence_parallel_size', 'self', '.', 'use_ulysses_sp', '=', 'self', '.', 'ulysses_sequence_parallel_size', '>', '1', 'self', '.', 'compute_entropy_from_logits', '=', '(', 'torch', '.', 'compile', '(', 'verl_F', '.', 'entropy_from_logits', ',', 'dynamic', '=', 'True', ')', 'if', 'self', '.', 'config', '.', 'get', '(', '""', 'use_torch_compile', '""', ',', 'True', ')', '#  use torch compile by default', 'else', 'verl_F', '.', 'entropy_from_logits', ')', 'self', '.', 'device_name', '=', 'get_device_name', '(', ')']","Initialize actor module with configuration, optimizer, and optional performance settings.","['Initialize', 'actor', 'module', 'with', 'configuration', ',', 'optimizer', ',', 'and', 'optional', 'performance', 'settings', '.']"
467,"def greet_user(
    name: str = Field(description=""The name of the person to greet""),
    title: str = Field(description=""Optional title like Mr/Ms/Dr"", default=""""),
    times: int = Field(description=""Number of times to repeat the greeting"", default=1),
) -> str:
    
    greeting = f""Hello {title + ' ' if title else ''}{name}!""
    return ""\n"".join([greeting] * times)","['def', 'greet_user', '(', 'name', ':', 'str', '=', 'Field', '(', 'description', '=', '""', 'The name of the person to greet', '""', ')', ',', 'title', ':', 'str', '=', 'Field', '(', 'description', '=', '""', 'Optional title like Mr/Ms/Dr', '""', ',', 'default', '=', '""', '""', ')', ',', 'times', ':', 'int', '=', 'Field', '(', 'description', '=', '""', 'Number of times to repeat the greeting', '""', ',', 'default', '=', '1', ')', ',', ')', '-', '>', 'str', ':', 'greeting', '=', 'f', '""', 'Hello ', '{', 'title', '+', ""'"", ""'"", 'if', 'title', 'else', ""'"", ""'"", '}', '{', 'name', '}', '!', '""', 'return', '""', '\\n', '""', '.', 'join', '(', '[', 'greeting', ']', '*', 'times', ')']",Generate a personalized greeting message repeated multiple times.,"['Generate', 'a', 'personalized', 'greeting', 'message', 'repeated', 'multiple', 'times', '.']"
468,"def collate_fn(data_list: list[dict]) -> dict:
    
    tensors = defaultdict(list)
    non_tensors = defaultdict(list)

    for data in data_list:
        for key, val in data.items():
            if isinstance(val, torch.Tensor):
                tensors[key].append(val)
            else:
                non_tensors[key].append(val)

    for key, val in tensors.items():
        tensors[key] = torch.stack(val, dim=0)

    for key, val in non_tensors.items():
        non_tensors[key] = np.array(val, dtype=object)

    return {**tensors, **non_tensors}","['def', 'collate_fn', '(', 'data_list', ':', 'list', '[', 'dict', ']', ')', '-', '>', 'dict', ':', 'tensors', '=', 'defaultdict', '(', 'list', ')', 'non_tensors', '=', 'defaultdict', '(', 'list', ')', 'for', 'data', 'in', 'data_list', ':', 'for', 'key', ',', 'val', 'in', 'data', '.', 'items', '(', ')', ':', 'if', 'isinstance', '(', 'val', ',', 'torch', '.', 'Tensor', ')', ':', 'tensors', '[', 'key', ']', '.', 'append', '(', 'val', ')', 'else', ':', 'non_tensors', '[', 'key', ']', '.', 'append', '(', 'val', ')', 'for', 'key', ',', 'val', 'in', 'tensors', '.', 'items', '(', ')', ':', 'tensors', '[', 'key', ']', '=', 'torch', '.', 'stack', '(', 'val', ',', 'dim', '=', '0', ')', 'for', 'key', ',', 'val', 'in', 'non_tensors', '.', 'items', '(', ')', ':', 'non_tensors', '[', 'key', ']', '=', 'np', '.', 'array', '(', 'val', ',', 'dtype', '=', 'object', ')', 'return', '{', '*', '*', 'tensors', ',', '*', '*', 'non_tensors', '}']",Aggregate and organize tensor and non-tensor data from a list of dictionaries.,"['Aggregate', 'and', 'organize', 'tensor', 'and', 'non-tensor', 'data', 'from', 'a', 'list', 'of', 'dictionaries', '.']"
470,"def format_category(
        self, category: RouterCategory, index: int | None = None
    ) -> str:
        

        index_str = f""{index}. "" if index is not None else "" ""
        category_str = """"

        if isinstance(category, ServerRouterCategory):
            category_str = self._format_server_category(category)
        elif isinstance(category, AgentRouterCategory):
            category_str = self._format_agent_category(category)
        else:
            category_str = self._format_function_category(category)

        return f""{index_str}{category_str}""","['def', 'format_category', '(', 'self', ',', 'category', ':', 'RouterCategory', ',', 'index', ':', 'int', '|', 'None', '=', 'None', ')', '-', '>', 'str', ':', 'index_str', '=', 'f', '""', '{', 'index', '}', '. ', '""', 'if', 'index', 'is', 'not', 'None', 'else', '""', '""', 'category_str', '=', '""', '""', 'if', 'isinstance', '(', 'category', ',', 'ServerRouterCategory', ')', ':', 'category_str', '=', 'self', '.', '_format_server_category', '(', 'category', ')', 'elif', 'isinstance', '(', 'category', ',', 'AgentRouterCategory', ')', ':', 'category_str', '=', 'self', '.', '_format_agent_category', '(', 'category', ')', 'else', ':', 'category_str', '=', 'self', '.', '_format_function_category', '(', 'category', ')', 'return', 'f', '""', '{', 'index_str', '}', '{', 'category_str', '}', '""']",Formats a category object into a string with optional index prefix.,"['Formats', 'a', 'category', 'object', 'into', 'a', 'string', 'with', 'optional', 'index', 'prefix', '.']"
474,"def add_prompt(self, prompt: Prompt, key: str | None = None) -> Prompt:
        
        key = key or prompt.name

        # Check for duplicates
        existing = self._prompts.get(key)
        if existing:
            if self.duplicate_behavior == ""warn"":
                logger.warning(f""Prompt already exists: {key}"")
                self._prompts[key] = prompt
            elif self.duplicate_behavior == ""replace"":
                self._prompts[key] = prompt
            elif self.duplicate_behavior == ""error"":
                raise ValueError(f""Prompt already exists: {key}"")
            elif self.duplicate_behavior == ""ignore"":
                return existing
        else:
            self._prompts[key] = prompt
        return prompt","['def', 'add_prompt', '(', 'self', ',', 'prompt', ':', 'Prompt', ',', 'key', ':', 'str', '|', 'None', '=', 'None', ')', '-', '>', 'Prompt', ':', 'key', '=', 'key', 'or', 'prompt', '.', 'name', '# Check for duplicates', 'existing', '=', 'self', '.', '_prompts', '.', 'get', '(', 'key', ')', 'if', 'existing', ':', 'if', 'self', '.', 'duplicate_behavior', '==', '""', 'warn', '""', ':', 'logger', '.', 'warning', '(', 'f', '""', 'Prompt already exists: ', '{', 'key', '}', '""', ')', 'self', '.', '_prompts', '[', 'key', ']', '=', 'prompt', 'elif', 'self', '.', 'duplicate_behavior', '==', '""', 'replace', '""', ':', 'self', '.', '_prompts', '[', 'key', ']', '=', 'prompt', 'elif', 'self', '.', 'duplicate_behavior', '==', '""', 'error', '""', ':', 'raise', 'ValueError', '(', 'f', '""', 'Prompt already exists: ', '{', 'key', '}', '""', ')', 'elif', 'self', '.', 'duplicate_behavior', '==', '""', 'ignore', '""', ':', 'return', 'existing', 'else', ':', 'self', '.', '_prompts', '[', 'key', ']', '=', 'prompt', 'return', 'prompt']","Add a prompt to a collection, handling duplicates based on specified behavior.","['Add', 'a', 'prompt', 'to', 'a', 'collection', ',', 'handling', 'duplicates', 'based', 'on', 'specified', 'behavior', '.']"
475,"def database_setup_and_cleanup(db_session: Session) -> Generator[None, None, None]:
    

    inspector = cast(Inspector, inspect(db_session.bind))

    # Check if all tables defined in models are created in the db
    for table in Base.metadata.tables.values():
        if not inspector.has_table(table.name):
            pytest.exit(f""Table {table} does not exist in the database."")

    clear_database(db_session)
    yield  # This allows the test to run
    clear_database(db_session)","['def', 'database_setup_and_cleanup', '(', 'db_session', ':', 'Session', ')', '-', '>', 'Generator', '[', 'None', ',', 'None', ',', 'None', ']', ':', 'inspector', '=', 'cast', '(', 'Inspector', ',', 'inspect', '(', 'db_session', '.', 'bind', ')', ')', '# Check if all tables defined in models are created in the db', 'for', 'table', 'in', 'Base', '.', 'metadata', '.', 'tables', '.', 'values', '(', ')', ':', 'if', 'not', 'inspector', '.', 'has_table', '(', 'table', '.', 'name', ')', ':', 'pytest', '.', 'exit', '(', 'f', '""', 'Table ', '{', 'table', '}', ' does not exist in the database.', '""', ')', 'clear_database', '(', 'db_session', ')', 'yield', '# This allows the test to run', 'clear_database', '(', 'db_session', ')']","Ensure database tables exist, clear data before and after tests","['Ensure', 'database', 'tables', 'exist', ',', 'clear', 'data', 'before', 'and', 'after', 'tests']"
478,"def decode_unicode_escapes_to_utf8(text: str) -> str:
	

	if r'\u' not in text:
		# doesn't have any escape sequences that need to be decoded
		return text

	try:
		# Try to decode Unicode escape sequences
		return text.encode('latin1').decode('unicode_escape')
	except (UnicodeEncodeError, UnicodeDecodeError):
		# logger.debug(f""Failed to decode unicode escape sequences while generating gif text: {text}"")
		return text","['def', 'decode_unicode_escapes_to_utf8', '(', 'text', ':', 'str', ')', '-', '>', 'str', ':', 'if', 'r', ""'"", '\\', 'u', ""'"", 'not', 'in', 'text', ':', ""# doesn't have any escape sequences that need to be decoded"", 'return', 'text', 'try', ':', '# Try to decode Unicode escape sequences', 'return', 'text', '.', 'encode', '(', ""'"", 'latin1', ""'"", ')', '.', 'decode', '(', ""'"", 'unicode_escape', ""'"", ')', 'except', '(', 'UnicodeEncodeError', ',', 'UnicodeDecodeError', ')', ':', '# logger.debug(f""Failed to decode unicode escape sequences while generating gif text: {text}"")', 'return', 'text']","Convert Unicode escape sequences in a string to UTF-8 format, handling errors gracefully.","['Convert', 'Unicode', 'escape', 'sequences', 'in', 'a', 'string', 'to', 'UTF-8', 'format', ',', 'handling', 'errors', 'gracefully', '.']"
479,"def check_prime(nums: list[int]) -> str:
  
  primes = set()
  for number in nums:
    number = int(number)
    if number <= 1:
      continue
    is_prime = True
    for i in range(2, int(number**0.5) + 1):
      if number % i == 0:
        is_prime = False
        break
    if is_prime:
      primes.add(number)
  return (
      ""No prime numbers found.""
      if not primes
      else f""{', '.join(str(num) for num in primes)} are prime numbers.""
  )","['def', 'check_prime', '(', 'nums', ':', 'list', '[', 'int', ']', ')', '-', '>', 'str', ':', 'primes', '=', 'set', '(', ')', 'for', 'number', 'in', 'nums', ':', 'number', '=', 'int', '(', 'number', ')', 'if', 'number', '<', '=', '1', ':', 'continue', 'is_prime', '=', 'True', 'for', 'i', 'in', 'range', '(', '2', ',', 'int', '(', 'number', '*', '*', '0.5', ')', '+', '1', ')', ':', 'if', 'number', '%', 'i', '==', '0', ':', 'is_prime', '=', 'False', 'break', 'if', 'is_prime', ':', 'primes', '.', 'add', '(', 'number', ')', 'return', '(', '""', 'No prime numbers found.', '""', 'if', 'not', 'primes', 'else', 'f', '""', '{', ""'"", ', ', ""'"", '.', 'join', '(', 'str', '(', 'num', ')', 'for', 'num', 'in', 'primes', ')', '}', ' are prime numbers.', '""', ')']",Determine and return prime numbers from a list of integers.,"['Determine', 'and', 'return', 'prime', 'numbers', 'from', 'a', 'list', 'of', 'integers', '.']"
480,"def worker_task(thread_id, args):
    
    for i in range(args.iterations):
        iteration = i + 1
        test_id = f""{thread_id:02d}_{iteration:02d}""
        text = generate_test_sentence(thread_id, iteration)
        success = request_tts(
            args.url, test_id, text, args.voice, args.output_dir, args.debug
        )

        if not success:
            log_message(
                f""Thread {thread_id}: Iteration {iteration} failed"",
                args.debug,
                is_error=True,
            )

        # Small delay between iterations to avoid overwhelming the API
        time.sleep(0.1)","['def', 'worker_task', '(', 'thread_id', ',', 'args', ')', ':', 'for', 'i', 'in', 'range', '(', 'args', '.', 'iterations', ')', ':', 'iteration', '=', 'i', '+', '1', 'test_id', '=', 'f', '""', '{', 'thread_id', ':', '02d', '}', '_', '{', 'iteration', ':', '02d', '}', '""', 'text', '=', 'generate_test_sentence', '(', 'thread_id', ',', 'iteration', ')', 'success', '=', 'request_tts', '(', 'args', '.', 'url', ',', 'test_id', ',', 'text', ',', 'args', '.', 'voice', ',', 'args', '.', 'output_dir', ',', 'args', '.', 'debug', ')', 'if', 'not', 'success', ':', 'log_message', '(', 'f', '""', 'Thread ', '{', 'thread_id', '}', ': Iteration ', '{', 'iteration', '}', ' failed', '""', ',', 'args', '.', 'debug', ',', 'is_error', '=', 'True', ',', ')', '# Small delay between iterations to avoid overwhelming the API', 'time', '.', 'sleep', '(', '0.1', ')']","Execute text-to-speech requests iteratively, logging failures for each thread.","['Execute', 'text-to-speech', 'requests', 'iteratively', ',', 'logging', 'failures', 'for', 'each', 'thread', '.']"
481,"def _annotate_span_for_generation_message(
        self,
        span: trace.Span,
        message: MessageParamT | str | List[MessageParamT],
    ) -> None:
        
        if not self.context.tracing_enabled:
            return
        if isinstance(message, str):
            span.set_attribute(""message.content"", message)
        elif isinstance(message, list):
            for i, msg in enumerate(message):
                if isinstance(msg, str):
                    span.set_attribute(f""message.{i}.content"", msg)
                else:
                    span.set_attribute(f""message.{i}"", str(msg))
        else:
            span.set_attribute(""message"", str(message))","['def', '_annotate_span_for_generation_message', '(', 'self', ',', 'span', ':', 'trace', '.', 'Span', ',', 'message', ':', 'MessageParamT', '|', 'str', '|', 'List', '[', 'MessageParamT', ']', ',', ')', '-', '>', 'None', ':', 'if', 'not', 'self', '.', 'context', '.', 'tracing_enabled', ':', 'return', 'if', 'isinstance', '(', 'message', ',', 'str', ')', ':', 'span', '.', 'set_attribute', '(', '""', 'message.content', '""', ',', 'message', ')', 'elif', 'isinstance', '(', 'message', ',', 'list', ')', ':', 'for', 'i', ',', 'msg', 'in', 'enumerate', '(', 'message', ')', ':', 'if', 'isinstance', '(', 'msg', ',', 'str', ')', ':', 'span', '.', 'set_attribute', '(', 'f', '""', 'message.', '{', 'i', '}', '.content', '""', ',', 'msg', ')', 'else', ':', 'span', '.', 'set_attribute', '(', 'f', '""', 'message.', '{', 'i', '}', '""', ',', 'str', '(', 'msg', ')', ')', 'else', ':', 'span', '.', 'set_attribute', '(', '""', 'message', '""', ',', 'str', '(', 'message', ')', ')']",Annotate trace span with message content if tracing is enabled.,"['Annotate', 'trace', 'span', 'with', 'message', 'content', 'if', 'tracing', 'is', 'enabled', '.']"
482,"def load_local_model(local_path: str):
    
    try:
        nexa_model = NexaImageInference(
            model_path=""local_model"",
            local_path=local_path
        )
        # update options after successful local model load
        update_model_options(specified_run_type, model_map)
        return nexa_model
    except Exception as e:
        st.error(f""Error loading local model: {str(e)}"")
        return None","['def', 'load_local_model', '(', 'local_path', ':', 'str', ')', ':', 'try', ':', 'nexa_model', '=', 'NexaImageInference', '(', 'model_path', '=', '""', 'local_model', '""', ',', 'local_path', '=', 'local_path', ')', '# update options after successful local model load', 'update_model_options', '(', 'specified_run_type', ',', 'model_map', ')', 'return', 'nexa_model', 'except', 'Exception', 'as', 'e', ':', 'st', '.', 'error', '(', 'f', '""', 'Error loading local model: ', '{', 'str', '(', 'e', ')', '}', '""', ')', 'return', 'None']",Load a local image inference model and update configuration options,"['Load', 'a', 'local', 'image', 'inference', 'model', 'and', 'update', 'configuration', 'options']"
484,"def __str__(self) -> str:
        
        base = f""{self.action.ljust(11)}. {self.target}""
        if self.details:
            base += f"" - {self.details}""
        if self.agent_name:
            base = f""[{self.agent_name}] {base}""
        return base","['def', '__str__', '(', 'self', ')', '-', '>', 'str', ':', 'base', '=', 'f', '""', '{', 'self', '.', 'action', '.', 'ljust', '(', '11', ')', '}', '. ', '{', 'self', '.', 'target', '}', '""', 'if', 'self', '.', 'details', ':', 'base', '+', '=', 'f', '""', ' - ', '{', 'self', '.', 'details', '}', '""', 'if', 'self', '.', 'agent_name', ':', 'base', '=', 'f', '""', '[', '{', 'self', '.', 'agent_name', '}', '] ', '{', 'base', '}', '""', 'return', 'base']",Formats and returns a descriptive string of an action with optional details and agent name.,"['Formats', 'and', 'returns', 'a', 'descriptive', 'string', 'of', 'an', 'action', 'with', 'optional', 'details', 'and', 'agent', 'name', '.']"
486,"def normalize_model_name(name, pp_rank, vpp_rank, transformer_config, layer_name=""layers""):
    
    from verl.utils.megatron_utils import get_transformer_layer_offset

    layer_offset = get_transformer_layer_offset(pp_rank, vpp_rank, transformer_config)

    if layer_name in name:  # belong to an intermediate layer
        split_name = name.split(""."")
        # find the num next to split_name
        for i, name in enumerate(split_name):
            if name == layer_name:
                break
        layer_num_idx = i + 1
        # check the name
        assert len(split_name) >= layer_num_idx + 1, f""split_name = {split_name}""
        assert split_name[layer_num_idx].isdigit(), f""split_name = {split_name}""
        # increment layer_num_idx by layer_offset
        split_name[layer_num_idx] = str(int(split_name[layer_num_idx]) + layer_offset)
        name = ""."".join(split_name)  # weight name in inference_tp_model
    return name","['def', 'normalize_model_name', '(', 'name', ',', 'pp_rank', ',', 'vpp_rank', ',', 'transformer_config', ',', 'layer_name', '=', '""', 'layers', '""', ')', ':', 'from', 'verl', '.', 'utils', '.', 'megatron_utils', 'import', 'get_transformer_layer_offset', 'layer_offset', '=', 'get_transformer_layer_offset', '(', 'pp_rank', ',', 'vpp_rank', ',', 'transformer_config', ')', 'if', 'layer_name', 'in', 'name', ':', '# belong to an intermediate layer', 'split_name', '=', 'name', '.', 'split', '(', '""', '.', '""', ')', '# find the num next to split_name', 'for', 'i', ',', 'name', 'in', 'enumerate', '(', 'split_name', ')', ':', 'if', 'name', '==', 'layer_name', ':', 'break', 'layer_num_idx', '=', 'i', '+', '1', '# check the name', 'assert', 'len', '(', 'split_name', ')', '>', '=', 'layer_num_idx', '+', '1', ',', 'f', '""', 'split_name = ', '{', 'split_name', '}', '""', 'assert', 'split_name', '[', 'layer_num_idx', ']', '.', 'isdigit', '(', ')', ',', 'f', '""', 'split_name = ', '{', 'split_name', '}', '""', '# increment layer_num_idx by layer_offset', 'split_name', '[', 'layer_num_idx', ']', '=', 'str', '(', 'int', '(', 'split_name', '[', 'layer_num_idx', ']', ')', '+', 'layer_offset', ')', 'name', '=', '""', '.', '""', '.', 'join', '(', 'split_name', ')', '# weight name in inference_tp_model', 'return', 'name']",Adjusts model layer identifiers based on rank offsets for distributed processing.,"['Adjusts', 'model', 'layer', 'identifiers', 'based', 'on', 'rank', 'offsets', 'for', 'distributed', 'processing', '.']"
487,"def interpreter_feedback(self, output:str) -> str:
        
        if self.execution_failure_check(output):
            feedback = f""[failure] Error in execution:\n{output}""
        else:
            feedback = ""[success] Execution success, code output:\n"" + output
        return feedback","['def', 'interpreter_feedback', '(', 'self', ',', 'output', ':', 'str', ')', '-', '>', 'str', ':', 'if', 'self', '.', 'execution_failure_check', '(', 'output', ')', ':', 'feedback', '=', 'f', '""', '[failure] Error in execution:', '\\n', '{', 'output', '}', '""', 'else', ':', 'feedback', '=', '""', '[success] Execution success, code output:', '\\n', '""', '+', 'output', 'return', 'feedback']",Determine execution success or failure and generate feedback message,"['Determine', 'execution', 'success', 'or', 'failure', 'and', 'generate', 'feedback', 'message']"
488,"def __repr__(self) -> str:
        
        output_str = f""sub question: {self.sub_question}""
        if self.chunks:
            output_str += f""\nretrieved chunks:\n{self.chunks}""
        if self.spo:
            output_str += f""\nretrieved spo:\n{self.spo}""
        if self.summary:
            output_str += f""\nsummary:\n{self.summary}""
        return output_str","['def', '__repr__', '(', 'self', ')', '-', '>', 'str', ':', 'output_str', '=', 'f', '""', 'sub question: ', '{', 'self', '.', 'sub_question', '}', '""', 'if', 'self', '.', 'chunks', ':', 'output_str', '+', '=', 'f', '""', '\\n', 'retrieved chunks:', '\\n', '{', 'self', '.', 'chunks', '}', '""', 'if', 'self', '.', 'spo', ':', 'output_str', '+', '=', 'f', '""', '\\n', 'retrieved spo:', '\\n', '{', 'self', '.', 'spo', '}', '""', 'if', 'self', '.', 'summary', ':', 'output_str', '+', '=', 'f', '""', '\\n', 'summary:', '\\n', '{', 'self', '.', 'summary', '}', '""', 'return', 'output_str']","Generate a string representation of an object detailing its sub-question, chunks, spo, and summary.","['Generate', 'a', 'string', 'representation', 'of', 'an', 'object', 'detailing', 'its', 'sub-question', ',', 'chunks', ',', 'spo', ',', 'and', 'summary', '.']"
489,"def setup_data_dir():
    
    # Get the project root directory (3 levels up from this file)
    current_dir = os.path.dirname(os.path.abspath(__file__))
    project_root = os.path.abspath(os.path.join(current_dir, "".."", ""..""))

    # Define the data directory path
    data_dir = os.path.join(project_root, ""data"")

    # Create the data directory if it doesn't exist
    if not os.path.exists(data_dir):
        os.makedirs(data_dir)
        print(f""Created data directory at: {data_dir}"")
    else:
        print(f""Data directory already exists at: {data_dir}"")

    # Return the path to the data directory
    return data_dir","['def', 'setup_data_dir', '(', ')', ':', '# Get the project root directory (3 levels up from this file)', 'current_dir', '=', 'os', '.', 'path', '.', 'dirname', '(', 'os', '.', 'path', '.', 'abspath', '(', '__file__', ')', ')', 'project_root', '=', 'os', '.', 'path', '.', 'abspath', '(', 'os', '.', 'path', '.', 'join', '(', 'current_dir', ',', '""', '..', '""', ',', '""', '..', '""', ')', ')', '# Define the data directory path', 'data_dir', '=', 'os', '.', 'path', '.', 'join', '(', 'project_root', ',', '""', 'data', '""', ')', ""# Create the data directory if it doesn't exist"", 'if', 'not', 'os', '.', 'path', '.', 'exists', '(', 'data_dir', ')', ':', 'os', '.', 'makedirs', '(', 'data_dir', ')', 'print', '(', 'f', '""', 'Created data directory at: ', '{', 'data_dir', '}', '""', ')', 'else', ':', 'print', '(', 'f', '""', 'Data directory already exists at: ', '{', 'data_dir', '}', '""', ')', '# Return the path to the data directory', 'return', 'data_dir']",Ensure data directory exists within project structure and return its path.,"['Ensure', 'data', 'directory', 'exists', 'within', 'project', 'structure', 'and', 'return', 'its', 'path', '.']"
491,"def create_empty_state(
    agent: BaseAgent, initialized_states: Optional[dict[str, Any]] = None
) -> dict[str, Any]:
  
  non_initialized_states = {}
  _create_empty_state(agent, non_initialized_states)
  for key in initialized_states or {}:
    if key in non_initialized_states:
      del non_initialized_states[key]
  return non_initialized_states","['def', 'create_empty_state', '(', 'agent', ':', 'BaseAgent', ',', 'initialized_states', ':', 'Optional', '[', 'dict', '[', 'str', ',', 'Any', ']', ']', '=', 'None', ')', '-', '>', 'dict', '[', 'str', ',', 'Any', ']', ':', 'non_initialized_states', '=', '{', '}', '_create_empty_state', '(', 'agent', ',', 'non_initialized_states', ')', 'for', 'key', 'in', 'initialized_states', 'or', '{', '}', ':', 'if', 'key', 'in', 'non_initialized_states', ':', 'del', 'non_initialized_states', '[', 'key', ']', 'return', 'non_initialized_states']",Generate a state dictionary excluding initialized agent states,"['Generate', 'a', 'state', 'dictionary', 'excluding', 'initialized', 'agent', 'states']"
492,"def _display_quality_summary(self, metrics: Dict[str, Any]):
        
        overall_score = metrics.get(""overall_score"", 0)
        issues = metrics.get(""issues"", [])

        # Use formatter for consistent display
        quality_display = self.formatter.format_quality_score(overall_score, issues)
        self.console.print(f""[dim]{quality_display}[/dim]"")

        # Highlight specific test concerns
        if metrics.get(""premature_attempt""):
            self.console.print(
                ""  [yellow]âš  Test detected premature answer attempt[/yellow]""
            )

        verbosity = metrics.get(""verbosity"", 0)
        if verbosity > 0.7:
            self.console.print(
                f""  [yellow]âš  High verbosity detected ({verbosity:.0%})[/yellow]""
            )","['def', '_display_quality_summary', '(', 'self', ',', 'metrics', ':', 'Dict', '[', 'str', ',', 'Any', ']', ')', ':', 'overall_score', '=', 'metrics', '.', 'get', '(', '""', 'overall_score', '""', ',', '0', ')', 'issues', '=', 'metrics', '.', 'get', '(', '""', 'issues', '""', ',', '[', ']', ')', '# Use formatter for consistent display', 'quality_display', '=', 'self', '.', 'formatter', '.', 'format_quality_score', '(', 'overall_score', ',', 'issues', ')', 'self', '.', 'console', '.', 'print', '(', 'f', '""', '[dim]', '{', 'quality_display', '}', '[/dim]', '""', ')', '# Highlight specific test concerns', 'if', 'metrics', '.', 'get', '(', '""', 'premature_attempt', '""', ')', ':', 'self', '.', 'console', '.', 'print', '(', '""', '  [yellow]âš  Test detected premature answer attempt[/yellow]', '""', ')', 'verbosity', '=', 'metrics', '.', 'get', '(', '""', 'verbosity', '""', ',', '0', ')', 'if', 'verbosity', '>', '0.7', ':', 'self', '.', 'console', '.', 'print', '(', 'f', '""', '  [yellow]âš  High verbosity detected (', '{', 'verbosity', ':', '.0%', '}', ')[/yellow]', '""', ')']",Display a quality summary using metrics with formatted output and warnings.,"['Display', 'a', 'quality', 'summary', 'using', 'metrics', 'with', 'formatted', 'output', 'and', 'warnings', '.']"
496,"def get_markdown(research_id):
    
    conn = get_db_connection()
    conn.row_factory = lambda cursor, row: {
        column[0]: row[idx] for idx, column in enumerate(cursor.description)
    }
    cursor = conn.cursor()
    cursor.execute(""SELECT * FROM research_history WHERE id = ?"", (research_id,))
    result = cursor.fetchone()
    conn.close()

    if not result or not result.get(""report_path""):
        return jsonify({""status"": ""error"", ""message"": ""Report not found""}), 404

    try:
        with open(result[""report_path""], ""r"", encoding=""utf-8"") as f:
            content = f.read()
        return jsonify({""status"": ""success"", ""content"": content})
    except Exception as e:
        return jsonify({""status"": ""error"", ""message"": str(e)}), 500","['def', 'get_markdown', '(', 'research_id', ')', ':', 'conn', '=', 'get_db_connection', '(', ')', 'conn', '.', 'row_factory', '=', 'lambda', 'cursor', ',', 'row', ':', '{', 'column', '[', '0', ']', ':', 'row', '[', 'idx', ']', 'for', 'idx', ',', 'column', 'in', 'enumerate', '(', 'cursor', '.', 'description', ')', '}', 'cursor', '=', 'conn', '.', 'cursor', '(', ')', 'cursor', '.', 'execute', '(', '""', 'SELECT * FROM research_history WHERE id = ?', '""', ',', '(', 'research_id', ',', ')', ')', 'result', '=', 'cursor', '.', 'fetchone', '(', ')', 'conn', '.', 'close', '(', ')', 'if', 'not', 'result', 'or', 'not', 'result', '.', 'get', '(', '""', 'report_path', '""', ')', ':', 'return', 'jsonify', '(', '{', '""', 'status', '""', ':', '""', 'error', '""', ',', '""', 'message', '""', ':', '""', 'Report not found', '""', '}', ')', ',', '404', 'try', ':', 'with', 'open', '(', 'result', '[', '""', 'report_path', '""', ']', ',', '""', 'r', '""', ',', 'encoding', '=', '""', 'utf-8', '""', ')', 'as', 'f', ':', 'content', '=', 'f', '.', 'read', '(', ')', 'return', 'jsonify', '(', '{', '""', 'status', '""', ':', '""', 'success', '""', ',', '""', 'content', '""', ':', 'content', '}', ')', 'except', 'Exception', 'as', 'e', ':', 'return', 'jsonify', '(', '{', '""', 'status', '""', ':', '""', 'error', '""', ',', '""', 'message', '""', ':', 'str', '(', 'e', ')', '}', ')', ',', '500']",Retrieve and return a research report in Markdown format from a database by ID.,"['Retrieve', 'and', 'return', 'a', 'research', 'report', 'in', 'Markdown', 'format', 'from', 'a', 'database', 'by', 'ID', '.']"
498,"def connect(self) -> bool:
        
        if self.sock:
            return True
            
        try:
            self.sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            self.sock.connect((self.host, self.port))
            logger.info(f""Connected to Blender at {self.host}:{self.port}"")
            return True
        except Exception as e:
            logger.error(f""Failed to connect to Blender: {str(e)}"")
            self.sock = None
            return False","['def', 'connect', '(', 'self', ')', '-', '>', 'bool', ':', 'if', 'self', '.', 'sock', ':', 'return', 'True', 'try', ':', 'self', '.', 'sock', '=', 'socket', '.', 'socket', '(', 'socket', '.', 'AF_INET', ',', 'socket', '.', 'SOCK_STREAM', ')', 'self', '.', 'sock', '.', 'connect', '(', '(', 'self', '.', 'host', ',', 'self', '.', 'port', ')', ')', 'logger', '.', 'info', '(', 'f', '""', 'Connected to Blender at ', '{', 'self', '.', 'host', '}', ':', '{', 'self', '.', 'port', '}', '""', ')', 'return', 'True', 'except', 'Exception', 'as', 'e', ':', 'logger', '.', 'error', '(', 'f', '""', 'Failed to connect to Blender: ', '{', 'str', '(', 'e', ')', '}', '""', ')', 'self', '.', 'sock', '=', 'None', 'return', 'False']","Establishes a socket connection to a specified host and port, logging success or failure.","['Establishes', 'a', 'socket', 'connection', 'to', 'a', 'specified', 'host', 'and', 'port', ',', 'logging', 'success', 'or', 'failure', '.']"
499,"def append_dims(x, target_dims):
    
    dims_to_append = target_dims - x.ndim
    if dims_to_append < 0:
        raise ValueError(f""input has {x.ndim} dims but target_dims is {target_dims}, which is less"")
    return x[(...,) + (None,) * dims_to_append]","['def', 'append_dims', '(', 'x', ',', 'target_dims', ')', ':', 'dims_to_append', '=', 'target_dims', '-', 'x', '.', 'ndim', 'if', 'dims_to_append', '<', '0', ':', 'raise', 'ValueError', '(', 'f', '""', 'input has ', '{', 'x', '.', 'ndim', '}', ' dims but target_dims is ', '{', 'target_dims', '}', ', which is less', '""', ')', 'return', 'x', '[', '(', '.', '.', '.', ',', ')', '+', '(', 'None', ',', ')', '*', 'dims_to_append', ']']",Add extra dimensions to an array until it matches the target dimensions.,"['Add', 'extra', 'dimensions', 'to', 'an', 'array', 'until', 'it', 'matches', 'the', 'target', 'dimensions', '.']"
500,"def api_delete_resource(research_id, resource_id):
    
    try:
        # Delete the resource
        success = delete_resource(resource_id)

        if success:
            return jsonify(
                {""status"": ""success"", ""message"": ""Resource deleted successfully""}
            )
        else:
            return jsonify({""status"": ""error"", ""message"": ""Resource not found""}), 404
    except Exception as e:
        logger.error(f""Error deleting resource: {str(e)}"")
        return jsonify({""status"": ""error"", ""message"": str(e)}), 500","['def', 'api_delete_resource', '(', 'research_id', ',', 'resource_id', ')', ':', 'try', ':', '# Delete the resource', 'success', '=', 'delete_resource', '(', 'resource_id', ')', 'if', 'success', ':', 'return', 'jsonify', '(', '{', '""', 'status', '""', ':', '""', 'success', '""', ',', '""', 'message', '""', ':', '""', 'Resource deleted successfully', '""', '}', ')', 'else', ':', 'return', 'jsonify', '(', '{', '""', 'status', '""', ':', '""', 'error', '""', ',', '""', 'message', '""', ':', '""', 'Resource not found', '""', '}', ')', ',', '404', 'except', 'Exception', 'as', 'e', ':', 'logger', '.', 'error', '(', 'f', '""', 'Error deleting resource: ', '{', 'str', '(', 'e', ')', '}', '""', ')', 'return', 'jsonify', '(', '{', '""', 'status', '""', ':', '""', 'error', '""', ',', '""', 'message', '""', ':', 'str', '(', 'e', ')', '}', ')', ',', '500']","Handles resource deletion via API, returning success or error messages.","['Handles', 'resource', 'deletion', 'via', 'API', ',', 'returning', 'success', 'or', 'error', 'messages', '.']"
